{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch\ub97c \uc774\uc6a9\ud55c \uc2e0\uacbd\ub9dd-\ubcc0\ud658(Neural-Transfer)\n======================================================\n**\uc800\uc790**: `Alexis Jacq <https://alexis-jacq.github.io>`_\n  **\ubc88\uc5ed**: `\uae40\ubd09\ubaa8 <http://fmttm.egloos.com>`_\n\n\uc18c\uac1c\n------------------\n\n\ud658\uc601\ud569\ub2c8\ub2e4!. \uc774 \ubb38\uc11c\ub294 Leon A. Gatys\uc640 Alexander S. Ecker, Matthias Bethge \uac00 \uac1c\ubc1c\ud55c\n\uc54c\uace0\ub9ac\uc998\uc778 `Neural-Style <https://arxiv.org/abs/1508.06576>`__ \ub97c \uad6c\ud604\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\n\uc124\uba85\ud558\ub294 \ud29c\ud1a0\ub9ac\uc5bc\uc785\ub2c8\ub2e4.\n\n\uc2e0\uacbd\ub9dd \ubb50\ub77c\uace0?\n~~~~~~~~~~~~~~~~~~~\n\n\uc2e0\uacbd\ub9dd \uc2a4\ud0c0\uc77c(Neural-Style), \ud639\uc740 \uc2e0\uacbd\ub9dd \ubcc0\ud654(Neural-Transfer)\ub294 \ucf58\ud150\uce20 \uc774\ubbf8\uc9c0(\uc608, \uac70\ubd81\uc774)\uc640 \n\uc2a4\ud0c0\uc77c \uc774\ubbf8\uc9c0(\uc608, \ud30c\ub3c4\ub97c \uadf8\ub9b0 \uc608\uc220 \uc791\ud488) \uc744 \uc785\ub825\uc73c\ub85c \ubc1b\uc544 \ucf58\ud150\uce20 \uc774\ubbf8\uc9c0\uc758 \ubaa8\uc591\ub300\ub85c \uc2a4\ud0c0\uc77c \uc774\ubbf8\uc9c0\uc758\n'\uadf8\ub9ac\ub294 \ubc29\uc2dd'\uc744 \uc774\uc6a9\ud574 \uadf8\ub9b0 \uac83\ucc98\ub7fc \uacb0\uacfc\ub97c \ub0b4\ub294 \uc54c\uace0\ub9ac\uc998\uc785\ub2c8\ub2e4:\n\n.. figure:: /_static/img/neural-style/neuralstyle.png\n   :alt: content1\n\n\uc5b4\ub5bb\uac8c \ub3d9\uc791\ud569\ub2c8\uae4c?\n~~~~~~~~~~~~~~~~~~~~~~~\n\n\uc6d0\ub9ac\ub294 \uac04\ub2e8\ud569\ub2c8\ub2e4. 2\uac1c\uc758 \uac70\ub9ac(distance)\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. \ud558\ub098\ub294 \ucf58\ud150\uce20( $D_C$ )\ub97c \uc704\ud55c \uac83\uc774\uace0 \n\ub2e4\ub978 \ud558\ub098\ub294 \uc2a4\ud0c0\uc77c( $D_S$ )\uc744 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n$D_C$ \ub294 \ucf58\ud150\uce20 \uc774\ubbf8\uc9c0\uc640 \uc2a4\ud0c0\uc77c \uc774\ubbf8\uc9c0 \uac04\uc758 \ucf58\ud150\uce20\uac00 \uc5bc\ub9c8\ub098 \ucc28\uc774\uac00 \uc788\ub294\uc9c0 \uce21\uc815\uc744 \ud569\ub2c8\ub2e4. \n\ubc18\uba74\uc5d0, $D_S$ \ub294 \ucf58\ud150\uce20 \uc774\ubbf8\uc9c0\uc640 \uc2a4\ud0c0\uc77c \uc774\ubbf8\uc9c0 \uac04\uc758 \uc2a4\ud0c0\uc77c\uc5d0\uc11c \uc5bc\ub9c8\ub098 \ucc28\uc774\uac00 \uc788\ub294\uc9c0\ub97c \uce21\uc815\ud569\ub2c8\ub2e4.\n\uadf8\ub7f0 \ub2e4\uc74c, \uc138 \ubc88\uc9f8 \uc774\ubbf8\uc9c0\ub97c \uc785\ub825(\uc608, \ub178\uc774\uc988\ub85c \uad6c\uc131\ub41c \uc774\ubbf8\uc9c0)\uc73c\ub85c\ubd80\ud130 \ucf58\ud150\uce20 \uc774\ubbf8\uc9c0\uc640\uc758 \ucf58\ud150\uce20 \uac70\ub9ac \n\ubc0f \uc2a4\ud0c0\uc77c \uc774\ubbf8\uc9c0\uc640\uc758 \uc2a4\ud0c0\uc77c \uac70\ub9ac\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \uc138 \ubc88\uc9f8 \uc774\ubbf8\uc9c0\ub97c \ubcc0\ud658\ud569\ub2c8\ub2e4.\n\n\uadf8\ub798\uc11c. \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud558\ub0d0\uace0\uc694?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\uc790, \ub354 \ub098\uc544\uac00\ub824\uba74 \uc218\ud559\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. $C_{nn}$ \ub97c \uc0ac\uc804 \ud6c8\ub828\ub41c \uae4a\uc740 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd \n\ub124\ud2b8\uc6cc\ud06c(pre-trained deep convolutional neural network)\ub77c\uace0 \ud558\uace0, $X$ \ub97c \uc5b4\ub5a4 \uc774\ubbf8\uc9c0\ub77c\uace0 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n$C_{nn}(X)$ \uc740 \uc785\ub825 \uc774\ubbf8\uc9c0 X\ub97c \uc785\ub825\uc73c\ub85c \ud574\uc11c CNN \uc744 \ud1b5\uacfc\ud55c \ub124\ud2b8\uc6cc\ud06c(\ubaa8\ub4e0 \ub808\uc774\uc5b4\ub4e4\uc758 \ud2b9\uc9d5 \ub9f5(feature map)\uc744 \ud3ec\ud568\ud558\ub294)\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.\n$F_{XL} \\in C_{nn}(X)$ \ub294 \uae4a\uc774 \ub808\ubca8 L\uc5d0\uc11c\uc758 \ud2b9\uc9d5 \ub9f5(feature map)\uc744 \uc758\ubbf8\ud558\uace0, \n\ubaa8\ub450 \ubca1\ud130\ud654(vectorized)\ub418\uace0 \uc5f0\uacb0\ub41c(concatenated) \ud558\ub098\uc758 \ub2e8\uc77c \ubca1\ud130\uc785\ub2c8\ub2e4.\n\uadf8\ub9ac\uace0, $Y$ \ub97c \uc774\ubbf8\uc9c0 $X$ \uc640 \ud06c\uae30\uac00 \uac19\uc740 \uc774\ubbf8\uc9c0\ub77c\uace0 \ud558\uba74, \n\ub808\uc774\uc5b4 $L$ \uc5d0 \ud574\ub2f9\ud558\ub294 \ucf58\ud150\uce20\uc758 \uac70\ub9ac\ub97c \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n\\begin{align}D_C^L(X,Y) = \\|F_{XL} - F_{YL}\\|^2 = \\sum_i (F_{XL}(i) - F_{YL}(i))^2\\end{align}\n\n$F_{XL}(i)$ \ub294 $F_{XL}$ \uc758 $i^{\ubc88\uc9f8}$ \uc694\uc18c(element) \uc785\ub2c8\ub2e4.\n\uc2a4\ud0c0\uc77c\uc5d0 \ud574\ub2f9\ud558\ub294 \ub0b4\uc6a9\uc740 \uc704 \ub0b4\uc6a9\ubcf4\ub2e4 \uc870\uae08 \ub354 \uc2e0\uacbd \uc4f8 \ubd80\ubd84\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n$F_{XL}^k$ \ub97c \ub808\uc774\uc5b4 $L$ \uc5d0\uc11c \ud2b9\uc9d5 \ub9f5(feature map) $K$ \uc758 $k^{\ubc88\uc9f8}$ \uc5d0 \ud574\ub2f9\ud558\ub294\n\ubca1\ud130\ud654\ub41c $k \\leq K$ \ub77c\uace0 \ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\uc2a4\ud0c0\uc77c $G_{XL}$ \uc758 $X$ \ub808\uc774\uc5b4\uc5d0\uc11c $L$ \uc740 \ubaa8\ub4e0 \ubca1\ud130\ud654\ub41c \ud2b9\uc9d5 \ub9f5(feature map) $F_{XL}^k$ \n\uc5d0\uc11c $k \\leq K$ \uadf8\ub78c(Gram)\uc73c\ub85c \uc815\uc758 \ub429\ub2c8\ub2e4.\n\ub2e4\uc2dc \ub9d0\ud558\uba74, $G_{XL}$ \ub294 $K$\\ x\\ $K$ \ud589\ub82c\uacfc \uc694\uc18c $G_{XL}(k,l)$ \uc758 $k^{\ubc88\uc9f8}$ \uc904\uacfc\n$l^{\ubc88\uc9f8}$ \ud589\uc758 $G_{XL}$ \ub294 $F_{XL}^k$ \uc640 $F_{XL}^l$ \uac04\uc758\n\ubca1\ud130\ud654 \uacf1\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4:\n\n\\begin{align}G_{XL}(k,l) = \\langle F_{XL}^k, F_{XL}^l\\rangle = \\sum_i F_{XL}^k(i) . F_{XL}^l(i)\\end{align}\n\n$F_{XL}^k(i)$ \ub294 $F_{XL}^k$ \uc758 $i^{\ubc88\uc9f8}$ \uc694\uc18c \uc785\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 $G_{XL}(k,l)$ \ub97c \ud2b9\uc9d5 \ub9f5(feature map) $k$ \uc640 $l$ \uac04\uc758 \n\uc0c1\uad00 \uad00\uacc4(correlation)\uc5d0 \ub300\ud55c \ucc99\ub3c4\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8\ub7f0 \uc758\ubbf8\uc5d0\uc11c, $G_{XL}$ \ub294 \ud2b9\uc9d5 \ub9f5(feature map) $X$ \uc758 \ub808\uc774\uc5b4 $L$ \uc5d0\uc11c\uc758 \n\uc0c1\uad00 \uad00\uacc4 \ud589\ub82c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n$G_{XL}$ \uc758 \ud06c\uae30\ub294 \ub2e8\uc9c0 \ud2b9\uc9d5 \ub9f5(feature map)\uc758 \uc22b\uc790\uc5d0\ub9cc \uc758\uc874\uc131\uc774 \uc788\uace0,\n$X$ \uc758 \ud06c\uae30\uc5d0\ub294 \uc758\uc874\uc131\uc774 \uc5c6\ub2e4\ub294 \uac83\uc744 \uc720\uc758 \ud574\uc57c \ud569\ub2c8\ub2e4.\n\uadf8\ub7ec\uba74, \ub9cc\uc57d $Y$ \uac00 \ub2e4\ub978 *\uc5b4\ub5a4 \ud06c\uae30\uc758* \uc774\ubbf8\uc9c0\ub77c\uba74,\n\uc6b0\ub9ac\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ub808\uc774\uc5b4 $L$ \uc5d0\uc11c \uc2a4\ud0c0\uc77c\uc758 \uac70\ub9ac\ub97c \uc815\uc758 \ud569\ub2c8\ub2e4.\n\n\\begin{align}D_S^L(X,Y) = \\|G_{XL} - G_{YL}\\|^2 = \\sum_{k,l} (G_{XL}(k,l) - G_{YL}(k,l))^2\\end{align}\n\n$D_C(X,C)$ \uc758 \ud55c \ubc88\uc758 \ucd5c\uc18c\ud654\ub97c \uc704\ud574\uc11c, \uc774\ubbf8\uc9c0 \ubcc0\uc218 $X$ \uc640 \ub300\uc0c1 \ucf58\ud150\uce20-\uc774\ubbf8\uc9c0 $C$ \uc640\n$D_S(X,S)$ \uc640 $X$ \uc640 \ub300\uc0c1 \uc2a4\ud0c0\uc77c-\uc774\ubbf8\uc9c0 $S$ , \ub458 \ub2e4 \uc5ec\ub7ec \ub808\uc774\uc5b4\ub4e4\uc5d0 \ub300\ud574\uc11c \uacc4\uc0b0\ub418\uc57c \ud558\uace0,\n\uc6b0\ub9ac\ub294 \uc6d0\ud558\ub294 \ub808\uc774\uc5b4 \uac01\uac01\uc5d0\uc11c\uc758 \uac70\ub9ac\uc758 \uadf8\ub77c\ub514\uc5b8\ud2b8\ub97c \uacc4\uc0b0\ud558\uace0 \ub354\ud569\ub2c8\ub2e4( $X$ \uc640 \uad00\ub828\ub41c \ub3c4\ud568\uc218):\n\n\\begin{align}\\nabla_{\textit{total}}(X,S,C) = \\sum_{L_C} w_{CL_C}.\\nabla_{\textit{content}}^{L_C}(X,C) + \\sum_{L_S} w_{SL_S}.\\nabla_{\textit{style}}^{L_S}(X,S)\\end{align}\n\n$L_C$ \uc640 $L_S$ \ub294 \uac01\uac01 \ucf58\ud150\uce20\uc640 \uc2a4\ud0c0\uc77c\uc758 \uc6d0\ud558\ub294 (\uc784\uc758 \uc0c1\ud0dc\uc758) \ub808\uc774\uc5b4\ub4e4\uc744 \uc758\ubbf8\ud558\uace0,\n$w_{CL_C}$ \uc640 $w_{SL_S}$ \ub294 \uc6d0\ud558\ub294 \ub808\uc774\uc5b4\uc5d0\uc11c\uc758\n\uc2a4\ud0c0\uc77c \ub610\ub294 \ucf58\ud150\uce20\uc758 \uac00\uc911\uce58\ub97c (\uc784\uc758 \uc0c1\ud0dc\uc758) \uc758\ubbf8\ud569\ub2c8\ub2e4.\n\uadf8\ub9ac\uace0 \ub098\uc11c, \uc6b0\ub9ac\ub294 $X$ \uc5d0 \ub300\ud574 \uacbd\uc0ac \ud558\uac15\ubc95\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4.\n\n\\begin{align}X \\leftarrow X - \\alpha \\nabla_{\textit{total}}(X,S,C)\\end{align}\n\n\ub124, \uc218\ud559\uc740 \uc774\uc815\ub3c4\uba74 \ucda9\ubd84\ud569\ub2c8\ub2e4. \ub9cc\uc57d \ub354 \uae4a\uc774 \uc54c\uace0 \uc2f6\ub2e4\uba74 (\uadf8\ub808\uc774\uc5b8\ud2b8\ub97c \uc5b4\ub5bb\uac8c \uacc4\uc0b0\ud558\ub294\uc9c0),\nLeon A. Gatys and AL\uac00 \uc791\uc131\ud55c **\uc6d0\ub798\uc758 \ub17c\ubb38\uc744 \uc77d\uc5b4 \ubcfc \uac83\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4** \n\ub17c\ubb38\uc5d0\ub294 \uc55e\uc11c \uc124\uba85\ud55c \ub0b4\uc6a9\ub4e4 \ubaa8\ub450\uc5d0 \ub300\ud574 \ubcf4\ub2e4 \uc790\uc138\ud558\uace0 \uba85\ud655\ud558\uac8c \uc598\uae30\ud569\ub2c8\ub2e4.\n\n\uad6c\ud604\uc744 \uc704\ud574\uc11c PyTorch\uc5d0\uc11c\ub294 \uc774\ubbf8 \uc6b0\ub9ac\uac00 \ud544\uc694\ub85c\ud558\ub294 \ubaa8\ub4e0 \uac83\uc744 \uac16\ucd94\uace0 \uc788\uc2b5\ub2c8\ub2e4. \n\uc2e4\uc81c\ub85c PyTorch\ub97c \uc0ac\uc6a9\ud558\uba74 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \ub3d9\uc548 \ubaa8\ub4e0 \uadf8\ub77c\ub514\uc5b8\ud2b8(Gradient)\uac00 \n\uc790\ub3d9,\ub3d9\uc801\uc73c\ub85c \uacc4\uc0b0\ub429\ub2c8\ub2e4.(\ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \ub3d9\uc548)\n\uc774\ub7f0 \uc810\uc774 PyTorch\uc5d0\uc11c \uc54c\uace0\ub9ac\uc998 \uad6c\ud604\uc744 \ub9e4\uc6b0 \ud3b8\ub9ac\ud558\uac8c \ud569\ub2c8\ub2e4.\n\nPyTorch \uad6c\ud604\n----------------------\n\n\uc704\uc758 \ubaa8\ub4e0 \uc218\ud559\uc744 \uc774\ud574\ud560 \uc218 \uc5c6\ub2e4\uba74, \uad6c\ud604\ud568\uc73c\ub85c\uc368 \uc774\ud574\ub3c4\ub97c \ub192\uc5ec \uac08 \uc218 \uc788\uc744 \uac83 \uc785\ub2c8\ub2e4. \nPyTorch\ub97c \uc774\uc6a9\ud560 \uc608\uc815\uc774\ub77c\uba74, \uba3c\uc800 \uc774 \ubb38\uc11c :doc:`Introduction to PyTorch </beginner/deep_learning_60min_blitz>` \ub97c \uc77d\uc5b4\ubcfc \uac83\uc744 \ucd94\ucc9c \ud569\ub2c8\ub2e4.\n\n\ud328\ud0a4\uc9c0\ub4e4\n~~~~~~~~\n\n\uc6b0\ub9ac\ub294 \ub2e4\uc74c \ud328\ud0a4\uc9c0\ub4e4\uc744 \ud65c\uc6a9 \ud560 \uac83\uc785\ub2c8\ub2e4:\n\n-  ``torch`` , ``torch.nn``, ``numpy`` (PyTorch\ub85c \uc2e0\uacbd\ub9dd \ucc98\ub9ac\ub97c \uc704\ud55c \ud544\uc218 \ud328\ud0a4\uc9c0)\n-  ``torch.optim`` (\ud6a8\uc728\uc801\uc778 \uadf8\ub77c\ub514\uc5b8\ud2b8 \ub514\uc13c\ud2b8)\n-  ``PIL`` , ``PIL.Image`` , ``matplotlib.pyplot`` (\uc774\ubbf8\uc9c0\ub97c \uc77d\uace0 \ubcf4\uc5ec\uc8fc\ub294 \ud328\ud0a4\uc9c0)\n-  ``torchvision.transforms`` (PIL\ud0c0\uc785\uc758 \uc774\ubbf8\uc9c0\ub4e4\uc744 \ud1a0\uce58 \ud150\uc11c \ud615\ud0dc\ub85c \ubcc0\ud615\ud574\uc8fc\ub294 \ud328\ud0a4\uc9c0)\n-  ``torchvision.models`` (\uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\ub4e4\uc758 \ud559\uc2b5 \ub610\ub294 \uc77d\uae30 \ud328\ud0a4\uc9c0)\n-  ``copy`` (\ubaa8\ub378\ub4e4\uc758 \uae4a\uc740 \ubcf5\uc0ac\ub97c \uc704\ud55c \uc2dc\uc2a4\ud15c \ud328\ud0a4\uc9c0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ucfe0\ub2e4(CUDA)\n~~~~~~~~~~~~~~\n\n\ucef4\ud4e8\ud130\uc5d0 GPU\uac00 \uc788\ub294 \uacbd\uc6b0, \ud2b9\ud788 VGG\uc640 \uac19\uc774 \uae4a\uc740 \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud558\ub824\ub294 \uacbd\uc6b0 \n\uc54c\uace0\ub9ac\uc998\uc744 CUDA \ud658\uacbd\uc5d0\uc11c \uc2e4\ud589\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4. \nCUDA\ub97c \uc4f0\uae30 \uc704\ud574\uc11c Pytorch\uc5d0\uc11c\ub294 ``torch.cuda.is_available()`` \ub97c \uc81c\uacf5\ud558\ub294\ub370, \n\uc791\uc5c5\ud558\ub294 \ucef4\ud4e8\ud130\uc5d0\uc11c GPU \uc0ac\uc6a9\uc774 \uac00\ub2a5\ud558\uba74 ``True`` \ub97c \ub9ac\ud134 \ud569\ub2c8\ub2e4.\n\uc774\ud6c4\ub85c, \uc6b0\ub9ac\ub294 ``.cuda()`` \ub77c\ub294 \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub4c8\uacfc \uad00\ub828\ub41c \ud560\ub2f9\ub41c \ud504\ub85c\uc138\uc2a4\ub97c CPU\uc5d0\uc11c GPU\ub85c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774 \ubaa8\ub4c8\uc744 CPU\ub85c \ub418\ub3cc\ub9ac\uace0 \uc2f6\uc744 \ub54c\uc5d0\ub294 (\uc608 : numpy\uc5d0\uc11c \uc0ac\uc6a9), \uc6b0\ub9ac\ub294 ``.cpu ()`` \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uba74 \ub429\ub2c8\ub2e4.\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c, ``.type(dtype)`` \uba54\uc18c\ub4dc\ub294 ``torch.FloatTensor`` \ud0c0\uc785\uc744 \nGPU\uc5d0\uc11c \uc0ac\uc6a9 \ud560 \uc218 \uc788\ub3c4\ub85d ``torch.cuda.FloatTensor`` \ub85c \ubcc0\ud658\ud558\ub294\ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\ubbf8\uc9c0 \uc77d\uae30\n~~~~~~~~~~~~~\n\n\uad6c\ud604\uc744 \uac04\ub2e8\ud558\uac8c \ud558\uae30 \uc704\ud574\uc11c, \uc2a4\ud0c0\uc77c \uc774\ubbf8\uc9c0\uc640 \ucf58\ud150\uce20 \uc774\ubbf8\uc9c0\uc758 \ud06c\uae30\ub97c \ub3d9\uc77c\ud558\uac8c \ub9de\ucd94\uc5b4\uc11c \uc2dc\uc791\ud569\ub2c8\ub2e4.\n\uadf8\ub7f0 \ub2e4\uc74c \uc6d0\ud558\ub294 \ucd9c\ub825 \uc774\ubbf8\uc9c0 \ud06c\uae30\ub85c \ud655\uc7a5 \uc2dc\ud0b5\ub2c8\ub2e4.(\ubcf8 \uc608\uc81c\uc5d0\uc11c\ub294 128\uc774\ub098 512\ub85c \ud558\ub294\ub370 GPU\uac00 \uac00\ub2a5\ud55c \uc0c1\ud669\uc5d0 \ub9de\uac8c \uc120\ud0dd\ud574\uc11c \ud558\uc138\uc694.)\n\uadf8\ub9ac\uace0 \uc601\uc0c1 \ub370\uc774\ud130\ub97c \ud1a0\uce58 \ud150\uc11c\ub85c \ubcc0\ud658\ud558\uace0, \uc2e0\uacbd\ub9dd \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc900\ube44\ud569\ub2c8\ub2e4.\n\n.. Note::\n    \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2e4\ud589\ud558\ub294 \ub370 \ud544\uc694\ud55c \uc774\ubbf8\uc9c0\ub97c \ub2e4\uc6b4\ub85c\ub4dc\ud558\ub294 \ub9c1\ud06c\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.:\n    `picasso.jpg <http://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg>`__ \uc640\n    `dancing.jpg <http://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg>`__.\n    \uc704 \ub450\uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uc544 \ub514\ub809\ud1a0\ub9ac \uc774\ub984 ``images`` \uc5d0 \ucd94\uac00\ud558\uc138\uc694.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ucd9c\ub825 \uc774\ubbf8\uc9c0\uc758 \uc6d0\ud558\ub294 \ud06c\uae30\ub97c \uc815\ud558\uc138\uc694.\nimsize = 512 if torch.cuda.is_available() else 128  # gpu\uac00 \uc5c6\ub2e4\uba74 \uc791\uc740 \ud06c\uae30\ub85c\n\nloader = transforms.Compose([\n    transforms.Resize(imsize),  # \uc785\ub825 \uc601\uc0c1 \ud06c\uae30\ub97c \ub9de\ucda4\n    transforms.ToTensor()])  # \ud1a0\uce58 \ud150\uc11c\ub85c \ubcc0\ud658\n\n\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    # \ub124\ud2b8\uc6cc\ud06c\uc758 \uc785\ub825 \ucc28\uc6d0\uc744 \ub9de\ucd94\uae30 \uc704\ud574 \ud544\uc694\ud55c \uac00\uc9dc \ubc30\uce58 \ucc28\uc6d0\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\n\nstyle_img = image_loader(\"./data/images/neural-style/picasso.jpg\")\ncontent_img = image_loader(\"./data/images/neural-style/dancing.jpg\")\n\nassert style_img.size() == content_img.size(), \\\n    \"we need to import style and content images of the same size\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac00\uc838\uc628 PIL \uc774\ubbf8\uc9c0\ub294 0\uc5d0\uc11c 255 \uc0ac\uc774\uc758 \uc774\ubbf8\uc9c0 \ud53d\uc140\uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4. \n\ud1a0\uce58 \ud150\uc11c\ub85c \ubcc0\ud658\ud558\uba74 0\uc5d0\uc11c 1\uc758 \uac12\uc73c\ub85c \ubcc0\ud658\ub429\ub2c8\ub2e4. \n\uc774\ub294 \uc911\uc694\ud55c \ub514\ud14c\uc77c\ub85c: \ud1a0\uce58 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \uc2e0\uacbd\ub9dd\uc740 0\uc5d0\uc11c 1\uc758 \ud150\uc11c \uc774\ubbf8\uc9c0\ub85c \ud559\uc2b5\ud558\uac8c \ub429\ub2c8\ub2e4.\n0-255 \ud150\uc11c \uc774\ubbf8\uc9c0\ub97c \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uacf5\uae09 \ud558\ub824\uace0 \ud558\uba74 \ud65c\uc131\ud654\ub41c(activated) \ud2b9\uc9d5 \ub9f5(feature map)\uc740 \uc758\ubbf8\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.(\uc5ed\uc790\uc8fc, \uc785\ub825 \uac12\uc5d0 \ub530\ub77c RELU\uc640 \uac19\uc740 \ud65c\uc131\ud654 \ub808\uc774\uc5b4\uc5d0\uc11c \uc785\ub825\uc73c\ub85c \ub418\ub294 \uac12\uc758 \ubc94\uc704\uac00 \uc644\uc804\ud788 \ub2e4\ub974\uae30 \ub54c\ubb38)\nCaffe \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \uc0ac\uc804 \ud6c8\ub828\ub41c \ub124\ud2b8\uc6cc\ud06c\uc758 \uacbd\uc6b0\ub294 \uadf8\ub807\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4: \ud574\ub2f9 \ubaa8\ub378\ub4e4\uc740 0\uc5d0\uc11c 255 \uc0ac\uc774 \uac12\uc758 \ud150\uc11c \uc774\ubbf8\uc9c0\ub85c \ud559\uc2b5 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\uc774\ubbf8\uc9c0 \ud45c\uc2dc\ud558\uae30\n~~~~~~~~~~~~~~~~~~~~\n\n\uc6b0\ub9ac\ub294 \uc774\ubbf8\uc9c0\ub97c \ud45c\uc2dc\ud558\uae30 \uc704\ud574 ``plt.imshow`` \ub97c \uc774\uc6a9\ud569\ub2c8\ub2e4. \n\uadf8\ub7ec\uae30 \uc704\ud574 \uc6b0\uc120 \ud150\uc11c\ub97c PIL \uc774\ubbf8\uc9c0\ub85c \ubcc0\ud658\ud574 \uc8fc\uaca0\uc2b5\ub2c8\ub2e4:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unloader = transforms.ToPILImage()  # PIL \uc774\ubbf8\uc9c0\ub85c \uc7ac\ubcc0\ud658 \ud569\ub2c8\ub2e4\n\nplt.ion()\n\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone()  # \ud150\uc11c\uc758 \uac12\uc5d0 \ubcc0\ud654\uac00 \uc801\uc6a9\ub418\uc9c0 \uc54a\ub3c4\ub85d \ud150\uc11c\ub97c \ubcf5\uc81c\ud569\ub2c8\ub2e4\n    image = image.squeeze(0)      # \ud398\uc774\ud06c \ubc30\uce58 \ucc28\uc6d0\uc744 \uc81c\uac70 \ud569\ub2c8\ub2e4\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001) # \uadf8\ub9ac\ub294 \ubd80\ubd84\uc774 \uc5c5\ub370\uc774\ud2b8 \ub420 \uc218 \uc788\uac8c \uc7a0\uc2dc \uc815\uc9c0\ud569\ub2c8\ub2e4\n\n\nplt.figure()\nimshow(style_img, title='Style Image')\n\nplt.figure()\nimshow(content_img, title='Content Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ucf58\ud150\uce20 \ub85c\uc2a4\n~~~~~~~~~~~~\n\n\ucf58\ud150\uce20 \ub85c\uc2a4\ub294 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c $X$ \ub85c \uc785\ub825\uc744 \ubc1b\uc558\uc744 \ub54c \ub808\uc774\uc5b4 $L$ \uc5d0\uc11c \ud2b9\uc9d5 \ub9f5(feature map) $F_{XL}$ \uc744 \uc785\ub825\uc73c\ub85c \uac00\uc838 \uc640\uc11c \n\uc774 \uc774\ubbf8\uc9c0\uc640 \ucf58\ud150\uce20 \uc774\ubbf8\uc9c0 \uc0ac\uc774\uc758 \uac00\uc911\uce58 \ucf58\ud150\uce20 \uac70\ub9ac $w_{CL}.D_C^L(X,C)$ \ub97c \ubc18\ud658\ud558\ub294 \uae30\ub2a5\uc785\ub2c8\ub2e4. \n\ub530\ub77c\uc11c, \uac00\uc911\uce58 $w_{CL}$ \ubc0f \ubaa9\ud45c \ucf58\ud150\uce20 $F_{CL}$ \uc740 \ud568\uc218\uc758 \ud30c\ub77c\ubbf8\ud130 \uc785\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 \uc774 \ub9e4\uac1c \ubcc0\uc218\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uc0dd\uc131\uc790(constructor)\uac00 \uc788\ub294 \ud1a0\uce58 \ubaa8\ub4c8\ub85c \ud568\uc218\ub97c \uad6c\ud604\ud569\ub2c8\ub2e4. \n\uac70\ub9ac $\\|F_{XL} - F_{YL}\\|^2$ \ub294 \uc138 \ubc88\uc9f8 \ub9e4\uac1c \ubcc0\uc218\ub85c \uba85\uc2dc\ub41c \uae30\uc900 ``nn.MSELoss`` \ub97c \uc0ac\uc6a9\ud558\uc5ec\n\uacc4\uc0b0\ud560 \uc218 \uc788\ub294 \ub450 \uc138\ud2b8\uc758 \ud2b9\uc9d5 \ub9f5(feature map) \uc0ac\uc774\uc758 \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28(MSE, Mean Square Error)\uc785\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 \uc2e0\uacbd\ub9dd\uc758 \ucd94\uac00 \ubaa8\ub4c8\ub85c\uc11c \uac01 \ub808\uc774\uc5b4\uc5d0 \ucee8\ud150\uce20 \ub85c\uc2a4\ub97c \ucd94\uac00 \ud560 \uac83 \uc785\ub2c8\ub2e4. \n\uc774\ub807\uac8c \ud558\uba74 \uc785\ub825 \uc601\uc0c1 $X$ \ub97c \ub124\ud2b8\uc6cc\ud06c\uc5d0 \ubcf4\ub0bc \ub54c\ub9c8\ub2e4 \uc6d0\ud558\ub294 \ubaa8\ub4e0 \ub808\uc774\uc5b4\uc5d0\uc11c \n\ubaa8\ub4e0 \ucee8\ud150\uce20 \ub85c\uc2a4\uac00 \uacc4\uc0b0\ub418\uace0 \uc790\ub3d9 \uadf8\ub77c\ub514\uc5b8\ud2b8\ub85c \uc778\ud574 \ubaa8\ub4e0 \uadf8\ub77c\ub514\uc5b8\ud2b8\uac00 \uacc4\uc0b0\ub429\ub2c8\ub2e4. \n\uc774\ub97c \uc704\ud574 \uc6b0\ub9ac\ub294 \uc785\ub825\uc744 \ub9ac\ud134\ud558\ub294 ``forward`` \uba54\uc18c\ub4dc\ub97c \ub9cc\ub4e4\uae30\ub9cc \ud558\uba74 \ub429\ub2c8\ub2e4: \ubaa8\ub4c8\uc740 \uc2e0\uacbd\ub9dd\uc758 ''\ud22c\uba85 \ub808\uc774\uc5b4'' \uac00 \ub429\ub2c8\ub2e4. \n\uacc4\uc0b0\ub41c \ub85c\uc2a4\ub294 \ubaa8\ub4c8\uc758 \ub9e4\uac1c \ubcc0\uc218\ub85c \uc800\uc7a5\ub429\ub2c8\ub2e4.\n\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c \uadf8\ub77c\ub514\uc5b8\ud2b8\ub97c \uc7ac\uad6c\uc131\ud558\uae30 \uc704\ud574 nn.MSELoss\uc758 ``backward`` \uba54\uc11c\ub4dc\ub97c \ud638\ucd9c\ud558\ub294 \uac00\uc9dc backward \uba54\uc11c\ub4dc\ub97c \uc815\uc758 \ud569\ub2c8\ub2e4. \n\uc774 \uba54\uc11c\ub4dc\ub294 \uacc4\uc0b0\ub41c \ub85c\uc2a4\ub97c \ubc18\ud658 \ud569\ub2c8\ub2e4. \uc774\ub294 \uc2a4\ud0c0\uc77c \ubc0f \ucf58\ud150\uce20 \ub85c\uc2a4\uc758 \uc9c4\ud654\ub97c \ud45c\uc2dc\ud558\uae30 \uc704\ud574 \uadf8\ub77c\ub514\uc5b8\ud2b8 \ub514\uc13c\ud2b8\ub97c \uc2e4\ud589\ud560 \ub54c \uc720\uc6a9\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        # \uadf8\ub77c\ub514\uc5b8\ud2b8\ub97c \ub3d9\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ud2b8\ub9ac\uc5d0\uc11c \ub300\uc0c1 \ucf58\ud150\uce20\ub97c '\ubd84\ub9ac' \ud569\ub2c8\ub2e4.\n        # :\uc774 \uac12\uc740 \ubcc0\uc218(variable)\uac00 \uc544\ub2c8\ub77c \uba85\uc2dc\ub41c \uac12\uc785\ub2c8\ub2e4. \n        # \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 \uae30\uc900\uc758 \uc804\ub2ec \uba54\uc18c\ub4dc\uac00 \uc624\ub958\ub97c \ubc1c\uc0dd \uc2dc\ud0b5\ub2c8\ub2e4.\n        self.target = target.detach()\n\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n   **\uc911\uc694\ud55c \ub514\ud14c\uc77c**: \uc774 \ubaa8\ub4c8\uc740 ``ContentLoss`` \ub77c\uace0 \uc774\ub984 \uc9c0\uc5b4\uc84c\uc9c0\ub9cc \uc9c4\uc815\ud55c PyTorch Loss \ud568\uc218\ub294 \uc544\ub2d9\ub2c8\ub2e4. \ucee8\ud150\uce20 \uc190\uc2e4\uc744 PyTorch Loss\ub85c \uc815\uc758 \ud558\ub824\uba74 PyTorch autograd Function\uc744 \uc0dd\uc131 \ud558\uace0 ``backward`` \uba54\uc18c\ub4dc\uc5d0\uc11c \uc9c1\uc811 \uadf8\ub77c\ub514\uc5b8\ud2b8\ub97c \uc7ac\uacc4\uc0b0/\uad6c\ud604 \ud574\uc57c \ud569\ub2c8\ub2e4.\n\n\uc2a4\ud0c0\uc77c \ub85c\uc2a4\n~~~~~~~~~~~~~~~~~~\n\n\uc2a4\ud0c0\uc77c \uc190\uc2e4\uc744 \uc704\ud574 \uc6b0\ub9ac\ub294 \ub808\uc774\uc5b4 $L$ \uc5d0\uc11c $X$ \ub85c \uacf5\uae09\ub41c(\uc785\ub825\uc73c\ub85c \ud558\ub294) \uc2e0\uacbd\ub9dd\uc758 \ud2b9\uc9d5 \ub9f5(feature map) $F_{XL}$ \uc774 \uc8fc\uc5b4\uc9c4 \uacbd\uc6b0\n\uadf8\ub7a8 \uc0dd\uc131 $G_{XL}$ \uc744 \uacc4\uc0b0\ud558\ub294 \ubaa8\ub4c8\uc744 \uba3c\uc800 \uc815\uc758 \ud574\uc57c \ud569\ub2c8\ub2e4. \n$\\hat{F}_{XL}$ \uc744 KxN \ud589\ub82c\uc5d0 \ub300\ud55c $F_{XL}$\uc758 \ubaa8\uc591\uc744 \ubcc0\uacbd\ud55c \ubc84\uc804\uc774\ub77c\uace0 \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\uc5ec\uae30\uc11c, $K$\ub294 \ub808\uc774\uc5b4 $L$\uc5d0\uc11c\uc758 \ud2b9\uc9d5 \ub9f5(feature map)\ub4e4\uc758 \uc218\uc774\uace0, $N$ \uc740 \uc784\uc758\uc758 \ubca1\ud130\ud654 \ub41c \ud2b9\uc9d5 \ub9f5(feature map) $F_{XL}^k$ \uc758 \uae38\uc774\uac00 \ub429\ub2c8\ub2e4. \n$F_{XL}^k$ \uc758 $k^{\ubc88\uc9f8}$ \ubc88\uc9f8 \uc904\uc740 $F_{XL}^k$ \uc785\ub2c8\ub2e4. \nmath:`\\hat{F}_{XL} \\cdot \\hat{F}_{XL}^T = G_{XL}` \uc778\uc9c0 \ud655\uc778 \ud574\ubcf4\uae38 \ubc14\ub78d\ub2c8\ub2e4. \n\uc774\ub97c \ud655\uc778\ud574\ubcf4\uba74 \ubaa8\ub4c8\uc744 \uad6c\ud604\ud558\ub294 \uac83\uc774 \uc26c\uc6cc \uc9d1\ub2c8\ub2e4:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input):\n    a, b, c, d = input.size()  # a=\ubc30\uce58 \ud06c\uae30(=1)\n    # b=\ud2b9\uc9d5 \ub9f5\uc758 \ud06c\uae30\n    # (c,d)=\ud2b9\uc9d5 \ub9f5(N=c*d)\uc758 \ucc28\uc6d0\n\n    features = input.view(a * b, c * d)  # F_XL\uc744 \\hat F_XL\ub85c \ud06c\uae30 \uc870\uc815\ud569\ub2c8\ub2e4\n\n    G = torch.mm(features, features.t())  # \uadf8\ub7a8 \uacf1\uc744 \uc218\ud589\ud569\ub2c8\ub2e4\n\n    # \uadf8\ub7a8 \ud589\ub82c\uc758 \uac12\uc744 \uac01 \ud2b9\uc9d5 \ub9f5\uc758 \uc694\uc18c \uc22b\uc790\ub85c \ub098\ub204\ub294 \ubc29\uc2dd\uc73c\ub85c '\uc815\uaddc\ud654'\ub97c \uc218\ud589\ud569\ub2c8\ub2e4.\n    return G.div(a * b * c * d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud2b9\uc9d5 \ub9f5(feature map) \ucc28\uc6d0 $N$\uc774 \ud074\uc218\ub85d, \uadf8\ub7a8(Gram) \ud589\ub82c\uc758 \uac12\uc774 \ucee4\uc9d1\ub2c8\ub2e4. \n\ub530\ub77c\uc11c $N$\uc73c\ub85c \uc815\uaddc\ud654\ud558\uc9c0 \uc54a\uc73c\uba74 \uccab\ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0\uc11c \uacc4\uc0b0\ub41c \ub85c\uc2a4 (\ud480\ub9c1 \ub808\uc774\uc5b4 \uc804\uc5d0)\ub294\n\uacbd\uc0ac \ud558\uac15\ubc95 \ub3d9\uc548 \ud6e8\uc52c \ub354 \uc911\uc694\ud558\uac8c \ub429\ub2c8\ub2e4. (\uc5ed\uc790\uc8fc : \uc815\uaddc\ud654\ub97c \ud558\uc9c0 \uc54a\uc73c\uba74 \uccab\ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0\uc11c \uacc4\uc0b0\ub41c \uac12\ub4e4\uc758 \uac00\uc911\uce58\uac00 \ub192\uc544\uc838 \uc0c1\ub300\uc801\uc73c\ub85c \ub2e4\ub978 \ub808\uc774\uc5b4\uc5d0\uc11c \uacc4\uc0b0\ud55c \uac12\ub4e4\uc758 \ubc18\uc601\uc774 \uc801\uac8c \ub418\ubc84\ub9ac\uae30 \ub54c\ubb38\uc5d0 \uc815\uaddc\ud654\uac00 \ud544\uc694\ud574\uc9d1\ub2c8\ub2e4.)\n\uc2a4\ud0c0\uc77c \ud2b9\uc9d5\uc758 \ud765\ubbf8\ub85c\uc6b4 \ubd80\ubd84\ub4e4\uc740 \uac00\uc7a5 \uae4a\uc740 \ub808\uc774\uc5b4\uc5d0 \uc788\uae30 \ub54c\ubb38\uc5d0 \uadf8\ub807\uac8c \ub3d9\uc791\ud558\uc9c0 \uc54a\ub3c4\ub85d \ud574\uc57c \ud569\ub2c8\ub2e4!\n\n\uadf8\ub7f0 \ub2e4\uc74c \uc2a4\ud0c0\uc77c \ub85c\uc2a4 \ubaa8\ub4c8\uc740 \ucf58\ud150\uce20 \ub85c\uc2a4 \ubaa8\ub4c8\uacfc \uc644\uc804\ud788 \ub3d9\uc77c\ud55c \ubc29\uc2dd\uc73c\ub85c \uad6c\ud604\ub418\uc9c0\ub9cc\n\ub300\uc0c1\uacfc \uc785\ub825 \uac04\uc758 \uadf8\ub7a8 \ub9e4\ud2b8\ub9ad\uc2a4\uc758 \ucc28\uc774\ub97c \ube44\uad50\ud558\uac8c \ub429\ub2c8\ub2e4\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class StyleLoss(nn.Module):\n\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub274\ub7f4 \ub124\ud2b8\uc6cc\ud06c \uc77d\uae30\n~~~~~~~~~~~~~~~~~~~~~~~\n\n\uc790, \uc6b0\ub9ac\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \uc2e0\uacbd\ub9dd\uc744 \uac00\uc838\uc640\uc57c \ud569\ub2c8\ub2e4. \uc774 \ub17c\ubb38\uc5d0\uc11c\uc640 \uac19\uc774, \n\uc6b0\ub9ac\ub294 19 \ub808\uc774\uc5b4 \uce35\uc744 \uac00\uc9c0\ub294 VGG(VGG19) \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc804 \ud6c8\ub828\ub41c \ub124\ud2b8\uc6cc\ud06c\ub85c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\nPyTorch\uc758 VGG \uad6c\ud604\uc740 \ub450 \uac1c\uc758 \ud558\uc704 \uc21c\ucc28 \ubaa8\ub4c8\ub85c \ub098\ub25c \ubaa8\ub4c8 \uc785\ub2c8\ub2e4. \n``\ud2b9\uc9d5(features)`` \ubaa8\ub4c8 : \ud569\uc131\uacf1\uacfc \ud480\ub9c1 \ub808\uc774\uc5b4\ub4e4\uc744 \ud3ec\ud568 \ud569\ub2c8\ub2e4.\n``\ubd84\ub958(classifier)`` \ubaa8\ub4c8 : fully connected \ub808\uc774\uc5b4\ub4e4\uc744 \ud3ec\ud568 \ud569\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c ``\ud2b9\uc9d5`` \ubaa8\ub4c8\uc5d0 \uad00\uc2ec\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\uc77c\ubd80 \ub808\uc774\uc5b4\ub294 \ud559\uc2b5 \ubc0f \ud3c9\uac00\uc5d0 \uc788\uc5b4\uc11c \uc0c1\ud669\uc5d0 \ub530\ub77c \ub2e4\ub978 \ub3d9\uc791\uc744 \ud569\ub2c8\ub2e4. \n\uc774\ud6c4 \uc6b0\ub9ac\ub294 \uadf8\uac83\uc744 \ud2b9\uc9d5 \ucd94\ucd9c\uc790\ub85c \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \n\uc6b0\ub9ac\ub294 .eval() \uc744 \uc0ac\uc6a9\ud558\uc5ec \ub124\ud2b8\uc6cc\ud06c\ub97c \ud3c9\uac00 \ubaa8\ub4dc\ub85c \uc124\uc815 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub610\ud55c VGG \ub124\ud2b8\uc6cc\ud06c\ub294 \ud3c9\uade0 = [0.485, 0.456, 0.406] \ubc0f \ud45c\uc900\ud3b8\ucc28 = [0.229, 0.224, 0.225]\ub85c \uc815\uaddc\ud654 \ub41c \uac01 \ucc44\ub110\uc758 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \ud559\uc2b5\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4.\n(\uc5ed\uc790, \uc77c\ubc18\uc801\uc73c\ub85c \ub124\ud2b8\uc6cc\ud06c\ub294 \uc774\ubbf8\uc9c0\ub137\uc73c\ub85c \ud559\uc2b5\uc774 \ub418\uace0 \uc774\ubbf8\uc9c0\ub137 \ub370\uc774\ud130\uc758 \ud3c9\uade0\uacfc \ud45c\uc900\ud3b8\ucc28\uac00 \uc704\uc758 \uac12\uacfc \uac19\uc2b5\ub2c8\ub2e4.)\n\uc6b0\ub9ac\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ub124\ud2b8\uc6cc\ud06c\ub85c \ubcf4\ub0b4\uae30 \uc804\uc5d0 \uc815\uaddc\ud654 \ud558\ub294\ub370 \uc704 \ud3c9\uade0\uacfc \ud45c\uc900\ud3b8\ucc28 \uac12\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n# \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \uc815\uaddc\ud654\ud558\ub294 \ubaa8\ub4c8\uc744 \ub9cc\ub4e4\uc5b4 nn.Sequential\uc5d0 \uc27d\uac8c \uc785\ub825 \ud560 \uc218 \uc788\uac8c \ud558\uc138\uc694.\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        # .view(\ud150\uc11c\uc758 \ubaa8\uc591\uc744 \ubc14\uafb8\ub294 \ud568\uc218)\ub85c \ud3c9\uade0\uacfc \ud45c\uc900 \ud3b8\ucc28 \ud150\uc11c\ub97c [C x 1 x 1] \ud615\ud0dc\ub85c \ub9cc\ub4e4\uc5b4\n        # \ubc14\ub85c \uc785\ub825 \uc774\ubbf8\uc9c0 \ud150\uc11c\uc758 \ubaa8\uc591\uc778 [B x C x H x W] \uc5d0 \uc5f0\uc0b0\ud560 \uc218 \uc788\ub3c4\ub85d \ub9cc\ub4e4\uc5b4 \uc8fc\uc138\uc694.\n        # B\ub294 \ubc30\uce58 \ud06c\uae30, C\ub294 \ucc44\ub110 \uac12, H\ub294 \ub192\uc774, W\ub294 \ub113\uc774 \uc785\ub2c8\ub2e4.\n\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n\n    def forward(self, img):\n        # img \uac12 \uc815\uaddc\ud654(normalize)\n        return (img - self.mean) / self.std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``\uc21c\ucc28(Sequential)`` \ubaa8\ub4c8\uc5d0\ub294 \ud558\uc704 \ubaa8\ub4c8\uc758 \uc815\ub82c\ub41c \ubaa9\ub85d\uc774 \uc788\uc2b5\ub2c8\ub2e4. \n\uc608\ub97c \ub4e4\uc5b4 ``vgg19.features`` \uc740 vgg19 \uad6c\uc870\uc758 \uc62c\ubc14\ub978 \uc21c\uc11c\ub85c \uc815\ub82c\ub41c \uc21c\uc11c \uc815\ubcf4(Conv2d, ReLU, MaxPool2d, Conv2d, ReLU ...)\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \n\ucf58\ud150\uce20 \ub85c\uc2a4 \uc139\uc158\uc5d0\uc11c \ub9d0\ud588\ub4ef\uc774 \uc6b0\ub9ac\ub294 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc6d0\ud558\ub294 \ub808\uc774\uc5b4\uc5d0 \ucd94\uac00 \ub808\uc774\uc5b4 '\ud22c\uba85(transparent)'\ub808\uc774\uc5b4\ub85c \uc2a4\ud0c0\uc77c \ubc0f \ucf58\ud150\uce20 \uc190\uc2e4 \ubaa8\ub4c8\uc744 \ucd94\uac00\ud558\ub824\uace0 \ud569\ub2c8\ub2e4. \n\uc774\ub97c \uc704\ud574 \uc0c8\ub85c\uc6b4 \uc21c\ucc28 \ubaa8\ub4c8\uc744 \uad6c\uc131\ud569\ub2c8\ub2e4.\uc774 \ubaa8\ub4c8\uc5d0\uc11c\ub294 vgg19\uc758 \ubaa8\ub4c8\uacfc \uc190\uc2e4 \ubaa8\ub4c8\uc744 \uc62c\ubc14\ub978 \uc21c\uc11c\ub85c \ucd94\uac00\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \uc2a4\ud0c0\uc77c/\ucf58\ud150\uce20 \ub85c\uc2a4\ub85c \uacc4\uc0b0\ud558\uae38 \uc6d0\ud558\ub294 \uae4a\uc774\uc758 \ub808\uc774\uc5b4\ub4e4:\ncontent_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    cnn = copy.deepcopy(cnn)\n\n    # \ud45c\uc900\ud654(normalization) \ubaa8\ub4c8\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n\n    # \ub2e8\uc9c0 \ubc18\ubcf5 \uac00\ub2a5\ud55c \uc811\uadfc\uc744 \uac16\uac70\ub098 \ucf58\ud150\uce20/\uc2a4\ud0c0\uc77c\uc758 \ub9ac\uc2a4\ud2b8\ub97c \uac16\uae30 \uc704\ud568\n    # \ub85c\uc2a4\uac12\n    content_losses = []\n    style_losses = []\n\n    # cnn\uc740 nn.Sequential \ud558\ub2e4\uace0 \uac00\uc815\ud558\ubbc0\ub85c, \uc0c8\ub85c\uc6b4 nn.Sequential\uc744 \ub9cc\ub4e4\uc5b4\n    # \uc6b0\ub9ac\uac00 \uc21c\ucc28\uc801\uc73c\ub85c \ud65c\uc131\ud654 \ud558\uace0\uc790\ud558\ub294 \ubaa8\ub4c8\ub4e4\uc744 \ub123\uaca0\uc2b5\ub2c8\ub2e4.\n    model = nn.Sequential(normalization)\n\n    i = 0  # conv\ub808\uc774\uc5b4\ub97c \ucc3e\uc744\ub54c\ub9c8\ub2e4 \uac12\uc744 \uc99d\uac00 \uc2dc\ud0b5\ub2c8\ub2e4\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            # in-place(\uc785\ub825 \uac12\uc744 \uc9c1\uc811 \uc5c5\ub370\uc774\ud2b8) \ubc84\uc804\uc740 \ucf58\ud150\uce20\ub85c\uc2a4\uc640 \uc2a4\ud0c0\uc77c\ub85c\uc2a4\uc5d0\n            # \uc88b\uc740 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uc9c0 \ubabb\ud569\ub2c8\ub2e4.\n            # \uadf8\ub798\uc11c \uc5ec\uae30\uc120 out-of-place\ub85c \ub300\uccb4 \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            # \ucf58\ud150\uce20 \ub85c\uc2a4 \ucd94\uac00:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            # \uc2a4\ud0c0\uc77c \ub85c\uc2a4 \ucd94\uac00:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # \uc774\uc81c \uc6b0\ub9ac\ub294 \ub9c8\uc9c0\ub9c9 \ucf58\ud150\uce20 \ubc0f \uc2a4\ud0c0\uc77c \ub85c\uc2a4 \uc774\ud6c4\uc758 \ub808\uc774\uc5b4\ub4e4\uc744 \uc798\ub77c\ub0c5\ub2c8\ub2e4.\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n   \ub17c\ubb38\uc5d0\uc11c\ub294 \ub9e5\uc2a4 \ud480\ub9c1(Max Pooling) \ub808\uc774\uc5b4\ub97c \uc5d0\ubc84\ub9ac\uc9c0 \ud480\ub9c1(Average Pooling) \ub808\uc774\uc5b4\ub85c \ubc14\uafb8\ub294 \uac83\uc744 \ucd94\ucc9c\ud569\ub2c8\ub2e4.\n   AlexNet\uc5d0\uc11c\ub294 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c VGG19 \ub124\ud2b8\uc6cc\ud06c\ubcf4\ub2e4 \uc0c1\ub300\uc801\uc73c\ub85c \uc791\uc740 \ub124\ud2b8\uc6cc\ud06c\ub77c \uacb0\uacfc \ud488\uc9c8\uc5d0\uc11c \n   \ud070 \ucc28\uc774\ub97c \ud655\uc778\ud558\uae30 \uc5b4\ub824\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n   \uadf8\ub7ec\ub098, \ub9cc\uc57d \ub2f9\uc2e0\uc774 \ub300\uccb4\ud574 \ubcf4\uae30\ub97c \uc6d0\ud55c\ub2e4\uba74 \uc544\ub798 \ucf54\ub4dc\ub4e4\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n   ::\n\n       # avgpool = nn.AvgPool2d(kernel_size=layer.kernel_size,\n       #                         stride=layer.stride, padding = layer.padding)\n       # model.add_module(name,avgpool)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc785\ub825 \uc774\ubbf8\uc9c0\n~~~~~~~~~~~~~~~~~~~\n\n\ub2e4\uc2dc, \ucf54\ub4dc\ub97c \uac04\ub2e8\ud558\uac8c \ud558\uae30 \uc704\ud574, \ucf58\ud150\uce20\uc640 \uc2a4\ud0c0\uc77c \uc774\ubbf8\uc9c0\ub4e4\uc758 \uac19\uc740 \ucc28\uc6d0\uc758 \uc774\ubbf8\uc9c0\ub97c \uac00\uc838\uc635\ub2c8\ub2e4.\n\ud574\ub2f9 \uc774\ubbf8\uc9c0\ub294 \ubc31\uc0c9 \ub178\uc774\uc988\uc77c \uc218 \uc788\uac70\ub098 \ucf58\ud150\uce20-\uc774\ubbf8\uc9c0\uc758 \uac12\ub4e4\uc744 \ubcf5\uc0ac\ud574\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_img = content_img.clone()\n# \ub300\uc2e0\uc5d0 \ubc31\uc0c9 \ub178\uc774\uc988\ub97c \uc774\uc6a9\ud558\uae38 \uc6d0\ud55c\ub2e4\uba74 \uc544\ub798 \uc904\uc758 \uc8fc\uc11d\ucc98\ub9ac\ub97c \uc81c\uac70\ud558\uc138\uc694:\n# input_img = torch.randn(content_img.data.size(), device=device)\n\n# \uc6d0\ubcf8 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ucc3d\uc5d0 \ucd94\uac00\ud569\ub2c8\ub2e4:\nplt.figure()\nimshow(input_img, title='Input Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uacbd\uc0ac \ud558\uac15\ubc95\n~~~~~~~~~~~~~~~~\n\n\uc54c\uace0\ub9ac\uc998\uc758 \uc800\uc790\uc778 Len Gatys \uac00 `\uc5ec\uae30\uc11c <https://discuss.pytorch.org/t/pytorch-tutorial-for-neural-transfert-of-artistic-style/336/20?u=alexis-jacq>`__ \uc81c\uc548\ud55c \ubc29\uc2dd\ub300\ub85c\n\uacbd\uc0ac \ud558\uac15\ubc95\uc744 \uc2e4\ud589\ud558\ub294\ub370 L-BFGS \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9 \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\uc77c\ubc18\uc801\uc778 \ub124\ud2b8\uc6cc\ud06c \ud559\uc2b5\uacfc\ub294 \ub2e4\ub974\uac8c, \uc6b0\ub9ac\ub294 \ucf58\ud150\uce20/\uc2a4\ud0c0\uc77c \ub85c\uc2a4\ub97c \ucd5c\uc18c\ud654 \ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \uc785\ub825 \uc601\uc0c1\uc744 \ud559\uc2b5 \uc2dc\ud0a4\ub824\uace0 \ud569\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 \uac04\ub2e8\ud788 PyTorch L-BFGS \uc635\ud2f0\ub9c8\uc774\uc800 ``optim.LBFGS`` \ub97c \uc0dd\uc131\ud558\ub824\uace0 \ud558\uba70, \ucd5c\uc801\ud654\ub97c \uc704\ud574 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ud150\uc11c \ud0c0\uc785\uc73c\ub85c \uc804\ub2ec\ud569\ub2c8\ub2e4. \n\uc6b0\ub9ac\ub294 ``.requires_grad_()`` \ub97c \uc0ac\uc6a9\ud558\uc5ec \ud574\ub2f9 \uc774\ubbf8\uc9c0\uac00 \uadf8\ub77c\ub514\uc5b8\ud2b8\uac00 \ud544\uc694\ud568\uc744 \ud655\uc2e4\ud558\uac8c \ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_input_optimizer(input_img):\n    # \uc774 \uc904\uc740 \uc785\ub825\uc740 \uadf8\ub808\uc774\ub358\ud2b8\uac00 \ud544\uc694\ud55c \ud30c\ub77c\ubbf8\ud130\ub77c\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \uc788\uc2b5\ub2c8\ub2e4.\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**\ub9c8\uc9c0\ub9c9 \ub2e8\uacc4**: \uacbd\uc0ac \ud558\uac15\uc758 \ubc18\ubcf5. \uac01 \ub2e8\uacc4\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc0c8\ub85c\uc6b4 \ub85c\uc2a4\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574\n\uc5c5\ub370\uc774\ud2b8 \ub41c \uc785\ub825\uc744 \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uacf5\uae09\ud574\uc57c \ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uadf8\ub77c\ub514\uc5b8\ud2b8\ub97c \ub3d9\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\uace0 \n\uadf8\ub77c\ub514\uc5b8\ud2b8 \ub514\uc13c\ud2b8\uc758 \ub2e8\uacc4\ub97c \uc218\ud589\ud558\uae30 \uc704\ud574 \uac01 \uc190\uc2e4\uc758 ``\uc5ed\ubc29\ud5a5(backward)`` \uba54\uc18c\ub4dc\ub97c \uc2e4\ud589\ud574\uc57c \ud569\ub2c8\ub2e4.\n\uc635\ud2f0\ub9c8\uc774\uc800\ub294 \uc778\uc218\ub85c\uc11c \"\ud074\ub85c\uc800(closure)\"\ub97c \ud544\uc694\ub85c \ud569\ub2c8\ub2e4: \uc989, \ubaa8\ub378\uc744 \uc7ac\ud3c9\uac00\ud558\uace0 \ub85c\uc2a4\ub97c \ubc18\ud658 \ud558\ub294 \ud568\uc218\uc785\ub2c8\ub2e4.\n\n\uadf8\ub7ec\ub098, \uc5ec\uae30\uc5d0 \uc791\uc740 \ud568\uc815\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ucd5c\uc801\ud654 \ub41c \uc774\ubbf8\uc9c0\ub294 0 \uacfc 1 \uc0ac\uc774\uc5d0 \uba38\ubb3c\uc9c0 \uc54a\uace0 $-\\infty$\uacfc $+\\infty$ \uc0ac\uc774\uc758 \uac12\uc744 \uac00\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\ub2e4\ub974\uac8c \ub9d0\ud558\uba74, \uc774\ubbf8\uc9c0\ub294 \uc798 \ucd5c\uc801\ud654\ub420 \uc218 \uc788\uace0(0-1 \uc0ac\uc774\uc758 \uc815\ud574\uc9c4 \uac12 \ubc94\uc704\ub0b4\uc758 \uac12\uc744 \uac00\uc9c8 \uc218 \uc788\uace0) \uc774\uc0c1\ud55c \uac12\uc744 \uac00\uc9c8 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \n\uc0ac\uc2e4 \uc6b0\ub9ac\ub294 \uc785\ub825 \uc774\ubbf8\uc9c0\uac00 \uc62c\ubc14\ub978 \ubc94\uc704\uc758 \uac12\uc744 \uc720\uc9c0\ud560 \uc218 \uc788\ub3c4\ub85d \uc81c\uc57d \uc870\uac74 \ud558\uc5d0\uc11c \ucd5c\uc801\ud654\ub97c \uc218\ud589\ud574\uc57c \ud569\ub2c8\ub2e4. \n\uac01 \ub2e8\uacc4\ub9c8\ub2e4 0-1 \uac04\uaca9\uc73c\ub85c \uac12\uc744 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc774\ubbf8\uc9c0\ub97c \uc218\uc815\ud558\ub294 \uac04\ub2e8\ud55c \ud574\uacb0\ucc45\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=300,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"\uc2a4\ud0c0\uc77c \ubcc0\ud658\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            # \uc785\ub825 \uc774\ubbf8\uc9c0\uc758 \uc5c5\ub370\uc774\ud2b8\ub41c \uac12\ub4e4\uc744 \ubcf4\uc815\ud569\ub2c8\ub2e4\n            input_img.data.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            style_score *= style_weight\n            content_score *= content_weight\n\n            loss = style_score + content_score\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # \ub9c8\uc9c0\ub9c9 \ubcf4\uc815...\n    input_img.data.clamp_(0, 1)\n\n    return input_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uc54c\uace0\ub9ac\uc998\uc744 \uc2e4\ud589 \uc2dc\ud0b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)\n\nplt.figure()\nimshow(output, title='Output Image')\n\n# sphinx_gallery_thumbnail_number = 4\nplt.ioff()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}