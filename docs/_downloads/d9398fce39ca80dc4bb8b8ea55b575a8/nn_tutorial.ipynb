{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# `torch.nn` \uc774 *\uc2e4\uc81c\ub85c* \ubb34\uc5c7\uc778\uac00\uc694?\n\n**\uc800\uc790**: Jeremy Howard, [fast.ai](https://www.fast.ai).  Rachel Thomas, Francisco Ingham\uc5d0 \uac10\uc0ac\ud569\ub2c8\ub2e4.\n\n**\ubc88\uc5ed**: [\ub0a8\uc0c1\ud638](https://github.com/namdori61)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2a4\ud06c\ub9bd\ud2b8\uac00 \uc544\ub2cc \ub178\ud2b8\ubd81\uc73c\ub85c \uc2e4\ud589\ud558\uae30\ub97c \uad8c\uc7a5\ud569\ub2c8\ub2e4. \ub178\ud2b8\ubd81 (``.ipynb``) \ud30c\uc77c\uc744 \ub2e4\uc6b4 \ubc1b\uc73c\uc2dc\ub824\uba74,\n\ud398\uc774\uc9c0 \uc0c1\ub2e8\uc5d0 \uc788\ub294 \ub9c1\ud06c\ub97c \ud074\ub9ad\ud574\uc8fc\uc138\uc694.\n\nPyTorch \ub294 \uc5ec\ub7ec\ubd84\uc774 \uc2e0\uacbd\ub9dd(neural network)\ub97c \uc0dd\uc131\ud558\uace0 \ud559\uc2b5\uc2dc\ud0a4\ub294 \uac83\uc744 \ub3c4\uc640\uc8fc\uae30 \uc704\ud574\uc11c\n[torch.nn](https://pytorch.org/docs/stable/nn.html) ,\n[torch.optim](https://pytorch.org/docs/stable/optim.html) ,\n[Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) ,\n\uadf8\ub9ac\uace0 [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n\uc640 \uac19\uc740 \uc798 \ub514\uc790\uc778\ub41c \ubaa8\ub4c8\uacfc \ud074\ub798\uc2a4\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\uc774\ub4e4\uc758 \uc131\ub2a5\uc744 \ucd5c\ub300\ud55c \ud65c\uc6a9\ud558\uace0 \uc5ec\ub7ec\ubd84\uc758 \ubb38\uc81c\uc5d0 \ub9de\uac8c \ucee4\uc2a4\ud130\ub9c8\uc774\uc988\ud558\uae30 \uc704\ud574\uc11c,\n\uc815\ud655\ud788 \uc774\ub4e4\uc774 \uc5b4\ub5a4 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294\uc9c0 \uc774\ud574\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774\ud574\ub97c \uc99d\uc9c4\ud558\uae30 \uc704\ud574\uc11c, \uc6b0\ub9ac\ub294 \uba3c\uc800 \uc774\ub4e4 \ubaa8\ub378\ub4e4\ub85c\ubd80\ud130 \uc544\ubb34 \ud53c\uccd0\ub3c4 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0\nMNIST \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \uae30\ucd08\uc801\uc778 \uc2e0\uacbd\ub9dd\uc744 \ud559\uc2b5\uc2dc\ud0ac \uac83\uc785\ub2c8\ub2e4;\n\uc6b0\ub9ac\ub294 \ucc98\uc74c\uc5d0\ub294 \uac00\uc7a5 \uae30\ucd08\uc801\uc778 PyTorch \ud150\uc11c(tensor) \uae30\ub2a5\ub9cc\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uadf8\ub9ac\uace0\ub098\uc11c \uc6b0\ub9ac\ub294 \uc810\ucc28\uc801\uc73c\ub85c ``torch.nn``, ``torch.optim``, ``Dataset``, \ub610\ub294\n``DataLoader`` \ub85c\ubd80\ud130 \ud55c\ubc88\uc5d0 \ud558\ub098\uc529 \ud53c\uccd0\ub97c \ucd94\uac00\ud558\uba74\uc11c, \uc815\ud655\ud788 \uac01 \ubd80\ubd84\uc774 \uc5b4\ub5a4 \uc77c\uc744 \ud558\ub294\uc9c0 \uadf8\ub9ac\uace0\n\uc774\uac83\uc774 \uc5b4\ub5bb\uac8c \ucf54\ub4dc\ub97c \ub354 \uac04\uacb0\ud558\uace0 \uc720\uc5f0\ud558\uac8c \ub9cc\ub4dc\ub294\uc9c0 \ubcf4\uc5ec\uc904 \uac83\uc785\ub2c8\ub2e4.\n\n**\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 \uc5ec\ub7ec\ubd84\uc774 \uc774\ubbf8 PyTorch\ub97c \uc124\uce58\ud558\uc600\uace0, \uadf8\ub9ac\uace0 \ud150\uc11c \uc5f0\uc0b0\uc758 \uae30\ucd08\uc5d0 \ub300\ud574 \uc775\uc219\ud558\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4.**\n(\ub9cc\uc57d \uc5ec\ub7ec\ubd84\uc774 NumPy \ubc30\uc5f4(array) \uc5f0\uc0b0\uc5d0 \uc775\uc219\ud558\ub2e4\uba74, \uc5ec\uae30\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 PyTorch \ud150\uc11c \uc5f0\uc0b0\ub3c4\n\uac70\uc758 \ub3d9\uc77c\ud558\ub2e4\ub294 \uac83\uc744 \uc54c\uac8c \ub420 \uac83\uc785\ub2c8\ub2e4).\n\n## MNIST \ub370\uc774\ud130 \uc900\ube44\n\n\uc6b0\ub9ac\ub294 \uc190\uc73c\ub85c \uc4f4 \uc22b\uc790(0\uc5d0\uc11c 9 \uc0ac\uc774)\uc758 \ud751\ubc31 \uc774\ubbf8\uc9c0\ub85c \uad6c\uc131\ub41c \ud074\ub798\uc2dd\n[MNIST](http://deeplearning.net/data/mnist/) \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud560 \uac83 \uc785\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 \uacbd\ub85c \uc124\uc815\uc744 \ub2f4\ub2f9\ud558\ub294 (Python3 \ud45c\uc900 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \uc77c\ubd80\uc778)\n[pathlib](https://docs.python.org/3/library/pathlib.html) \uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\uace0,\n[requests](http://docs.python-requests.org/en/master/) \ub97c \uc774\uc6a9\ud558\uc5ec\n\ub370\uc774\ud130\uc14b\uc744 \ub2e4\uc6b4\ub85c\ub4dc \ud560 \uac83\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud560 \ub54c\ub9cc \uc784\ud3ec\ud2b8(import) \ud560 \uac83\uc774\ubbc0\ub85c,\n\uc5ec\ub7ec\ubd84\uc740 \ub9e4 \ud3ec\uc778\ud2b8\ub9c8\ub2e4 \uc815\ud655\ud788 \uc5b4\ub5a4 \uac83\uc774 \uc0ac\uc6a9\ub418\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport requests\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774 \ub370\uc774\ud130\uc14b\uc740 NumPy \ubc30\uc5f4 \ud3ec\ub9f7\uc774\uace0, \ub370\uc774\ud130\ub97c \uc9c1\ub82c\ud654\ud558\uae30 \uc704\ud55c\npython \uc804\uc6a9 \ud3ec\ub9f7 pickle \uc744 \uc774\uc6a9\ud558\uc5ec \uc800\uc7a5\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\nimport gzip\n\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac01 \uc774\ubbf8\uc9c0\ub294 28 x 28 \ud615\ud0dc \uc774\uace0, 784 (=28x28) \ud06c\uae30\ub97c \uac00\uc9c4 \ud558\ub098\uc758 \ud589\uc73c\ub85c \uc800\uc7a5\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\ud558\ub098\ub97c \uc0b4\ud3b4 \ubd05\uc2dc\ub2e4; \uba3c\uc800 \uc6b0\ub9ac\ub294 \uc774 \uc774\ubbf8\uc9c0\ub97c 2d\ub85c \uc7ac\uad6c\uc131\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n# Colab\uc774 \uc544\ub2cc \uacbd\uc6b0\uc5d0\ub9cc ``pyplot.show()``\ntry:\n    import google.colab\nexcept ImportError:\n    pyplot.show()\nprint(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch\ub294 NumPy \ubc30\uc5f4 \ubcf4\ub2e4\ub294 ``torch.tensor`` \ub97c \uc0ac\uc6a9\ud558\ubbc0\ub85c, \uc6b0\ub9ac\ub294 \ub370\uc774\ud130\ub97c \ubcc0\ud658\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nn, c = x_train.shape\nprint(x_train, y_train)\nprint(x_train.shape)\nprint(y_train.min(), y_train.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (``torch.nn`` \uc5c6\uc774) \ubc11\ubc14\ub2e5\ubd80\ud130 \uc2e0\uacbd\ub9dd \ub9cc\ub4e4\uae30\n\nPyTorch \ud150\uc11c \uc5f0\uc0b0\ub9cc\uc73c\ub85c \uccab \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4.\n\uc5ec\ub7ec\ubd84\uc774 \uc2e0\uacbd\ub9dd\uc758 \uae30\ucd08\uc5d0 \ub300\ud574\uc11c \uc774\ubbf8 \uc775\uc219\ud558\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4.\n(\ub9cc\uc57d \uc775\uc219\ud558\uc9c0 \uc54a\ub2e4\uba74 [course.fast.ai](https://course.fast.ai) \uc5d0\uc11c \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4).\n\nPyTorch\ub294 \ub79c\ub364 \ub610\ub294 0\uc73c\ub85c\ub9cc \uc774\ub8e8\uc5b4\uc9c4 \ud150\uc11c\ub97c \uc0dd\uc131\ud558\ub294 \uba54\uc18c\ub4dc\ub97c \uc81c\uacf5\ud558\uace0,\n\uc6b0\ub9ac\ub294 \uac04\ub2e8\ud55c \uc120\ud615 \ubaa8\ub378\uc758 \uac00\uc911\uce58(weights)\uc640 \uc808\ud3b8(bias)\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574\uc11c \uc774\uac83\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc774\ub4e4\uc740 \uc77c\ubc18\uc801\uc778 \ud150\uc11c\uc5d0 \ub9e4\uc6b0 \ud2b9\ubcc4\ud55c \ud55c \uac00\uc9c0\uac00 \ucd94\uac00\ub41c \uac83\uc785\ub2c8\ub2e4: \uc6b0\ub9ac\ub294 PyTorch\uc5d0\uac8c \uc774\ub4e4\uc774\n\uae30\uc6b8\uae30(gradient)\uac00 \ud544\uc694\ud558\ub2e4\uace0 \uc54c\ub824\uc90d\ub2c8\ub2e4.\n\uc774\ub97c \ud1b5\ud574 PyTorch\ub294 \ud150\uc11c\uc5d0 \ud589\ud574\uc9c0\ub294 \ubaa8\ub4e0 \uc5f0\uc0b0\uc744 \uae30\ub85d\ud558\uac8c \ud558\uace0,\n\ub530\ub77c\uc11c *\uc790\ub3d9\uc801\uc73c\ub85c* \uc5ed\uc804\ud30c(back-propagation) \ub3d9\uc548\uc5d0 \uae30\uc6b8\uae30\ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4!\n\n\uac00\uc911\uce58\uc5d0 \ub300\ud574\uc11c\ub294 ``requires_grad`` \ub97c \ucd08\uae30\ud654(initialization) **\ub2e4\uc74c\uc5d0** \uc124\uc815\ud569\ub2c8\ub2e4,\n\uc65c\ub0d0\ud558\uba74 \uc6b0\ub9ac\ub294 \ud574\ub2f9 \ub2e8\uacc4\uac00 \uae30\uc6b8\uae30\uc5d0 \ud3ec\ud568\ub418\ub294 \uac83\uc744 \uc6d0\uce58 \uc54a\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n(PyTorch\uc5d0\uc11c ``_`` \ub2e4\uc74c\uc5d0 \uc624\ub294 \uba54\uc18c\ub4dc \uc774\ub984\uc740 \uc5f0\uc0b0\uc774 \uc778\ud50c\ub808\uc774\uc2a4(in-place)\ub85c \uc218\ud589\ub418\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.)\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>[Xavier initialisation](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n   \uae30\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \uac00\uc911\uce58\ub97c \ucd08\uae30\ud654 \ud569\ub2c8\ub2e4. (``1/sqrt(n)`` \uc744 \uacf1\ud574\uc11c \ucd08\uae30\ud654).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\n\nweights = torch.randn(784, 10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch\uc758 \uae30\uc6b8\uae30\ub97c \uc790\ub3d9\uc73c\ub85c \uacc4\uc0b0\ud574\uc8fc\ub294 \uae30\ub2a5 \ub355\ubd84\uc5d0, Python \ud45c\uc900 \ud568\uc218\n(\ub610\ub294 \ud638\ucd9c \uac00\ub2a5\ud55c \uac1d\uccb4)\ub97c \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4!\n\uadf8\ub7ec\ubbc0\ub85c \uac04\ub2e8\ud55c \uc120\ud615 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574\uc11c \ub2e8\uc21c\ud55c \ud589\ub82c \uacf1\uc148\uacfc \ube0c\ub85c\ub4dc\uce90\uc2a4\ud2b8(broadcast)\n\ub367\uc148\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc6b0\ub9ac\ub294 \ud65c\uc131\ud654 \ud568\uc218(activation function)\uac00 \ud544\uc694\ud558\ubbc0\ub85c,\n`log_softmax` \ub97c \uad6c\ud604\ud558\uace0 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\nPyTorch\uc5d0\uc11c \ub9ce\uc740 \uc0ac\uc804 \uad6c\ud604\ub41c \uc190\uc2e4 \ud568\uc218(loss function), \ud65c\uc131\ud654 \ud568\uc218\ub4e4\uc774 \uc81c\uacf5\ub418\uc9c0\ub9cc,\n\uc77c\ubc18\uc801\uc778 python\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc790\uc2e0\ub9cc\uc758 \ud568\uc218\ub97c \uc27d\uac8c \uc791\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uae30\uc5b5\ud574\uc8fc\uc138\uc694.\nPyTorch\ub294 \uc2ec\uc9c0\uc5b4 \uc5ec\ub7ec\ubd84\uc758 \ud568\uc218\ub97c \uc704\ud574\uc11c \ube60\ub978 GPU \ub610\ub294 \ubca1\ud130\ud654\ub41c CPU \ucf54\ub4dc\ub97c \ub9cc\ub4e4\uc5b4\uc904 \uac83\uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc704\uc5d0\uc11c, ``@`` \uae30\ud638\ub294 \ud589\ub82c \uacf1\uc148(matrix multiplication) \uc5f0\uc0b0\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 \ud558\ub098\uc758 \ubc30\uce58(batch) \ub370\uc774\ud130(\uc774 \uacbd\uc6b0\uc5d0\ub294 64\uac1c\uc758 \uc774\ubbf8\uc9c0\ub4e4)\uc5d0 \ub300\ud558\uc5ec \ud568\uc218\ub97c \ud638\ucd9c\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc774\uac83\uc740 \ud558\ub098\uc758 *\ud3ec\uc6cc\ub4dc \uc804\ub2ec(forward pass)* \uc785\ub2c8\ub2e4. \uc774 \ub2e8\uacc4\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ubb34\uc791\uc704(random) \uac00\uc911\uce58\ub85c\n\uc2dc\uc791\ud588\uae30 \ub54c\ubb38\uc5d0 \uc6b0\ub9ac\uc758 \uc608\uce21\uc774 \ubb34\uc791\uc704 \uc608\uce21\ubcf4\ub2e4 \uc804\ud600 \ub098\uc740 \uc810\uc774 \uc5c6\uc744 \uac83\uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bs = 64  # \ubc30\uce58 \ud06c\uae30\n\nxb = x_train[0:bs]  # x\ub85c\ubd80\ud130 \ubbf8\ub2c8\ubc30\uce58(mini-batch) \ucd94\ucd9c\npreds = model(xb)  # \uc608\uce21\npreds[0], preds.shape\nprint(preds[0], preds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc5ec\ub7ec\ubd84\uc774 \ubcf4\uc2dc\ub4ef\uc774, ``preds`` \ud150\uc11c(tensor)\ub294 \ud150\uc11c \uac12 \uc678\uc5d0\ub3c4, \ub610\ud55c\n\uae30\uc6b8\uae30 \ud568\uc218(gradient function)\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 \ub098\uc911\uc5d0 \uc774\uac83\uc744 \uc5ed\uc804\ud30c(backpropagation)\ub97c \uc704\ud574 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc774\uc81c \uc190\uc2e4\ud568\uc218(loss function)\ub85c \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \uc74c\uc758 \ub85c\uadf8 \uc6b0\ub3c4(negative log-likelihood)\ub97c\n\uad6c\ud604\ud569\uc2dc\ub2e4. (\ub2e4\uc2dc \ub9d0\ud558\uc9c0\ub9cc, \uc6b0\ub9ac\ub294 \ud45c\uc900 Python\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\n\nloss_func = nll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc6b0\ub9ac\uc758 \ubb34\uc791\uc704 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \uc810\uac80\ud574\ubd05\uc2dc\ub2e4, \uadf8\ub7fc\uc73c\ub85c\uc368 \uc6b0\ub9ac\ub294 \ub098\uc911\uc5d0 \uc5ed\uc804\ud30c \uc774\ud6c4\uc5d0 \uac1c\uc120\uc774 \uc788\ub294\uc9c0\n\ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "yb = y_train[0:bs]\nprint(loss_func(preds, yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub610\ud55c, \uc6b0\ub9ac \ubaa8\ub378\uc758 \uc815\ud655\ub3c4(accuracy)\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud55c \ud568\uc218\ub97c \uad6c\ud604\ud569\uc2dc\ub2e4.\n\ub9e4 \uc608\uce21\ub9c8\ub2e4, \ub9cc\uc57d \uac00\uc7a5 \ud070 \uac12\uc758 \uc778\ub371\uc2a4\uac00 \ubaa9\ud45c\uac12(target value)\uacfc \ub3d9\uc77c\ud558\ub2e4\uba74,\n\uadf8 \uc608\uce21\uc740 \uc62c\ubc14\ub978 \uac83\uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds == yb).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc6b0\ub9ac\uc758 \ubb34\uc791\uc704 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \uc810\uac80\ud574 \ubd05\uc2dc\ub2e4, \uadf8\ub7fc\uc73c\ub85c\uc368 \uc190\uc2e4\uc774 \uac1c\uc120\ub428\uc5d0 \ub530\ub77c\uc11c \uc815\ud655\ub3c4\uac00 \uac1c\uc120\ub418\ub294\uc9c0\n\ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(accuracy(preds, yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \uc6b0\ub9ac\ub294 \ud6c8\ub828 \ub8e8\ud504(training loop)\ub97c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9e4 \ubc18\ubcf5\ub9c8\ub2e4, \uc6b0\ub9ac\ub294 \ub2e4\uc74c\uc744 \uc218\ud589\ud560 \uac83\uc785\ub2c8\ub2e4:\n\n- \ub370\uc774\ud130\uc758 \ubbf8\ub2c8\ubc30\uce58\ub97c \uc120\ud0dd (``bs`` \ud06c\uae30)\n- \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\uc5ec \uc608\uce21 \uc218\ud589\n- \uc190\uc2e4 \uacc4\uc0b0\n- ``loss.backward()`` \ub97c \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uae30\uc6b8\uae30 \uc5c5\ub370\uc774\ud2b8, \uc774 \uacbd\uc6b0\uc5d0\ub294, ``weights`` \uc640 ``bias``.\n\n\uc774\uc81c \uc6b0\ub9ac\ub294 \uc774 \uae30\uc6b8\uae30\ub4e4\uc744 \uc774\uc6a9\ud558\uc5ec \uac00\uc911\uce58\uc640 \uc808\ud3b8\uc744 \uc5c5\ub370\uc774\ud2b8 \ud569\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 \uc774\uac83\uc744 ``torch.no_grad()`` \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc838(context manager) \ub0b4\uc5d0\uc11c \uc2e4\ud589\ud569\ub2c8\ub2e4,\n\uc65c\ub0d0\ud558\uba74 \uc774\ub7ec\ud55c \uc2e4\ud589\uc774 \ub2e4\uc74c \uae30\uc6b8\uae30\uc758 \uacc4\uc0b0\uc5d0 \uae30\ub85d\ub418\uc9c0 \uc54a\uae30\ub97c \uc6d0\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\nPyTorch\uc758 \uc790\ub3d9 \uae30\uc6b8\uae30(Autograd)\uac00 \uc5b4\ub5bb\uac8c \uc5f0\uc0b0\uc744 \uae30\ub85d\ud558\ub294\uc9c0\n[\uc5ec\uae30](https://pytorch.org/docs/stable/notes/autograd.html) \uc5d0\uc11c \ub354 \uc54c\uc544\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 \uadf8\ub7ec\uace0\ub098\uc11c \uae30\uc6b8\uae30\ub97c 0\uc73c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4, \uadf8\ub7fc\uc73c\ub85c\uc368 \ub2e4\uc74c \ub8e8\ud504(loop)\uc5d0 \uc900\ube44\ud558\uac8c \ub429\ub2c8\ub2e4.\n\uadf8\ub807\uc9c0 \uc54a\uc73c\uba74, \uc6b0\ub9ac\uc758 \uae30\uc6b8\uae30\ub4e4\uc740 \uc77c\uc5b4\ub09c \ubaa8\ub4e0 \uc5f0\uc0b0\uc758 \ub204\uc801 \uc9d1\uacc4\ub97c \uae30\ub85d\ud558\uac8c \ub418\ubc84\ub9bd\ub2c8\ub2e4.\n(\uc989, ``loss.backward()`` \uac00 \uc774\ubbf8 \uc800\uc7a5\ub41c \uac83\uc744 \ub300\uccb4\ud558\uae30\ubcf4\ub2e8, \uae30\uc874 \uac12\uc5d0 \uae30\uc6b8\uae30\ub97c *\ub354\ud558\uac8c* \ub429\ub2c8\ub2e4).\n\n.. tip:: \uc5ec\ub7ec\ubd84\ub4e4\uc740 PyTorch \ucf54\ub4dc\uc5d0 \ub300\ud558\uc5ec \ud45c\uc900 python \ub514\ubc84\uac70(debugger)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\ubbc0\ub85c,\n   \ub9e4 \ub2e8\uacc4\ub9c8\ub2e4 \ub2e4\uc591\ud55c \ubcc0\uc218 \uac12\uc744 \uc810\uac80\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n   \uc544\ub798\uc5d0\uc11c ``set_trace()`` \ub97c \uc8fc\uc11d \ud574\uc81c\ud558\uc5ec \uc0ac\uc6a9\ud574\ubcf4\uc138\uc694.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.core.debugger import set_trace\n\nlr = 0.5  # \ud559\uc2b5\ub960(learning rate)\nepochs = 2  # \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ud560 \uc5d0\ud3ed(epoch) \uc218\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        with torch.no_grad():\n            weights -= weights.grad * lr\n            bias -= bias.grad * lr\n            weights.grad.zero_()\n            bias.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \ub2e4 \ub410\uc2b5\ub2c8\ub2e4: \uc6b0\ub9ac\ub294 \uc81c\uc77c \uac04\ub2e8\ud55c \uc2e0\uacbd\ub9dd(neural network)\uc758 \ubaa8\ub4e0 \uac83\uc744 \ubc11\ubc14\ub2e5\ubd80\ud130 \uc0dd\uc131\ud558\uace0\n\ud6c8\ub828\ud558\uc600\uc2b5\ub2c8\ub2e4! (\uc774\ubc88\uc5d0\ub294 \uc740\ub2c9\uce35(hidden layer)\uc774 \uc5c6\uae30 \ub54c\ubb38\uc5d0,\n\ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0(logistic regression)\uc785\ub2c8\ub2e4).\n\n\uc774\uc81c \uc190\uc2e4\uacfc \uc815\ud655\ub3c4\ub97c \uc774\uc804 \uac12\ub4e4\uacfc \ube44\uad50\ud558\uba74\uc11c \ud655\uc778\ud574\ubd05\uc2dc\ub2e4.\n\uc6b0\ub9ac\ub294 \uc190\uc2e4\uc740 \uac10\uc18c\ud558\uace0, \uc815\ud655\ub3c4\ub294 \uc99d\uac00\ud558\uae30\ub97c \uae30\ub300\ud560 \uac83\uc774\uace0, \uadf8\ub4e4\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``torch.nn.functional`` \uc0ac\uc6a9\ud558\uae30\n\n\uc774\uc81c \uc6b0\ub9ac\ub294 \ucf54\ub4dc\ub97c \ub9ac\ud329\ud1a0\ub9c1(refactoring) \ud558\uaca0\uc2b5\ub2c8\ub2e4, \uadf8\ub7fc\uc73c\ub85c\uc368 \uc774\uc804\uacfc \ub3d9\uc77c\ud558\uc9c0\ub9cc,\nPyTorch\uc758 ``nn`` \ud074\ub798\uc2a4\uc758 \uc7a5\uc810\uc744 \ud65c\uc6a9\ud558\uc5ec \ub354 \uac04\uacb0\ud558\uace0 \uc720\uc5f0\ud558\uac8c \ub9cc\ub4e4 \uac83\uc785\ub2c8\ub2e4.\n\uc9c0\uae08\ubd80\ud130 \ub9e4 \ub2e8\uacc4\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ucf54\ub4dc\ub97c \ub354 \uc9e7\uace0, \uc774\ud574\ud558\uae30 \uc27d\uace0, \uc720\uc5f0\ud558\uac8c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n\n\ucc98\uc74c\uc774\uba74\uc11c \uc6b0\ub9ac\uc758 \ucf54\ub4dc\ub97c \uc9e7\uac8c \ub9cc\ub4e4\uae30 \uac00\uc7a5 \uc26c\uc6b4 \ub2e8\uacc4\ub294 \uc9c1\uc811 \uc791\uc131\ud55c \ud65c\uc131\ud654, \uc190\uc2e4 \ud568\uc218\ub97c\n``torch.nn.functional`` \uc758 \ud568\uc218\ub85c \ub300\uccb4\ud558\ub294 \uac83\uc785\ub2c8\ub2e4\n(\uad00\ub840\uc5d0 \ub530\ub77c, \uc77c\ubc18\uc801\uc73c\ub85c ``F`` \ub124\uc784\uc2a4\ud398\uc774\uc2a4(namespace)\ub97c \ud1b5\ud574 \uc784\ud3ec\ud2b8(import) \ud569\ub2c8\ub2e4).\n\uc774 \ubaa8\ub4c8\uc5d0\ub294 ``torch.nn`` \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ubaa8\ub4e0 \ud568\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4\n(\ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ub2e4\ub978 \ubd80\ubd84\uc5d0\ub294 \ud074\ub798\uc2a4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.)\n\ub2e4\uc591\ud55c \uc190\uc2e4 \ubc0f \ud65c\uc131\ud654 \ud568\uc218 \ubfd0\ub9cc \uc544\ub2c8\ub77c, \ud480\ub9c1(pooling) \ud568\uc218\uc640 \uac19\uc774 \uc2e0\uacbd\ub9dd\uc744 \ub9cc\ub4dc\ub294\ub370\n\ud3b8\ub9ac\ud55c \uba87 \uac00\uc9c0 \ud568\uc218\ub3c4 \uc5ec\uae30\uc5d0\uc11c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n(\ucee8\ubcfc\ub8e8\uc158(convolution) \uc5f0\uc0b0, \uc120\ud615(linear) \ub808\uc774\uc5b4, \ub4f1\uc744 \uc218\ud589\ud558\ub294 \ud568\uc218\ub3c4 \uc788\uc9c0\ub9cc,\n\uc55e\uc73c\ub85c \ubcf4\uc2dc\uaca0\uc9c0\ub9cc \uc77c\ubc18\uc801\uc73c\ub85c \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ub2e4\ub978 \ubd80\ubd84\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub354 \uc798 \ucc98\ub9ac \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.)\n\n\ub9cc\uc57d \uc5ec\ub7ec\ubd84\ub4e4\uc774 \uc74c\uc758 \ub85c\uadf8 \uc6b0\ub3c4 \uc190\uc2e4\uacfc \ub85c\uadf8 \uc18c\ud504\ud2b8\ub9e5\uc2a4 (log softmax) \ud65c\uc131\ud654 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0,\nPyTorch\ub294 \uc774 \ub458\uc744 \uacb0\ud569\ud558\ub294 \ub2e8\uc77c \ud568\uc218\uc778 ``F.cross_entropy`` \ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\ub530\ub77c\uc11c \ubaa8\ub378\uc5d0\uc11c \ud65c\uc131\ud654 \ud568\uc218\ub97c \uc81c\uac70\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n\nloss_func = F.cross_entropy\n\ndef model(xb):\n    return xb @ weights + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub354\uc774\uc0c1 ``model`` \ud568\uc218\uc5d0\uc11c ``log_softmax`` \ub97c \ud638\ucd9c\ud558\uc9c0 \uc54a\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\uc190\uc2e4\uacfc \uc815\ud655\ub3c4\uacfc \uc774\uc804\uacfc \ub3d9\uc77c\ud55c\uc9c0 \ud655\uc778\ud574\ubd05\uc2dc\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``nn.Module`` \uc744 \uc774\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1 \ud558\uae30\n\ub2e4\uc74c\uc73c\ub85c, \ub354 \uba85\ud655\ud558\uace0 \uac04\uacb0\ud55c \ud6c8\ub828 \ub8e8\ud504\ub97c \uc704\ud574 ``nn.Module`` \ubc0f ``nn.Parameter`` \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\uc6b0\ub9ac\ub294 ``nn.Module`` (\uc790\uccb4\uac00 \ud074\ub798\uc2a4\uc774\uace0 \uc0c1\ud0dc\ub97c \ucd94\ucc99\ud560 \uc218 \uc788\ub294) \ud558\uc704 \ud074\ub798\uc2a4(subclass)\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n\uc774 \uacbd\uc6b0\uc5d0\ub294, \ud3ec\uc6cc\ub4dc(forward) \ub2e8\uacc4\uc5d0 \ub300\ud55c \uac00\uc911\uce58, \uc808\ud3b8, \uadf8\ub9ac\uace0 \uba54\uc18c\ub4dc(method) \ub4f1\uc744 \uc720\uc9c0\ud558\ub294\n\ud074\ub798\uc2a4\ub97c \ub9cc\ub4e4\uace0\uc790 \ud569\ub2c8\ub2e4.\n``nn.Module`` \uc740 \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud560 \uba87 \uac00\uc9c0 \uc18d\uc131(attribute)\uacfc \uba54\uc18c\ub4dc\ub97c (``.parameters()`` \uc640\n``.zero_grad()`` \uac19\uc740) \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>``nn.Module`` (\ub300\ubb38\uc790 M) \uc740 PyTorch \uc758 \ud2b9\uc815 \uac1c\ub150\uc774\uace0, \uc6b0\ub9ac\ub294 \uc774 \ud074\ub798\uc2a4\ub97c\n   \ub9ce\uc774 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. ``nn.Module`` \ub97c Python \uc758 \ucf54\ub4dc\ub97c \uc784\ud3ec\ud2b8\ud558\uae30 \uc704\ud55c \ucf54\ub4dc \ud30c\uc77c\uc778\n   [module](https://docs.python.org/3/tutorial/modules.html) (\uc18c\ubb38\uc790 ``m``)\n   \uc758 \uac1c\ub150\uacfc \ud5f7\uac08\ub9ac\uc9c0 \ub9d0\uc544\uc8fc\uc138\uc694.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn\n\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        self.bias = nn.Parameter(torch.zeros(10))\n\n    def forward(self, xb):\n        return xb @ self.weights + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \ub300\uc2e0\uc5d0 \uc774\uc81c\ub294 \uc624\ube0c\uc81d\ud2b8(object) \ub97c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0,\n\uba3c\uc800 \ubaa8\ub378\uc744 \uc778\uc2a4\ud134\uc2a4\ud654(instantiate) \ud574\uc57c \ud569\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Mnist_Logistic()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \uc6b0\ub9ac\ub294 \uc774\uc804\uacfc \ub3d9\uc77c\ud55c \ubc29\uc2dd\uc73c\ub85c \uc190\uc2e4\uc744 \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc5ec\uae30\uc11c ``nn.Module`` \uc624\ube0c\uc81d\ud2b8\ub4e4\uc740 \ub9c8\uce58 \ud568\uc218\ucc98\ub7fc \uc0ac\uc6a9\ub429\ub2c8\ub2e4 (\uc989, \uc774\ub4e4\uc740 *\ud638\ucd9c\uac00\ub2a5* \ud569\ub2c8\ub2e4),\n\uadf8\ub7ec\ub098 \ubc30\ud6c4\uc5d0\uc11c PyTorch \ub294 \uc6b0\ub9ac\uc758 ``forward`` \uba54\uc18c\ub4dc\ub97c \uc790\ub3d9\uc73c\ub85c \ud638\ucd9c\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss_func(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc804\uc5d0\ub294 \ud6c8\ub828 \ub8e8\ud504\ub97c \uc704\ud574 \uc774\ub984 \ubcc4\ub85c \uac01 \ub9e4\uac1c\ubcc0\uc218(parameter)\uc758 \uac12\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\uace0 \ub2e4\uc74c\uacfc \uac19\uc774\n\uac01 \ub9e4\uac1c \ubcc0\uc218\uc5d0 \ub300\ud55c \uae30\uc6b8\uae30\ub4e4\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \uc218\ub3d9\uc73c\ub85c 0\uc73c\ub85c \uc81c\uac70\ud574\uc57c \ud588\uc2b5\ub2c8\ub2e4:\n\n```python\nwith torch.no_grad():\n    weights -= weights.grad * lr\n    bias -= bias.grad * lr\n    weights.grad.zero_()\n    bias.grad.zero_()\n```\n\uc774\uc81c \uc6b0\ub9ac\ub294 model.parameters() \ubc0f model.zero_grad() (\ubaa8\ub450\n``nn.Module`` \uc5d0 \ub300\ud574 PyTorch\uc5d0 \uc758\ud574 \uc815\uc758\ub428)\ub97c \ud65c\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c \ub2e8\uacc4\ub97c \ub354 \uac04\uacb0\ud558\uac8c\n\ub9cc\ub4e4\uace0, \ud2b9\ud788 \ub354 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c \uc77c\ubd80 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc78a\uc5b4 \ubc84\ub9ac\ub294 \uc624\ub958\ub97c \ub35c \ubc1c\uc0dd\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n```python\nwith torch.no_grad():\n    for p in model.parameters(): p -= p.grad * lr\n    model.zero_grad()\n```\n\uc774\uc81c \uc774\uac83\uc744 \ub098\uc911\uc5d0 \ub2e4\uc2dc \uc2e4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d ``fit`` \ud568\uc218\ub85c \uc791\uc740 \ud6c8\ub828 \ub8e8\ud504\ub97c \uac10\uc300 \uac83\uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fit():\n    for epoch in range(epochs):\n        for i in range((n - 1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n            xb = x_train[start_i:end_i]\n            yb = y_train[start_i:end_i]\n            pred = model(xb)\n            loss = loss_func(pred, yb)\n\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters():\n                    p -= p.grad * lr\n                model.zero_grad()\n\nfit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc190\uc2e4\uc774 \uc904\uc5b4\ub4e4\uc5c8\ub294\uc9c0 \ub2e4\uc2dc \ud55c\ubc88 \ud655\uc778\ud569\uc2dc\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss_func(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``nn.Linear`` \ub97c \uc0ac\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1 \ud558\uae30\n\n\uacc4\uc18d\ud574\uc11c \ucf54\ub4dc\ub97c \ub9ac\ud329\ud1a0\ub9c1 \ud569\ub2c8\ub2e4. ``self.weights`` \ubc0f ``self.bias`` \ub97c \uc218\ub3d9\uc73c\ub85c \uc815\uc758 \ubc0f\n\ucd08\uae30\ud654\ud558\uace0, ``xb  @ self.weights + self.bias`` \ub97c \uacc4\uc0b0\ud558\ub294 \ub300\uc2e0\uc5d0,\n\uc704\uc758 \ubaa8\ub4e0 \uac83\uc744 \ud574\uc904 PyTorch \ud074\ub798\uc2a4\uc778\n[nn.Linear](https://pytorch.org/docs/stable/nn.html#linear-layers) \ub97c \uc120\ud615\n\ub808\uc774\uc5b4\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\nPyTorch \uc5d0\ub294 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ucf54\ub4dc\ub97c \ud06c\uac8c \ub2e8\uc21c\ud654 \ud560 \uc218 \uc788\ub294 \ubbf8\ub9ac \uc815\uc758\ub41c \ub808\uc774\uc5b4\uac00 \uc788\uace0 \uc774\ub294 \ub610\ud55c\n\uc885\uc885 \uae30\uc874 \ucf54\ub4dc\ubcf4\ub2e4 \uc18d\ub3c4\ub97c \ube60\ub974\uac8c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        return self.lin(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc804\uacfc \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \ubaa8\ub378\uc744 \uc778\uc2a4\ud134\uc2a4\ud654\ud558\uace0 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Mnist_Logistic()\nprint(loss_func(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc6b0\ub9ac\ub294 \uc5ec\uc804\ud788 \uc774\uc804\uacfc \ub3d9\uc77c\ud55c ``fit`` \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit()\n\nprint(loss_func(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``torch.optim`` \uc744 \uc774\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1 \ud558\uae30\n\nPyTorch\uc5d0\ub294 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654(optimization) \uc54c\uace0\ub9ac\uc998\uc744 \uac00\uc9c4 \ud328\ud0a4\uc9c0\uc778 ``torch.optim`` \ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n\uac01 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc218\ub3d9\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ud558\ub294 \ub300\uc2e0, \uc635\ud2f0\ub9c8\uc774\uc800(optimizer)\uc758 ``step`` \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec\n\uc5c5\ub370\uc774\ud2b8\ub97c \uc9c4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub807\uac8c \ud558\uba74 \uc774\uc804\uc5d0 \uc218\ub3d9\uc73c\ub85c \ucf54\ub529\ud55c \ucd5c\uc801\ud654 \ub2e8\uacc4\ub97c \ub300\uccb4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n```python\nwith torch.no_grad():\n    for p in model.parameters(): p -= p.grad * lr\n    model.zero_grad()\n```\n\ub300\uc2e0\uc5d0 \uc774\ub807\uac8c \ub9d0\uc774\uc8e0:\n\n```python\nopt.step()\nopt.zero_grad()\n```\n(``optim.zero_grad()`` \ub294 \uae30\uc6b8\uae30\ub97c 0\uc73c\ub85c \uc7ac\uc124\uc815 \ud574\uc90d\ub2c8\ub2e4. \ub2e4\uc74c \ubbf8\ub2c8 \ubc30\uce58\uc5d0 \ub300\ud55c\n\uae30\uc6b8\uae30\ub97c \uacc4\uc0b0\ud558\uae30 \uc804\uc5d0 \ud638\ucd9c\ud574\uc57c \ud569\ub2c8\ub2e4.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub098\uc911\uc5d0 \ub2e4\uc2dc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc838\ub97c \ub9cc\ub4dc\ub294 \uc791\uc740 \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_model():\n    model = Mnist_Logistic()\n    return model, optim.SGD(model.parameters(), lr=lr)\n\nmodel, opt = get_model()\nprint(loss_func(model(xb), yb))\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset \uc744 \uc774\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1\ud558\uae30\n\nPyTorch \uc5d0\ub294 \ucd94\uc0c1 Dataset \ud074\ub798\uc2a4\uac00 \uc788\uc2b5\ub2c8\ub2e4. Dataset \uc740\n``__len__`` \ud568\uc218 (Python\uc758 \ud45c\uc900 ``len`` \ud568\uc218\uc5d0 \uc758\ud574 \ud638\ucd9c\ub428) \ubc0f\n``__getitem__`` \ud568\uc218\ub97c \uac00\uc9c4 \uc5b4\ub5a4 \uac83\uc774\ub77c\ub3c4 \ub420 \uc218 \uc788\uc73c\uba70, \uc774 \ud568\uc218\ub4e4\uc744 \uc778\ub371\uc2f1(indexing)\ud558\uae30\n\uc704\ud55c \ubc29\ubc95\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n[\uc774 \ud29c\ud1a0\ub9ac\uc5bc](https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html)\n\uc740 ``Dataset`` \uc758 \ud558\uc704 \ud074\ub798\uc2a4\ub85c\uc368, \uc0ac\uc6a9\uc790 \uc9c0\uc815 ``FacialLandmarkDataset`` \ud074\ub798\uc2a4\ub97c \ub9cc\ub4dc\ub294\n\uc88b\uc740 \uc608\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4.\n\nPyTorch \uc758 [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset)\n\uc740 \ud150\uc11c\ub97c \uac10\uc2f8\ub294(wrapping) Dataset \uc785\ub2c8\ub2e4.\n\uae38\uc774\uc640 \uc778\ub371\uc2f1 \ubc29\uc2dd\uc744 \uc815\uc758\ud568\uc73c\ub85c\uc368 \ud150\uc11c\uc758 \uccab \ubc88\uc9f8 \ucc28\uc6d0\uc744 \ub530\ub77c \ubc18\ubcf5, \uc778\ub371\uc2f1 \ubc0f \uc2ac\ub77c\uc774\uc2a4(slice)\ud558\ub294 \ubc29\ubc95\ub3c4 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\uc774\ub807\uac8c\ud558\uba74 \ud6c8\ub828 \ud560 \ub54c \ub3d9\uc77c\ud55c \ub77c\uc778\uc5d0\uc11c \ub3c5\ub9bd(independent) \ubcc0\uc218\uc640 \uc885\uc18d(dependent) \ubcc0\uc218\uc5d0 \uc27d\uac8c \uc561\uc138\uc2a4 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``x_train`` \ubc0f ``y_train`` \ubaa8\ub450 \ud558\ub098\uc758 ``TensorDataset`` \uc5d0 \ud569\uccd0\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4,\n\ub530\ub77c\uc11c \ubc18\ubcf5\uc2dc\ud0a4\uace0 \uc2ac\ub77c\uc774\uc2a4 \ud558\uae30 \ud3b8\ub9ac\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds = TensorDataset(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc804\uc5d0\ub294 ``x`` \ubc0f ``y`` \uac12\uc758 \ubbf8\ub2c8 \ubc30\uce58\ub97c \ubcc4\ub3c4\ub85c \ubc18\ubcf5\ud574\uc57c \ud588\uc2b5\ub2c8\ub2e4:\n\n```python\nxb = x_train[start_i:end_i]\nyb = y_train[start_i:end_i]\n```\n\uc774\uc81c \uc774 \ub450 \ub2e8\uacc4\ub97c \ud568\uaed8 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n```python\nxb,yb = train_ds[i*bs : i*bs+bs]\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model, opt = get_model()\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        xb, yb = train_ds[i * bs: i * bs + bs]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``DataLoader`` \ub97c \uc0ac\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1\ud558\uae30\n\nPyTorch \uc758 ``DataLoader`` \ub294 \ubc30\uce58 \uad00\ub9ac\ub97c \ub2f4\ub2f9\ud569\ub2c8\ub2e4.\n\uc5ec\ub7ec\ubd84\ub4e4\uc740 \ubaa8\ub4e0 ``Dataset`` \uc73c\ub85c\ubd80\ud130 ``DataLoader`` \ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n``DataLoader`` \ub294 \ubc30\uce58\ub4e4\uc5d0 \ub300\ud574\uc11c \ubc18\ubcf5\ud558\uae30 \uc27d\uac8c \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4.\n``train_ds[i*bs : i*bs+bs]`` \ub97c \uc0ac\uc6a9\ud558\ub294 \ub300\uc2e0,\n``DataLoader`` \ub294 \ub9e4 \ubbf8\ub2c8\ubc30\uce58\ub97c \uc790\ub3d9\uc801\uc73c\ub85c \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n\ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc804\uc5d0\ub294 \ub8e8\ud504\uac00 \ub2e4\uc74c\uacfc \uac19\uc774 \ubc30\uce58 ``(xb, yb)`` \ub97c \ubc18\ubcf5\ud588\uc2b5\ub2c8\ub2e4:\n\n```python\nfor i in range((n-1)//bs + 1):\n    xb,yb = train_ds[i*bs : i*bs+bs]\n    pred = model(xb)\n```\n\uc774\uc81c (xb, yb)\uac00 DataLoader \uc5d0\uc11c \uc790\ub3d9\uc73c\ub85c \ub85c\ub4dc\ub418\ubbc0\ub85c \ub8e8\ud504\uac00 \ud6e8\uc52c \uae68\ub057\ud574\uc84c\uc2b5\ub2c8\ub2e4:\n\n```python\nfor xb,yb in train_dl:\n    pred = model(xb)\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model, opt = get_model()\n\nfor epoch in range(epochs):\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch\uc758 nn.Module, nn.Parameter, Dataset \ubc0f DataLoader \ub355\ubd84\uc5d0 \uc774\uc81c \ud6c8\ub828 \ub8e8\ud504\uac00\n\ud6e8\uc52c \ub354 \uc791\uc544\uc9c0\uace0 \uc774\ud574\ud558\uae30 \uc26c\uc6cc\uc84c\uc2b5\ub2c8\ub2e4.\n\uc774\uc81c \uc2e4\uc81c\ub85c \ud6a8\uacfc\uc801\uc778 \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294 \ub370 \ud544\uc694\ud55c \uae30\ubcf8 \uae30\ub2a5\uc744 \ucd94\uac00\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n## \uac80\uc99d(validation) \ucd94\uac00\ud558\uae30\n\n\uc139\uc158 1\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \ud569\ub9ac\uc801\uc778 \ud6c8\ub828 \ub8e8\ud504\ub97c \uc124\uc815\ud558\ub824\uace0\ud588\uc2b5\ub2c8\ub2e4.\n\uc2e4\uc804\uc5d0\uc11c, \uc5ec\ub7ec\ubd84\ub4e4\uc740 \uacfc\uc801\ud569(overfitting)\uc744 \ud655\uc778\ud558\uae30 \uc704\ud574\uc11c **\ud56d\uc0c1**\n[\uac80\uc99d \ub370\uc774\ud130\uc14b(validation set)](https://www.fast.ai/2017/11/13/validation-sets/) \uc774\n\uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n\n\ud6c8\ub828 \ub370\uc774\ud130\ub97c \uc11e\ub294(shuffling) \uac83\uc740 \ubc30\uce58\uc640 \uacfc\uc801\ud569 \uc0ac\uc774\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubc29\uc9c0\ud558\uae30 \uc704\ud574\n[\uc911\uc694\ud569\ub2c8\ub2e4.](https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks)\n\ubc18\uba74\uc5d0, \uac80\uc99d \uc190\uc2e4(validation loss)\uc740 \uac80\uc99d \ub370\uc774\ud130\uc14b\uc744 \uc11e\ub4e0 \uc548\uc11e\ub4e0 \ub3d9\uc77c\ud569\ub2c8\ub2e4.\n\ub370\uc774\ud130\ub97c \uc11e\ub294 \uac83\uc740 \ucd94\uac00 \uc2dc\uac04\uc774 \uac78\ub9ac\ubbc0\ub85c, \uac80\uc99d \ub370\uc774\ud130\ub97c \uc11e\ub294 \uac83\uc740 \uc758\ubbf8\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.\n\n\uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ubc30\uce58 \ud06c\uae30\ub294 \ud559\uc2b5 \ub370\uc774\ud130\uc14b \ubc30\uce58 \ud06c\uae30\uc758 2\ubc30\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc774\ub294 \uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574\uc11c\ub294 \uc5ed\uc804\ud30c(backpropagation)\uac00 \ud544\uc694\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uba54\ubaa8\ub9ac\ub97c\n\ub35c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4 (\uae30\uc6b8\uae30\ub97c \uc800\uc7a5\ud560 \ud544\uc694\uac00 \uc5c6\uc74c).\n\ub354 \ud070 \ubc30\uce58 \ud06c\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc190\uc2e4\uc744 \ub354 \ube68\ub9ac \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \uc774\ub807\uac8c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n\nvalid_ds = TensorDataset(x_valid, y_valid)\nvalid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac01 \uc5d0\ud3ed\uc774 \ub05d\ub0a0 \ub54c \uac80\uc99d \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ud504\ub9b0\ud2b8 \ud560 \uac83\uc785\ub2c8\ub2e4.\n\n(\ud6c8\ub828 \uc804\uc5d0 \ud56d\uc0c1 ``model.train()`` \uc744 \ud638\ucd9c\ud558\uace0, \ucd94\ub860(inference) \uc804\uc5d0 ``model.eval()``\n\uc744 \ud638\ucd9c\ud569\ub2c8\ub2e4, \uc774\ub294 ``nn.BatchNorm2d`` \ubc0f ``nn.Dropout`` \uacfc \uac19\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c\n\uc774\ub7ec\ud55c \ub2e4\ub978 \ub2e8\uacc4(\ud6c8\ub828, \ucd94\ub860) \uc5d0 \ub300\ud55c \uc801\uc808\ud55c \ub3d9\uc791\uc774 \uc77c\uc5b4\ub098\uac8c \ud558\uae30 \uc704\ud568\uc785\ub2c8\ub2e4.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model, opt = get_model()\n\nfor epoch in range(epochs):\n    model.train()\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    model.eval()\n    with torch.no_grad():\n        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n\n    print(epoch, valid_loss / len(valid_dl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## fit() \uc640 get_data() \uc0dd\uc131\ud558\uae30\n\n\uc774\uc81c \uc6b0\ub9ac\ub294 \uc6b0\ub9ac\ub9cc\uc758 \uc791\uc740 \ub9ac\ud329\ud1a0\ub9c1\uc744 \uc218\ud589\ud560 \uac83\uc785\ub2c8\ub2e4.\n\ud6c8\ub828 \ub370\uc774\ud130\uc14b\uacfc \uac80\uc99d \ub370\uc774\ud130\uc14b \ubaa8\ub450\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uc720\uc0ac\ud55c \ud504\ub85c\uc138\uc2a4\ub97c \ub450 \ubc88 \uac70\uce58\ubbc0\ub85c,\n\uc774\ub97c \ud558\ub098\uc758 \ubc30\uce58\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uc790\uccb4 \ud568\uc218 ``loss_batch`` \ub85c \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc804\ub2ec\ud558\uace0 \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed\uc804\ud30c\ub97c \uc218\ud589\ud569\ub2c8\ub2e4.\n\uac80\uc99d \ub370\uc774\ud130\uc14b\uc758 \uacbd\uc6b0 \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc804\ub2ec\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uba54\uc18c\ub4dc\uac00 \uc5ed\uc804\ud30c\ub97c \uc218\ud589\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``fit`` \uc740 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uace0 \uac01 \uc5d0\ud3ed\uc5d0 \ub300\ud55c \ud6c8\ub828 \ubc0f \uac80\uc99d \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``get_data`` \ub294 \ud559\uc2b5 \ubc0f \uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c dataloader \ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c dataloader\ub97c \uac00\uc838\uc624\uace0 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\ub294 \uc804\uccb4 \ud504\ub85c\uc138\uc2a4\ub97c 3 \uc904\uc758 \ucf54\ub4dc\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\nmodel, opt = get_model()\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\ub7ec\ud55c \uae30\ubcf8 3\uc904\uc758 \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ubaa8\ub378\uc744 \ud6c8\ub828\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ucee8\ubcfc\ub8e8\uc158 \uc2e0\uacbd\ub9dd(CNN)\uc744 \ud6c8\ub828\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294\uc9c0 \uc0b4\ud3b4 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4!\n\n## CNN \uc73c\ub85c \ub118\uc5b4\uac00\uae30\n\n\uc774\uc81c 3\uac1c\uc758 \ucee8\ubcfc\ub8e8\uc158 \ub808\uc774\uc5b4\ub85c \uc2e0\uacbd\ub9dd\uc744 \uad6c\ucd95\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc774\uc804 \uc139\uc158\uc758 \uc5b4\ub5a4 \ud568\uc218\ub3c4 \ubaa8\ub378\uc758 \ud615\uc2dd\uc5d0 \ub300\ud574 \uac00\uc815\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0,\n\ubcc4\ub3c4\uc758 \uc218\uc815\uc5c6\uc774 CNN\uc744 \ud559\uc2b5\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\nPytorch\uc758 \uc0ac\uc804\uc815\uc758\ub41c\n[Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) \ud074\ub798\uc2a4\ub97c\n\ucee8\ubcfc\ub8e8\uc158 \ub808\uc774\uc5b4\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. 3\uac1c\uc758 \ucee8\ubcfc\ub8e8\uc158 \ub808\uc774\uc5b4\ub85c CNN\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.\n\uac01 \ucee8\ubcfc\ub8e8\uc158 \ub4a4\uc5d0\ub294 ReLU\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ud3c9\uade0 \ud480\ub9c1(average pooling)\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.\n(``view`` \ub294 PyTorch\uc758 NumPy ``reshape`` \ubc84\uc804\uc785\ub2c8\ub2e4.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mnist_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\nlr = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[\ubaa8\uba58\ud140(Momentum)](https://cs231n.github.io/neural-networks-3/#sgd) \uc740\n\uc774\uc804 \uc5c5\ub370\uc774\ud2b8\ub3c4 \uace0\ub824\ud558\uace0 \uc77c\ubc18\uc801\uc73c\ub85c \ub354 \ube60\ub978 \ud6c8\ub828\uc73c\ub85c \uc774\uc5b4\uc9c0\ub294 \ud655\ub960\uc801 \uacbd\uc0ac\ud558\uac15\ubc95(stochastic gradient descent)\n\uc758 \ubcc0\ud615\uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Mnist_CNN()\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``nn.Sequential`` \uc0ac\uc6a9\ud558\uae30\n\n``torch.nn`` \uc5d0\ub294 \ucf54\ub4dc\ub97c \uac04\ub2e8\ud788 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub610 \ub2e4\ub978 \ud3b8\ub9ac\ud55c \ud074\ub798\uc2a4\uc778\n[Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)\n\uc774 \uc788\uc2b5\ub2c8\ub2e4..\n``Sequential`` \uac1d\uccb4\ub294 \uadf8 \uc548\uc5d0 \ud3ec\ud568\ub41c \uac01 \ubaa8\ub4c8\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \uc2e4\ud589\ud569\ub2c8\ub2e4.\n\uc774\uac83\uc740 \uc6b0\ub9ac\uc758 \uc2e0\uacbd\ub9dd\uc744 \uc791\uc131\ud558\ub294 \ub354 \uac04\ub2e8\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4.\n\n\uc774\ub97c \ud65c\uc6a9\ud558\ub824\uba74 \uc8fc\uc5b4\uc9c4 \ud568\uc218\uc5d0\uc11c **\uc0ac\uc6a9\uc790\uc815\uc758 \ub808\uc774\uc5b4(custom layer)** \ub97c \uc27d\uac8c\n\uc815\uc758\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uc5b4, PyTorch\uc5d0\ub294 `view` \ub808\uc774\uc5b4\uac00 \uc5c6\uc73c\ubbc0\ub85c \uc6b0\ub9ac\uc758 \uc2e0\uacbd\ub9dd \uc6a9\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n``Lambda`` \ub294 ``Sequential`` \ub85c \uc2e0\uacbd\ub9dd\uc744 \uc815\uc758\ud560 \ub54c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub808\uc774\uc5b4\ub97c \uc0dd\uc131\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``Sequential`` \ub85c \uc0dd\uc131\ub41c \ubaa8\ub4e4\uc740 \uac04\ub2e8\ud558\uac8c \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n    Lambda(preprocess),\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.AvgPool2d(4),\n    Lambda(lambda x: x.view(x.size(0), -1)),\n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``DataLoader`` \uac10\uc2f8\uae30\n\n\uc6b0\ub9ac\uc758 CNN\uc740 \uc0c1\ub2f9\ud788 \uac04\uacb0\ud558\uc9c0\ub9cc, MNIST\uc5d0\uc11c\ub9cc \uc791\ub3d9\ud569\ub2c8\ub2e4, \uc65c\ub0d0\ud558\uba74:\n - \uc785\ub825\uc774 28\\*28\uc758 \uae34 \ubca1\ud130\ub77c\uace0 \uac00\uc815\ud569\ub2c8\ub2e4.\n - \ucd5c\uc885\uc801\uc73c\ub85c CNN \uadf8\ub9ac\ub4dc \ud06c\uae30\ub294 4\\*4 \ub77c\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. (\uc774\uac83\uc740 \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud55c \ud3c9\uade0 \ud480\ub9c1 \ucee4\ub110 \ud06c\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.)\n\n\uc774 \ub450 \uac00\uc9c0 \uac00\uc815\uc744 \uc81c\uac70\ud558\uc5ec \ubaa8\ub378\uc774 \ubaa8\ub4e0 2d \ub2e8\uc77c \ucc44\ub110(channel) \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc791\ub3d9\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\uba3c\uc800 \ucd08\uae30 Lambda \ub808\uc774\uc5b4\ub97c \uc81c\uac70\ud558\uace0 \ub370\uc774\ud130 \uc804\ucc98\ub9ac\ub97c \uc81c\ub124\ub808\uc774\ud130(generator)\ub85c \uc774\ub3d9\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        for b in self.dl:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub2e4\uc74c\uc73c\ub85c ``nn.AvgPool2d`` \ub97c ``nn.AdaptiveAvgPool2d`` \ub85c \ub300\uccb4\ud558\uc5ec \uc6b0\ub9ac\uac00 \uac00\uc9c4\n*\uc785\ub825* \ud150\uc11c\uac00 \uc544\ub2c8\ub77c \uc6d0\ud558\ub294 *\ucd9c\ub825* \ud150\uc11c\uc758 \ud06c\uae30\ub97c \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uacb0\uacfc\uc801\uc73c\ub85c \uc6b0\ub9ac \ubaa8\ub378\uc740 \ubaa8\ub4e0 \ud06c\uae30\uc758 \uc785\ub825\uacfc \ud568\uaed8 \uc791\ub3d9\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.AdaptiveAvgPool2d(1),\n    Lambda(lambda x: x.view(x.size(0), -1)),\n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud55c\ubc88 \uc2e4\ud589\ud574 \ubd05\uc2dc\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU \uc0ac\uc6a9\ud558\uae30\n\n\ub9cc\uc57d \uc5ec\ub7ec\ubd84\ub4e4\uc774 \uc6b4\uc774 \uc88b\uc544\uc11c CUDA \uc9c0\uc6d0 GPU (\ub300\ubd80\ubd84\uc758 \ud074\ub77c\uc6b0\ub4dc \uc81c\uacf5 \uc5c5\uccb4\uc5d0\uc11c\n\uc2dc\uac04\ub2f9 \uc57d $0.50 \uc5d0 \uc774\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4) \ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4\uba74, \ucf54\ub4dc \uc2e4\ud589 \uc18d\ub3c4\ub97c \ub192\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uba3c\uc800 GPU\uac00 PyTorch\uc5d0\uc11c \uc791\ub3d9\ud558\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uadf8\ub9ac\uace0 \uc774\uc5d0 \ub300\ud55c \ub514\ubc14\uc774\uc2a4 \uc624\ube0c\uc81d\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = torch.device(\n    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPU\ub85c \ubc30\uce58\ub97c \uc62e\uae30\ub3c4\ub85d ``preprocess`` \ub97c \uc5c5\ub370\uc774\ud2b8 \ud569\uc2dc\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub9c8\uc9c0\ub9c9\uc73c\ub85c \ubaa8\ub378\uc744 GPU\ub85c \uc774\ub3d9\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.to(dev)\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \ub354 \ube68\ub9ac \uc2e4\ud589\ub429\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ub9c8\uce58\uba74\uc11c\n\n\uc774\uc81c PyTorch\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc77c\ubc18 \ub370\uc774\ud130 \ud30c\uc774\ud504 \ub77c\uc778\uacfc\n\ud6c8\ub828 \ub8e8\ud504\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774\uc81c \ubaa8\ub378 \ud559\uc2b5\uc774 \uc5bc\ub9c8\ub098 \uac04\ub2e8\ud55c\uc9c0 \ud655\uc778\ud558\ub824\uba74 [mnist_sample \ub178\ud2b8\ubd81](https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb)_ \uc744 \uc0b4\ud3b4\ubcf4\uc138\uc694.\n\n\ubb3c\ub860 \ub370\uc774\ud130 \uc99d\uac15(data augmentation), \ucd08\ub9e4\uac1c\ubcc0\uc218 \uc870\uc815(hyperparameter tuning),\n\ud6c8\ub828\uacfc\uc815 \ubaa8\ub2c8\ud130\ub9c1(monitoring training), \uc804\uc774 \ud559\uc2b5(transfer learning) \ub4f1\uacfc \uac19\uc774\n\ucd94\uac00\ud558\uace0 \uc2f6\uc740 \ud56d\ubaa9\ub4e4\uc774 \ub9ce\uc774 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4.\n\uc774\ub7ec\ud55c \uae30\ub2a5\ub4e4\uc740 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0 \ud45c\uc2dc\ub41c \uac83\uacfc \ub3d9\uc77c\ud55c \uc124\uacc4 \uc811\uadfc \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac1c\ubc1c\ub41c fastai \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c\n\uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, \ubaa8\ub378\uc744 \ub354\uc6b1 \ubc1c\uc804\uc2dc\ud0a4\ub824\ub294 \uc2e4\ubb34\uc790\uc5d0\uac8c \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \ub2e4\uc74c \ub2e8\uacc4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \uc2dc\uc791 \ubd80\ubd84\uc5d0\uc11c ``torch.nn``, ``torch.optim``, ``Dataset``,\n\uadf8\ub9ac\uace0 ``DataLoader`` \uc758 \uac01 \uc608\uc81c\ub97c \ud1b5\ud574 \uc124\uba85\ud558\uaca0\ub2e4\uace0 \uc774\uc57c\uae30\ud588\uc5c8\uc2b5\ub2c8\ub2e4.\n\uc774\uc81c \uc704\uc758 \ub0b4\uc6a9\ub4e4\uc744 \uc694\uc57d\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n\n - ``torch.nn``:\n\n   + ``Module``: \ud568\uc218\ucc98\ub7fc \ub3d9\uc791\ud558\uc9c0\ub9cc, \ub610\ud55c \uc0c1\ud0dc(state) (\uc608\ub97c \ub4e4\uc5b4, \uc2e0\uacbd\ub9dd\uc758 \ub808\uc774\uc5b4 \uac00\uc911\uce58)\ub97c\n     \ud3ec\ud568\ud560 \uc218 \uc788\ub294 \ud638\ucd9c \uac00\ub2a5\ud55c \uc624\ube0c\uc81d\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n     \uc774\ub294 \ud3ec\ud568\ub41c ``Parameter`` (\ub4e4)\uac00 \uc5b4\ub5a4 \uac83\uc778\uc9c0 \uc54c\uace0, \ubaa8\ub4e0 \uae30\uc6b8\uae30\ub97c 0\uc73c\ub85c \uc124\uc815\ud558\uace0 \uac00\uc911\uce58\n     \uc5c5\ub370\uc774\ud2b8 \ub4f1\uc744 \uc704\ud574 \ubc18\ubcf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n   + ``Parameter``: ``Module`` \uc5d0 \uc5ed\uc804\ud30c \ub3d9\uc548 \uc5c5\ub370\uc774\ud2b8\uac00 \ud544\uc694\ud55c \uac00\uc911\uce58\uac00 \uc788\uc74c\uc744 \uc54c\ub824\uc8fc\ub294\n     \ud150\uc11c\uc6a9 \ub798\ud37c\uc785\ub2c8\ub2e4. `requires_grad` \uc18d\uc131\uc774 \uc124\uc815\ub41c \ud150\uc11c\ub9cc \uc5c5\ub370\uc774\ud2b8 \ub429\ub2c8\ub2e4.\n   + ``functional``: \ud65c\uc131\ud654 \ud568\uc218, \uc190\uc2e4 \ud568\uc218 \ub4f1\uc744 \ud3ec\ud568\ud558\ub294 \ubaa8\ub4c8 (\uad00\ub840\uc5d0 \ub530\ub77c \uc77c\ubc18\uc801\uc73c\ub85c\n     ``F`` \ub124\uc784\uc2a4\ud398\uc774\uc2a4\ub85c \uc784\ud3ec\ud2b8 \ub429\ub2c8\ub2e4) \uc774\uace0, \ubb3c\ub860 \ucee8\ubcfc\ub8e8\uc158 \ubc0f \uc120\ud615 \ub808\uc774\uc5b4 \ub4f1\uc5d0 \ub300\ud574\uc11c\n     \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud558\uc9c0\uc54a\ub294(non-stateful) \ubc84\uc804\uc758 \ub808\uc774\uc5b4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.\n - ``torch.optim``: \uc5ed\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c ``Parameter`` \uc758 \uac00\uc911\uce58\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\ub294,\n   ``SGD`` \uc640 \uac19\uc740 \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.\n - ``Dataset``: ``TensorDataset`` \uacfc \uac19\uc774 PyTorch\uc640 \ud568\uaed8 \uc81c\uacf5\ub418\ub294 \ud074\ub798\uc2a4\ub97c \ud3ec\ud568\ud558\uc5ec ``__len__`` \ubc0f\n   ``__getitem__`` \uc774 \uc788\ub294 \uac1d\uccb4\uc758 \ucd94\uc0c1 \uc778\ud130\ud398\uc774\uc2a4\n - ``DataLoader``: \ubaa8\ub4e0 \uc885\ub958\uc758 ``Dataset`` \uc744 \uae30\ubc18\uc73c\ub85c \ub370\uc774\ud130\uc758 \ubc30\uce58\ub4e4\uc744 \ucd9c\ub825\ud558\ub294 \ubc18\ubcf5\uc790(iterator)\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}