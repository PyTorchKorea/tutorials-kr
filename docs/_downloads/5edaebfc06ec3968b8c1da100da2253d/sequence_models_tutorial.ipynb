{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \uc2dc\ud000\uc2a4 \ubaa8\ub378\uacfc LSTM \ub124\ud2b8\uc6cc\ud06c\n**\ubc88\uc5ed**: [\ubc15\uc218\ubbfc](https://github.com/convin305)\n\n\uc9c0\uae08\uae4c\uc9c0 \uc6b0\ub9ac\ub294 \ub2e4\uc591\ud55c \uc21c\uc804\ud30c(feed-forward) \uc2e0\uacbd\ub9dd\ub4e4\uc744 \ubcf4\uc544 \uc654\uc2b5\ub2c8\ub2e4. \n\uc989, \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uc758\ud574 \uc720\uc9c0\ub418\ub294 \uc0c1\ud0dc\uac00 \uc804\ud600 \uc5c6\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \n\uc774\uac83\uc740 \uc544\ub9c8 \uc6b0\ub9ac\uac00 \uc6d0\ud558\ub294 \ub3d9\uc791\uc774 \uc544\ub2d0 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \n\uc2dc\ud000\uc2a4 \ubaa8\ub378\uc740 NLP\uc758 \ud575\uc2ec\uc785\ub2c8\ub2e4. \uc774\ub294 \uc785\ub825 \uac04\uc5d0 \uc77c\uc885\uc758 \uc2dc\uac04\uc801 \uc885\uc18d\uc131\uc774 \uc874\uc7ac\ud558\ub294 \ubaa8\ub378\uc744 \ub9d0\ud569\ub2c8\ub2e4. \n\uc2dc\ud000\uc2a4 \ubaa8\ub378\uc758 \uace0\uc804\uc801\uc778 \uc608\ub294 \ud488\uc0ac \ud0dc\uae45\uc744 \uc704\ud55c \ud788\ub4e0 \ub9c8\ub974\ucf54\ud504 \ubaa8\ub378\uc785\ub2c8\ub2e4. \n\ub610 \ub2e4\ub978 \uc608\ub294 \uc870\uac74\ubd80 \ub79c\ub364 \ud544\ub4dc\uc785\ub2c8\ub2e4. \n\n\uc21c\ud658 \uc2e0\uacbd\ub9dd\uc740 \uc77c\uc885\uc758 \uc0c1\ud0dc\ub97c \uc720\uc9c0\ud558\ub294 \ub124\ud2b8\uc6cc\ud06c\uc785\ub2c8\ub2e4. \n\uc608\ub97c \ub4e4\uba74, \ucd9c\ub825\uc740 \ub2e4\uc74c \uc785\ub825\uc758 \uc77c\ubd80\ub85c \uc0ac\uc6a9\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\uc815\ubcf4\ub294 \ub124\ud2b8\uc6cc\ud06c\uac00 \uc2dc\ud000\uc2a4\ub97c \ud1b5\uacfc\ud560 \ub54c \uc804\ud30c\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \nLSTM\uc758 \uacbd\uc6b0\uc5d0, \uc2dc\ud000\uc2a4\uc758 \uac01 \uc694\uc18c\uc5d0 \ub300\uc751\ud558\ub294 *\uc740\ub2c9 \uc0c1\ud0dc(hidden state)* $h_t$ \uac00 \uc874\uc7ac\ud558\uba70,\n\uc774\ub294 \uc6d0\uce59\uc801\uc73c\ub85c \uc2dc\ud000\uc2a4\uc758 \uc55e\ubd80\ubd84\uc5d0 \uc788\ub294 \uc784\uc758 \ud3ec\uc778\ud2b8\uc758 \uc815\ubcf4\ub97c \ud3ec\ud568\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\uc6b0\ub9ac\ub294 \uc740\ub2c9 \uc0c1\ud0dc\ub97c \uc774\uc6a9\ud558\uc5ec \uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c\uc758 \ub2e8\uc5b4,\n\ud488\uc0ac \ud0dc\uadf8 \ub4f1 \ubb34\uc218\ud788 \ub9ce\uc740 \uac83\ub4e4\uc744 \uc608\uce21\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\n\n## Pytorch\uc5d0\uc11c\uc758 LSTM\n\n\uc608\uc81c\ub97c \uc2dc\uc791\ud558\uae30 \uc804\uc5d0, \uba87 \uac00\uc9c0 \uc0ac\ud56d\uc744 \uc720\uc758\ud558\uc138\uc694. \nPytorch\uc5d0\uc11c\uc758 LSTM\uc740 \ubaa8\ub4e0 \uc785\ub825\uc774 3D Tensor \uc77c \uac83\uc73c\ub85c \uc608\uc0c1\ud569\ub2c8\ub2e4. \n\uc774\ub7ec\ud55c \ud150\uc11c \ucd95\uc758 \uc758\ubbf8\ub294 \uc911\uc694\ud569\ub2c8\ub2e4. \n\uccab \ubc88\uc9f8 \ucd95\uc740 \uc2dc\ud000\uc2a4 \uc790\uccb4\uc774\uace0, \ub450 \ubc88\uc9f8 \ucd95\uc740 \ubbf8\ub2c8 \ubc30\uce58\uc758 \uc778\uc2a4\ud134\uc2a4\ub97c \uc778\ub371\uc2f1\ud558\uba70, \n\uc138 \ubc88\uc9f8 \ucd95\uc740 \uc785\ub825 \uc694\uc18c\ub97c \uc778\ub371\uc2f1\ud569\ub2c8\ub2e4. \n\ubbf8\ub2c8 \ubc30\uce58\uc5d0 \ub300\ud574\uc11c\ub294 \ub17c\uc758\ud558\uc9c0 \uc54a\uc558\uc73c\ubbc0\ub85c \uc774\ub97c \ubb34\uc2dc\ud558\uace0,\n\ub450 \ubc88\uc9f8 \ucd95\uc5d0 \ub300\ud574\uc11c\ub294 \ud56d\uc0c1 1\ucc28\uc6d0\ub9cc \uac00\uc9c8 \uac83\uc774\ub77c\uace0 \uac00\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4. \n\ub9cc\uc57d \uc6b0\ub9ac\uac00 \"The cow jumped.\"\ub77c\ub294 \ubb38\uc7a5\uc5d0 \ub300\ud574 \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc744 \uc2e4\ud589\ud558\ub824\uba74,\n\uc785\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc544\uc57c \ud569\ub2c8\ub2e4. \n\n\\begin{align}\\begin{bmatrix}\n   \\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n   q_\\text{cow} \\\\\n   q_\\text{jumped}\n   \\end{bmatrix}\\end{align}\n\n\ub2e4\ub9cc, \uc0ac\uc774\uc988\uac00 1\uc778 \ucd94\uac00\uc801\uc778 2\ucc28\uc6d0\uc774 \uc788\ub2e4\ub294 \uac83\uc744 \uae30\uc5b5\ud574\uc57c \ud569\ub2c8\ub2e4. \n\n\ub610\ud55c \ud55c \ubc88\uc5d0 \ud558\ub098\uc529 \uc2dc\ud000\uc2a4\ub97c \uc9c4\ud589\ud560 \uc218 \uc788\uc73c\uba70,\n\uc774 \uacbd\uc6b0 \uccab \ubc88\uc9f8 \ucd95\ub3c4 \uc0ac\uc774\uc988\uac00 1\uc774 \ub429\ub2c8\ub2e4. \n\n\uac04\ub2e8\ud55c \uc608\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Robert Guthrie\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lstm = nn.LSTM(3, 3)  # \uc785\ub825 3\ucc28\uc6d0, \ucd9c\ub825 3\ucc28\uc6d0\ninputs = [torch.randn(1, 3) for _ in range(5)]  # \uae38\uc774\uac00 5\uc778 \uc2dc\ud000\uc2a4\ub97c \ub9cc\ub4ed\ub2c8\ub2e4\n\n# \uc740\ub2c9 \uc0c1\ud0dc\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.\nhidden = (torch.randn(1, 1, 3),\n          torch.randn(1, 1, 3))\nfor i in inputs:\n    # \ud55c \ubc88\uc5d0 \ud55c \uc694\uc18c\uc529 \uc2dc\ud000\uc2a4\ub97c \ud1b5\uacfc\ud569\ub2c8\ub2e4.\n    # \uac01 \ub2e8\uacc4\uac00 \ub05d\ub098\uba74, hidden\uc5d0\ub294 \uc740\ub2c9 \uc0c1\ud0dc\uac00 \ud3ec\ud568\ub429\ub2c8\ub2e4.\n    out, hidden = lstm(i.view(1, 1, -1), hidden)\n\n# \uc544\ub2c8\uba74 \uc6b0\ub9ac\ub294 \uc804\uccb4 \uc2dc\ud000\uc2a4\ub97c \ud55c \ubc88\uc5d0 \uc218\ud589\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \n# LSTM\uc5d0 \uc758\ud574 \ubc18\ud658\ub41c \uccab \ubc88\uc9f8 \uac12\uc740 \uc2dc\ud000\uc2a4 \uc804\uccb4\uc5d0 \ub300\ud55c \uc740\ub2c9 \uc0c1\ud0dc\uc785\ub2c8\ub2e4. \n# \ub450 \ubc88\uc9f8\ub294 \uac00\uc7a5 \ucd5c\uadfc\uc758 \uc740\ub2c9 \uc0c1\ud0dc\uc785\ub2c8\ub2e4. \n# (\uc544\ub798\uc758 \"hidden\"\uacfc \"out\"\uc758 \ub9c8\uc9c0\ub9c9 \uc2ac\ub77c\uc774\uc2a4(slice)\ub97c \ube44\uad50\ud574 \ubcf4\uba74 \ub458\uc740 \ub3d9\uc77c\ud569\ub2c8\ub2e4.)\n# \uc774\ub807\uac8c \ud558\ub294 \uc774\uc720\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n# \"out\"\uc740 \uc2dc\ud000\uc2a4\uc758 \ubaa8\ub4e0 \uc740\ub2c9 \uc0c1\ud0dc\uc5d0 \ub300\ud55c \uc561\uc138\uc2a4\ub97c \uc81c\uacf5\ud558\uace0,\n# \"hidden\"\uc740 \ub098\uc911\uc5d0 lstm\uc5d0 \uc778\uc218 \ud615\ud0dc\ub85c \uc804\ub2ec\ud558\uc5ec \n# \uc2dc\ud000\uc2a4\ub97c \uacc4\uc18d\ud558\uace0, \uc5ed\uc804\ud30c \ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4. \n# \ucd94\uac00\ub85c \ub450 \ubc88\uc9f8 \ucc28\uc6d0\uc744 \ub354\ud569\ub2c8\ub2e4. \ninputs = torch.cat(inputs).view(len(inputs), 1, -1)\nhidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # \uc740\ub2c9 \uc0c1\ud0dc\ub97c \uc9c0\uc6c1\ub2c8\ub2e4.\nout, hidden = lstm(inputs, hidden)\nprint(out)\nprint(hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uc608\uc2dc: \ud488\uc0ac \ud0dc\uae45\uc744 \uc704\ud55c LSTM\n\n\uc774 \uc139\uc158\uc5d0\uc11c\ub294 \uc6b0\ub9ac\ub294 \ud488\uc0ac \ud0dc\uadf8\ub97c \uc5bb\uae30 \uc704\ud574 LSTM\uc744 \uc774\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \n\ube44\ud130\ube44(Viterbi)\ub098 \uc21c\ubc29\ud5a5-\uc5ed\ubc29\ud5a5(Forward-Backward) \uac19\uc740 \uac83\ub4e4\uc740 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc744 \uac83\uc785\ub2c8\ub2e4. \n\uadf8\ub7ec\ub098 (\ub3c4\uc804\uc801\uc778) \uc5f0\uc2b5\uc73c\ub85c, \uc5b4\ub5bb\uac8c \ub3cc\uc544\uac00\ub294\uc9c0\ub97c \ud655\uc778\ud55c \ub4a4\uc5d0 \n\ube44\ud130\ube44\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294\uc9c0\uc5d0 \ub300\ud574\uc11c \uc0dd\uac01\ud574 \ubcf4\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4. \n\uc774 \uc608\uc2dc\uc5d0\uc11c\ub294 \uc784\ubca0\ub529\ub3c4 \ucc38\uc870\ud569\ub2c8\ub2e4. \ub9cc\uc57d\uc5d0 \uc784\ubca0\ub529\uc5d0 \uc775\uc219\ud558\uc9c0 \uc54a\ub2e4\uba74, \n[\uc5ec\uae30](https://tutorials.pytorch.kr/beginner/nlp/word_embeddings_tutorial.html)_.\n\uc5d0\uc11c \uad00\ub828 \ub0b4\uc6a9\uc744 \uc77d\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ubaa8\ub378\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \ub2e8\uc5b4\uac00 $w_i \\in V$ \uc77c \ub54c, \n\uc785\ub825 \ubb38\uc7a5\uc744 $w_1, \\dots, w_M$ \ub77c\uace0 \ud569\uc2dc\ub2e4. \ub610\ud55c, \n$T$ \ub97c \uc6b0\ub9ac\uc758 \ud0dc\uadf8 \uc9d1\ud569\ub77c\uace0 \ud558\uace0, $w_i$ \uc758 \ub2e8\uc5b4 \ud0dc\uadf8\ub97c $y_i$ \ub77c\uace0 \ud569\ub2c8\ub2e4. \n\ub2e8\uc5b4 $w_i$ \uc5d0 \ub300\ud55c \uc608\uce21\ub41c \ud0dc\uadf8\ub97c $\\hat{y}_i$ \ub85c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \n\n\n\uc774\uac83\uc740 $\\hat{y}_i \\in T$ \uc77c \ub54c, \ucd9c\ub825\uc774 $\\hat{y}_1, \\dots, \\hat{y}_M$ \uc2dc\ud000\uc2a4\uc778\n\uad6c\uc870 \uc608\uce21 \ubaa8\ub378\uc785\ub2c8\ub2e4. \n\n\uc608\uce21\uc744 \ud558\uae30 \uc704\ud574, LSTM\uc5d0 \ubb38\uc7a5\uc744 \uc804\ub2ec\ud569\ub2c8\ub2e4. \ud55c \uc2dc\uac04 \ub2e8\uacc4\n$i$ \uc758 \uc740\ub2c9 \uc0c1\ud0dc\ub294 $h_i$ \ub85c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \ub610\ud55c \uac01 \ud0dc\uadf8\uc5d0\n\uace0\uc720\ud55c \uc778\ub371\uc2a4\ub97c \ud560\ub2f9\ud569\ub2c8\ub2e4 (\ub2e8\uc5b4 \uc784\ubca0\ub529 \uc139\uc158\uc5d0\uc11c word\\_to\\_ix \ub97c \uc0ac\uc6a9\ud55c \uac83\uacfc \uc720\uc0ac\ud569\ub2c8\ub2e4.)\n\uadf8\ub7ec\uba74 $\\hat{y}_i$  \uc608\uce21 \uaddc\uce59\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \n\n\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\\end{align}\n\n\uc989, \uc740\ub2c9 \uc0c1\ud0dc\uc758 \uc544\ud540 \ub9f5(affine map)\uc5d0 \ub300\ud574 \ub85c\uadf8 \uc18c\ud504\ud2b8\ub9e5\uc2a4(log softmax)\ub97c \ucde8\ud558\uace0,\n\uc608\uce21\ub41c \ud0dc\uadf8\ub294 \uc774 \ubca1\ud130\uc5d0\uc11c \uac00\uc7a5 \ud070 \uac12\uc744 \uac00\uc9c0\ub294 \ud0dc\uadf8\uac00 \ub429\ub2c8\ub2e4. \n\uc774\uac83\uc740 \uace7 $A$ \uc758 \ud0c0\uae43 \uacf5\uac04\uc758 \ucc28\uc6d0\uc774 $|T|$ \ub77c\ub294 \uac83\uc744 \n\uc758\ubbf8\ud55c\ub2e4\ub294 \uac83\uc744 \uc54c\uc544\ub450\uc138\uc694.\n\n\n\ub370\uc774\ud130 \uc900\ube44:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\n\ntraining_data = [\n    # \ud0dc\uadf8\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: DET - \ud55c\uc815\uc0ac;NN - \uba85\uc0ac;V - \ub3d9\uc0ac\n    # \uc608\ub97c \ub4e4\uc5b4, \"The\" \ub77c\ub294 \ub2e8\uc5b4\ub294 \ud55c\uc815\uc0ac\uc785\ub2c8\ub2e4.\n    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n]\nword_to_ix = {}\n# training_data\uc758 \uac01 \ud29c\ud50c\uc5d0 \uc788\ub294 \uac01 \ub2e8\uc5b4 \ubaa9\ub85d(\ubb38\uc7a5) \ubc0f \ud0dc\uadf8 \ubaa9\ub85d\uc5d0 \ub300\ud574\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word_to_ix:  # word\ub294 \uc544\uc9c1 \ubc88\ud638\uac00 \ud560\ub2f9\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4\n            word_to_ix[word] = len(word_to_ix)  # \uac01 \ub2e8\uc5b4\uc5d0 \uace0\uc720\ud55c \ubc88\ud638 \ud560\ub2f9\nprint(word_to_ix)\ntag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # \uac01 \ud0dc\uadf8\uc5d0 \uace0\uc720\ud55c \ubc88\ud638 \ud560\ub2f9\n\n# \uc774\uac83\ub4e4\uc740 \uc77c\ubc18\uc801\uc73c\ub85c 32\ub098 64\ucc28\uc6d0\uc5d0 \uac00\uae5d\uc2b5\ub2c8\ub2e4. \n# \ud6c8\ub828\ud560 \ub54c \uac00\uc911\uce58\uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\ub3c4\ub85d, \uc791\uac8c \uc720\uc9c0\ud558\uaca0\uc2b5\ub2c8\ub2e4. \nEMBEDDING_DIM = 6\nHIDDEN_DIM = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \uc0dd\uc131:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM\uc740 \ub2e8\uc5b4 \uc784\ubca0\ub529\uc744 \uc785\ub825\uc73c\ub85c \ubc1b\uace0, \n        # \ucc28\uc6d0\uc774 hidden_dim\uc778 \uc740\ub2c9 \uc0c1\ud0dc\ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4. \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # \uc740\ub2c9 \uc0c1\ud0dc \uacf5\uac04\uc5d0\uc11c \ud0dc\uadf8 \uacf5\uac04\uc73c\ub85c \ub9e4\ud551\ud558\ub294 \uc120\ud615 \ub808\uc774\uc5b4\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \ud559\uc2b5:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# \ud6c8\ub828 \uc804\uc758 \uc810\uc218\ub97c \ud655\uc778\ud558\uc138\uc694.\n# \ucd9c\ub825\uc758 i,j\uc694\uc18c\ub294 \ub2e8\uc5b4 i\uc5d0 \ub300\ud55c \ud0dc\uadf8 j\uc758 \uc810\uc218\uc785\ub2c8\ub2e4.\n# \uc5ec\uae30\uc11c\ub294 \ud6c8\ub828\uc744 \ud560 \ud544\uc694\uac00 \uc5c6\uc73c\ubbc0\ub85c, \ucf54\ub4dc\ub294 torch.no_grad()\ub85c \ub798\ud551 \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\nwith torch.no_grad():\n    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n    tag_scores = model(inputs)\n    print(tag_scores)\n\nfor epoch in range(300):  # \ub2e4\uc2dc \ub9d0\ud558\uc9c0\ub9cc, \uc77c\ubc18\uc801\uc73c\ub85c 300\uc5d0\ud3ed\uc744 \uc218\ud589\ud558\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\uac74 \uc7a5\ub09c\uac10 \ub370\uc774\ud130\uc774\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n    for sentence, tags in training_data:\n        # 1\ub2e8\uacc4, Pytorch\ub294 \ubcc0\ud654\ub3c4\ub97c \ucd95\uc801\ud55c\ub2e4\ub294 \uac83\uc744 \uae30\uc5b5\ud558\uc138\uc694. \n        # \uac01 \uc778\uc2a4\ud134\uc2a4 \uc804\uc5d0 \uc774\ub97c \uc9c0\uc6cc\uc918\uc57c \ud569\ub2c8\ub2e4. \n        model.zero_grad()\n\n        # 2\ub2e8\uacc4, \ub124\ud2b8\uc6cc\ud06c\uc5d0 \ub9de\uac8c \uc785\ub825\uc744 \uc900\ube44\uc2dc\ud0b5\ub2c8\ub2e4. \n        # \uc989, \uc785\ub825\ub4e4\uc744 \ub2e8\uc5b4 \uc778\ub371\uc2a4\ub4e4\uc758 \ud150\uc11c\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4. \n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n\n        # 3\ub2e8\uacc4, \uc21c\uc804\ud30c \ub2e8\uacc4(forward pass)\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4.\n        tag_scores = model(sentence_in)\n\n        # 4\ub2e8\uacc4, \uc190\uc2e4\uacfc \uae30\uc6b8\uae30\ub97c \uacc4\uc0b0\ud558\uace0, optimizer.step()\uc744 \ud638\ucd9c\ud558\uc5ec \n        # \ub9e4\uac1c\ubcc0\uc218\ub97c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. \n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n\n# \ud6c8\ub828 \ud6c4\uc758 \uc810\uc218\ub97c \ud655\uc778\ud574 \ubcf4\uc138\uc694. \nwith torch.no_grad():\n    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n    tag_scores = model(inputs)\n\n    # \ubb38\uc7a5\uc740 \"the dog ate the apple\"\uc785\ub2c8\ub2e4. i\uc640 j\ub294 \ub2e8\uc5b4 i\uc5d0 \ub300\ud55c \ud0dc\uadf8 j\uc758 \uc810\uc218\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.\n    # \uc608\uce21\ub41c \ud0dc\uadf8\ub294 \uac00\uc7a5 \uc810\uc218\uac00 \ub192\uc740 \ud0dc\uadf8\uc785\ub2c8\ub2e4.\n    # \uc790, \uc544\ub798\uc758 \uc608\uce21\ub41c \uc21c\uc11c\uac00 0 1 2 0 1\uc774\ub77c\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n    # 0\uc740 1\ud589\uc5d0 \ub300\ud55c \ucd5c\ub313\uac12\uc774\ubbc0\ub85c, \n    # 1\uc740 2\ud589\uc5d0 \ub300\ud55c \ucd5c\ub313\uac12\uc774 \ub418\ub294 \uc2dd\uc785\ub2c8\ub2e4.\n    # DET NOUN VERB DET NOUN\uc740 \uc62c\ubc14\ub978 \uc21c\uc11c\uc785\ub2c8\ub2e4!\n    print(tag_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uc5f0\uc2b5 : \ubb38\uc790-\ub2e8\uc704 \ud2b9\uc9d5\uacfc LSTM \ud488\uc0ac \ud0dc\uac70 \uc99d\uac15\n\n\uc704\uc758 \uc608\uc81c\uc5d0\uc11c, \uac01 \ub2e8\uc5b4\ub294 \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc5d0 \uc785\ub825 \uc5ed\ud560\uc744 \ud558\ub294 \uc784\ubca0\ub529\uc744 \uac00\uc9d1\ub2c8\ub2e4. \n\ub2e8\uc5b4\uc758 \ubb38\uc790\uc5d0\uc11c \ud30c\uc0dd\ub41c \ud45c\ud604\uc73c\ub85c \ub2e8\uc5b4 \uc784\ubca0\ub529\uc744 \uc99d\uac00\uc2dc\ucf1c\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \n\uc811\uc0ac(affixes)\uc640 \uac19\uc740 \ubb38\uc790 \uc218\uc900\uc758 \uc815\ubcf4\ub294 \ud488\uc0ac\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\uae30 \ub54c\ubb38\uc5d0, \n\uc0c1\ub2f9\ud55c \ub3c4\uc6c0\uc774 \ub420 \uac83\uc73c\ub85c \uc608\uc0c1\ud569\ub2c8\ub2e4. \n\uc608\ub97c \ub4e4\uc5b4, \uc811\uc0ac *-ly* \uac00 \uc788\ub294 \ub2e8\uc5b4\ub294\n\uc601\uc5b4\uc5d0\uc11c \uac70\uc758 \ud56d\uc0c1 \ubd80\uc0ac\ub85c \ud0dc\uadf8\uac00 \uc9c0\uc815\ub429\ub2c8\ub2e4.\n\n\uc774\uac83\uc744 \ud558\uae30 \uc704\ud574\uc11c, $c_w$ \ub97c \ub2e8\uc5b4 $w$ \uc758 C\ub97c \ub2e8\uc5b4 w\uc758 \ubb38\uc790 \uc218\uc900 \ud45c\ud604\uc774\ub77c\uace0 \ud558\uace0, \n\uc804\uacfc \uac19\uc774 $x_w$ \ub97c \ub2e8\uc5b4\uc784\ubca0\ub529\uc774\ub77c\uace0 \ud569\uc2dc\ub2e4. \n\uadf8\ub807\ub2e4\uba74 \uc6b0\ub9ac\uc758 \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc785\ub825\uc740 $x_w$ \uc640\n$c_w$ \uc758 \uc5f0\uacb0\uc774\ub77c\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d\uc5d0 $x_w$ \uac00 \ucc28\uc6d0 5\ub97c \uac00\uc9c0\uace0, $c_w$\n\ucc28\uc6d0 3\uc744 \uac00\uc9c0\uba74 LSTM\uc740 \ucc28\uc6d0 8\uc758 \uc785\ub825\uc744 \ubc1b\uc544\ub4e4\uc5ec\uc57c \ud569\ub2c8\ub2e4. \n\n\ubb38\uc790 \uc218\uc900\uc758 \ud45c\ud604\uc744 \uc5bb\uae30 \uc704\ud574\uc11c, \ub2e8\uc5b4\uc758 \ubb38\uc790\uc5d0 \ub300\ud574\uc11c LSTM\uc744 \uc218\ud589\ud558\uace0\n$c_w$ \ub97c LSTM\uc758 \ucd5c\uc885 \uc740\ub2c9 \uc0c1\ud0dc\uac00 \ub418\ub3c4\ub85d \ud569\ub2c8\ub2e4. \n\ud78c\ud2b8:\n\n* \uc0c8 \ubaa8\ub378\uc5d0\ub294 \ub450 \uac1c\uc758 LSTM\uc774 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4. \n  POS \ud0dc\uadf8 \uc810\uc218\ub97c \ucd9c\ub825\ud558\ub294 \uc6d0\ub798\uc758 LSTM\uacfc \n  \uac01 \ub2e8\uc5b4\uc758 \ubb38\uc790 \uc218\uc900 \ud45c\ud604\uc744 \ucd9c\ub825\ud558\ub294 \uc0c8\ub85c\uc6b4 LSTM\uc785\ub2c8\ub2e4. \n* \ubb38\uc790\uc5d0 \ub300\ud574 \uc2dc\ud000\uc2a4 \ubaa8\ub378\uc744 \uc218\ud589\ud558\ub824\uba74, \ubb38\uc790\ub97c \uc784\ubca0\ub529\ud574\uc57c \ud569\ub2c8\ub2e4. \n  \ubb38\uc790 \uc784\ubca0\ub529\uc740 \ubb38\uc790 LSTM\uc5d0 \ub300\ud55c \uc785\ub825\uc774 \ub429\ub2c8\ub2e4.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}