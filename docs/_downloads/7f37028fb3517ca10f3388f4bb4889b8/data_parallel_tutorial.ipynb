{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\uc120\ud0dd \uc0ac\ud56d: \ub370\uc774\ud130 \ubcd1\ub82c \ucc98\ub9ac (Data Parallelism)\n====================================================\n\n**\uae00\uc4f4\uc774**: `Sung Kim <https://github.com/hunkim>`_ and `Jenny Kang <https://github.com/jennykang>`_\n**\ubc88\uc5ed**: '\uc815\uc544\uc9c4 <https://github.com/ajin-jng>'\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 ``DataParallel`` (\ub370\uc774\ud130 \ubcd1\ub82c) \uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec GPU\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc95\uc744 \ubc30\uc6b0\uaca0\uc2b5\ub2c8\ub2e4.\n\nPyTorch\ub97c \ud1b5\ud574 GPU\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0 \uc27d\uc2b5\ub2c8\ub2e4. \uba3c\uc800, \ubaa8\ub378\uc744 GPU\uc5d0 \ub123\uc2b5\ub2c8\ub2e4:\n\n.. code:: python\n\n    device = torch.device(\"cuda:0\")\n    model.to(device)\n\n\uadf8 \ub2e4\uc74c\uc73c\ub85c\ub294 \ubaa8\ub4e0 Tensors \ub97c GPU\ub85c \ubcf5\uc0ac\ud569\ub2c8\ub2e4:\n\n.. code:: python\n\n    mytensor = my_tensor.to(device)\n\n''my_tensor.to(device)'' \ub97c \ud638\ucd9c \uc2dc \uc5d0\ub294 ''my_tensor'' \ub97c \ub2e4\uc2dc\uc4f0\ub294 \ub300\uc2e0 ''my_tensor'' \uc758 \ub610\ub2e4\ub978 \ubcf5\uc0ac\ubcf8\uc774 \uc0dd\uae34\ub2e4\ub294 \uc0ac\uc2e4\uc744 \uae30\uc5b5\ud558\uc2ed\uc2dc\uc624.\n\ub2f9\uc2e0\uc740 \uadf8\uac83\uc744 \uc0c8\ub85c\uc6b4 tensor \uc5d0 \uc18c\uc18d\uc2dc\ud0a4\uace0 GPU\uc5d0 \uadf8 tensor\ub97c \uc368\uc57c\ud569\ub2c8\ub2e4.\n\n\uc5ec\ub7ec GPU\ub97c \ud1b5\ud574 \uc55e\uacfc \ub4a4\uc758 \uc804\ud30c\ub97c \uc2e4\ud589\ud558\ub294 \uac83\uc740 \ub2f9\uc5f0\ud55c \uc77c \uc785\ub2c8\ub2e4.\n\uadf8\ub7ec\ub098 PyTorch\ub294 \uae30\ubcf8\uc801 \ud558\ub098\uc758 GPU\ub9cc \uc0ac\uc6a9\ud569\ub2c8\ub2e4. ``DataParallel`` \uc744 \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ubcd1\ub82c\ub85c \uc2e4\ud589\ud558\uc5ec \ub2e4\uc218\uc758 GPU \uc5d0\uc11c \uc27d\uac8c \uc791\uc5c5\uc744 \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n.. code:: python\n\n    model = nn.DataParallel(model)\n\n\uc774\uac83\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ud575\uc2ec\uc785\ub2c8\ub2e4. \uc790\uc138\ud55c \uc0ac\ud56d\ub4e4\uc5d0 \ub300\ud574\uc11c\ub294 \uc544\ub798\uc5d0\uc11c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubd88\ub7ec\uc624\uae30\uc640 \ub9e4\uac1c\ubcc0\uc218\n----------------------\n\nPyTorch \ubaa8\ub4c8\uc744 \ubd88\ub7ec\uc624\uace0 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc815\uc758\ud558\uc2ed\uc2dc\uc624.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# \ub9e4\uac1c\ubcc0\uc218\uc640 DataLoaders\ninput_size = 5\noutput_size = 2\n\nbatch_size = 30\ndata_size = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uae30\uae30\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub354\ubbf8(Dummy) \ub370\uc774\ud130\uc14b\n-----------------------\n\n\ub354\ubbf8(ramdom) \ub370\uc774\ud130\uc14b\uc744 \ub9cc\ub4e4\uc5b4 \ubd05\uc2dc\ub2e4. Getitem \ub9cc \uad6c\ud604\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RandomDataset(Dataset):\n\n    def __init__(self, size, length):\n        self.len = length\n        self.data = torch.randn(length, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n\nrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n                         batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac04\ub2e8\ud55c \ubaa8\ub378\n------------\n\n\ub370\ubaa8\ub97c \uc704\ud574 \ubaa8\ub378\uc740 \uc785\ub825\uc744 \ubc1b\uace0 \uc120\ud615 \uc5f0\uc0b0\uc744 \uc218\ud589\ud558\uba70 \ucd9c\ub825\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 ``DataParallel`` \uc758 \uc5b4\ub5a4 \ubaa8\ub378 (CNN, RNN, Capsule Net \ub4f1) \uc5d0\uc11c\ub4e0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\ub294 input\uacfc output\uc758 \ud06c\uae30\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud558\uae30 \uc704\ud574 \ubaa8\ub378\uc548\uc5d0 print \ubb38\uc744 \ub123\uc5c8\uc2b5\ub2c8\ub2e4.\n\ubb34\uc5c7\uc774 \ubc30\uce58 \uc21c\uc704 (batch rank) 0 \uc5d0 \ud504\ub9b0\ud2b8 \ub418\ub294\uc9c0 \uc8fc\uc758 \uae4a\uac8c \ubd10\uc8fc\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n    # \uc6b0\ub9ac\uc758 \ubaa8\ub378\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uacfc \ub370\uc774\ud130 \ubcd1\ub82c\uc758 \uad6c\ud604\n-----------------------------\n\n\uc774\uac83\uc740 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ud575\uc2ec \ubd80\ubd84\uc785\ub2c8\ub2e4. \uba3c\uc800, model instance \ub97c \ub9cc\ub4e4\uace0 \uac00\uc9c0\uace0 \uc788\ub294 GPU\uac00 \uc5ec\ub7ec\uac1c\uc778\uc9c0 \ud655\uc778\ud574\uc57c\ud569\ub2c8\ub2e4.\n\ub9cc\uc57d \ub2e4\uc218\uc758 GPU\ub97c \ubcf4\uc720\uc911\uc774\ub77c\uba74, ``nn.DataParallel`` \uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ub798\ud551 (wrapping) \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8\ub7f0 \ub2e4\uc74c ``model.to(device)`` \ub97c \ud1b5\ud558\uc5ec \ubaa8\ub378\uc744 GPU\uc5d0 \ub123\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \uc2e4\ud589\n-------------\n\n\uc774\uc81c \uc6b0\ub9ac\ub294 \uc785\ub825\uacfc \ucd9c\ub825 tensor\uc758 \ud06c\uae30\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uacb0\uacfc\n-------\n\nGPU\uac00 \uc5c6\uac70\ub098 \ud558\ub098\uc778 \uacbd\uc6b0 30\uac1c\uc758 \uc785\ub825\uacfc 30\uac1c\uc758 \ucd9c\ub825\uc744 \uc77c\uad04 \ucc98\ub9ac\ud558\uba74 \ubaa8\ub378\uc774 \uc608\uc0c1\ub300\ub85c 30\uc744 \uc785\ub825\ubc1b\uace0 30\uc744 \ucd9c\ub825\ud569\ub2c8\ub2e4.\n\ud558\uc9c0\ub9cc \ub9cc\uc57d \ub2f9\uc2e0\uc774 \ub2e4\uc218\uc758 GPU\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n2 GPUs\n~~~~~~\n\n2\uac1c\uc758 GPU\ub97c \uac16\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\uc2dc\ub420 \uac83 \uc785\ub2c8\ub2e4:\n\n.. code:: bash\n\n    # on 2 GPUs\n    Let's use 2 GPUs!\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n3 GPUs\n~~~~~~\n\n\ub9cc\uc57d \ub2f9\uc2e0\uc774 3\uac1c\uc758 GPU\ub97c \uac16\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\uc2dc\ub420 \uac83 \uc785\ub2c8\ub2e4:\n.. code:: bash\n\n    Let's use 3 GPUs!\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n        In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n8 GPUs\n~~~~~~~~~~~~~~\n\n\ub9cc\uc57d \ub2f9\uc2e0\uc774 8\uac1c\uc758 GPU\ub97c \uac16\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\uc2dc\ub420 \uac83 \uc785\ub2c8\ub2e4:\n\n.. code:: bash\n\n    Let's use 8 GPUs!\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n        In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc694\uc57d\n-------\n\nDataParallel\uc740 \ub2f9\uc2e0\uc758 \ub370\uc774\ud130\ub97c \uc790\ub3d9\uc73c\ub85c \ubd84\ud560\ud558\uace0 \uc5ec\ub7ec GPU\uc5d0 \uc788\ub294 \ub2e4\uc218\uc758 \ubaa8\ub378\uc5d0 \uc791\uc5c5\uc744 \uc9c0\uc2dc\ud569\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc774 \uc791\uc5c5\uc744 \uc644\ub8cc\ud558\uba74 DataParallel\uc740\n\uc0ac\uc6a9\uc790\uc5d0\uac8c \uacb0\uacfc\ub97c \ubc18\ud658\ud558\uae30 \uc804\uc5d0 \ubaa8\ub4e0 \uacb0\uacfc\ubb3c\ub4e4\uc744 \uc218\uc9d1\ud558\uc5ec \ubcd1\ud569\ud569\ub2c8\ub2e4.\n\n\ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \uc54c\uace0\uc2f6\ub2e4\uba74 \uc544\ub798 \uc8fc\uc18c\ub97c \ucc38\uace0\ud558\uc138\uc694.\nhttps://tutorials.pytorch.kr/beginner/former_torchies/parallelism_tutorial.html\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}