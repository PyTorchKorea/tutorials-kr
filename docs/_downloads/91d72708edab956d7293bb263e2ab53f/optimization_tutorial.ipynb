{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n[\ud30c\uc774\ud1a0\uce58(PyTorch) \uae30\ubcf8 \uc775\ud788\uae30](intro.html) ||\n[\ube60\ub978 \uc2dc\uc791](quickstart_tutorial.html) ||\n[\ud150\uc11c(Tensor)](tensorqs_tutorial.html) ||\n[Dataset\uacfc Dataloader](data_tutorial.html) ||\n[\ubcc0\ud615(Transform)](transforms_tutorial.html) ||\n[\uc2e0\uacbd\ub9dd \ubaa8\ub378 \uad6c\uc131\ud558\uae30](buildmodel_tutorial.html) ||\n[Autograd](autogradqs_tutorial.html) ||\n**\ucd5c\uc801\ud654(Optimization)** ||\n[\ubaa8\ub378 \uc800\uc7a5\ud558\uace0 \ubd88\ub7ec\uc624\uae30](saveloadrun_tutorial.html)\n\n# \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218 \ucd5c\uc801\ud654\ud558\uae30\n\n\uc774\uc81c \ubaa8\ub378\uacfc \ub370\uc774\ud130\uac00 \uc900\ube44\ub418\uc5c8\uc73c\ub2c8, \ub370\uc774\ud130\uc5d0 \ub9e4\uac1c\ubcc0\uc218\ub97c \ucd5c\uc801\ud654\ud558\uc5ec \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uace0, \uac80\uc99d\ud558\uace0, \ud14c\uc2a4\ud2b8\ud560 \ucc28\ub840\uc785\ub2c8\ub2e4.\n\ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \uacfc\uc815\uc740 \ubc18\ubcf5\uc801\uc778 \uacfc\uc815\uc744 \uac70\uce69\ub2c8\ub2e4; \uac01 \ubc18\ubcf5 \ub2e8\uacc4\uc5d0\uc11c \ubaa8\ub378\uc740 \ucd9c\ub825\uc744 \ucd94\uce21\ud558\uace0,\n\ucd94\uce21\uacfc \uc815\ub2f5 \uc0ac\uc774\uc758 \uc624\ub958(\\ *\uc190\uc2e4(loss)*\\ )\ub97c \uacc4\uc0b0\ud558\uace0, ([\uc774\uc804 \uc7a5](autograd_tutorial.html)\\ \uc5d0\uc11c \ubcf8 \uac83\ucc98\ub7fc)\n\ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c \uc624\ub958\uc758 \ub3c4\ud568\uc218(derivative)\ub97c \uc218\uc9d1\ud55c \ub4a4, \uacbd\uc0ac\ud558\uac15\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 **\ucd5c\uc801\ud654(optimize)**\\ \ud569\ub2c8\ub2e4.\n\uc774 \uacfc\uc815\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc124\uba85\uc740 [3Blue1Brown\uc758 \uc5ed\uc804\ud30c](https://www.youtube.com/watch?v=tIeHLnjs5U8)_ \uc601\uc0c1\uc744 \ucc38\uace0\ud558\uc138\uc694.\n\n## \uae30\ubcf8(Pre-requisite) \ucf54\ub4dc\n\uc774\uc804 \uc7a5\uc778 [Dataset\uacfc DataLoader](data_tutorial.html)\\ \uc640 [\uc2e0\uacbd\ub9dd \ubaa8\ub378 \uad6c\uc131\ud558\uae30](buildmodel_tutorial.html)\\ \uc5d0\uc11c\n\ucf54\ub4dc\ub97c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130(Hyperparameter)\n\n\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130(Hyperparameter)\ub294 \ubaa8\ub378 \ucd5c\uc801\ud654 \uacfc\uc815\uc744 \uc81c\uc5b4\ud560 \uc218 \uc788\ub294 \uc870\uc808 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\uc785\ub2c8\ub2e4.\n\uc11c\ub85c \ub2e4\ub978 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac12\uc740 \ubaa8\ub378 \ud559\uc2b5\uacfc \uc218\ub834\uc728(convergence rate)\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n(\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd(tuning)\uc5d0 \ub300\ud574 [\ub354 \uc54c\uc544\ubcf4\uae30](https://tutorials.pytorch.kr/beginner/hyperparameter_tuning_tutorial.html)_)\n\n\ud559\uc2b5 \uc2dc\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc815\uc758\ud569\ub2c8\ub2e4:\n - **\uc5d0\ud3ed(epoch) \uc218** - \ub370\uc774\ud130\uc14b\uc744 \ubc18\ubcf5\ud558\ub294 \ud69f\uc218\n - **\ubc30\uce58 \ud06c\uae30(batch size)** - \ub9e4\uac1c\ubcc0\uc218\uac00 \uac31\uc2e0\ub418\uae30 \uc804 \uc2e0\uacbd\ub9dd\uc744 \ud1b5\ud574 \uc804\ud30c\ub41c \ub370\uc774\ud130 \uc0d8\ud50c\uc758 \uc218\n - **\ud559\uc2b5\ub960(learning rate)** - \uac01 \ubc30\uce58/\uc5d0\ud3ed\uc5d0\uc11c \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc870\uc808\ud558\ub294 \ube44\uc728. \uac12\uc774 \uc791\uc744\uc218\ub85d \ud559\uc2b5 \uc18d\ub3c4\uac00 \ub290\ub824\uc9c0\uace0, \uac12\uc774 \ud06c\uba74 \ud559\uc2b5 \uc911 \uc608\uce21\ud560 \uc218 \uc5c6\ub294 \ub3d9\uc791\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\nbatch_size = 64\nepochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ucd5c\uc801\ud654 \ub2e8\uacc4(Optimization Loop)\n\n\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc124\uc815\ud55c \ub4a4\uc5d0\ub294 \ucd5c\uc801\ud654 \ub2e8\uacc4\ub97c \ud1b5\ud574 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uace0 \ucd5c\uc801\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ucd5c\uc801\ud654 \ub2e8\uacc4\uc758 \uac01 \ubc18\ubcf5(iteration)\uc744 **\uc5d0\ud3ed**\\ \uc774\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4.\n\n\ud558\ub098\uc758 \uc5d0\ud3ed\uc740 \ub2e4\uc74c \ub450 \ubd80\ubd84\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4:\n - **\ud559\uc2b5 \ub2e8\uacc4(train loop)** - \ud559\uc2b5\uc6a9 \ub370\uc774\ud130\uc14b\uc744 \ubc18\ubcf5(iterate)\ud558\uace0 \ucd5c\uc801\uc758 \ub9e4\uac1c\ubcc0\uc218\ub85c \uc218\ub834\ud569\ub2c8\ub2e4.\n - **\uac80\uc99d/\ud14c\uc2a4\ud2b8 \ub2e8\uacc4(validation/test loop)** - \ubaa8\ub378 \uc131\ub2a5\uc774 \uac1c\uc120\ub418\uace0 \uc788\ub294\uc9c0\ub97c \ud655\uc778\ud558\uae30 \uc704\ud574 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc744 \ubc18\ubcf5(iterate)\ud569\ub2c8\ub2e4.\n\n\ud559\uc2b5 \ub2e8\uacc4(training loop)\uc5d0\uc11c \uc77c\uc5b4\ub098\ub294 \uba87 \uac00\uc9c0 \uac1c\ub150\ub4e4\uc744 \uac04\ub7b5\ud788 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ucd5c\uc801\ud654 \ub2e8\uacc4(optimization loop)\ub97c \ubcf4\ub824\uba74\n`full-impl-label` \ubd80\ubd84\uc73c\ub85c \uac74\ub108\ub6f0\uc2dc\uba74 \ub429\ub2c8\ub2e4.\n\n### \uc190\uc2e4 \ud568\uc218(loss function)\n\n\ud559\uc2b5\uc6a9 \ub370\uc774\ud130\ub97c \uc81c\uacf5\ud558\uba74, \ud559\uc2b5\ub418\uc9c0 \uc54a\uc740 \uc2e0\uacbd\ub9dd\uc740 \uc815\ub2f5\uc744 \uc81c\uacf5\ud558\uc9c0 \uc54a\uc744 \ud655\ub960\uc774 \ub192\uc2b5\ub2c8\ub2e4. **\uc190\uc2e4 \ud568\uc218(loss function)**\\ \ub294\n\ud68d\ub4dd\ud55c \uacb0\uacfc\uc640 \uc2e4\uc81c \uac12 \uc0ac\uc774\uc758 \ud2c0\ub9b0 \uc815\ub3c4(degree of dissimilarity)\ub97c \uce21\uc815\ud558\uba70, \ud559\uc2b5 \uc911\uc5d0 \uc774 \uac12\uc744 \ucd5c\uc18c\ud654\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.\n\uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130 \uc0d8\ud50c\uc744 \uc785\ub825\uc73c\ub85c \uacc4\uc0b0\ud55c \uc608\uce21\uacfc \uc815\ub2f5(label)\uc744 \ube44\uad50\ud558\uc5ec \uc190\uc2e4(loss)\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\n\uc77c\ubc18\uc801\uc778 \uc190\uc2e4\ud568\uc218\uc5d0\ub294 \ud68c\uadc0 \ubb38\uc81c(regression task)\uc5d0 \uc0ac\uc6a9\ud558\ub294 [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\\ (\ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28(MSE; Mean Square Error))\ub098\n\ubd84\ub958(classification)\uc5d0 \uc0ac\uc6a9\ud558\ub294 [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (\uc74c\uc758 \ub85c\uadf8 \uc6b0\ub3c4(Negative Log Likelihood)),\n\uadf8\ub9ac\uace0 ``nn.LogSoftmax``\\ \uc640 ``nn.NLLLoss``\\ \ub97c \ud569\uce5c [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n\ub4f1\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ubaa8\ub378\uc758 \ucd9c\ub825 \ub85c\uc9d3(logit)\uc744 ``nn.CrossEntropyLoss``\\ \uc5d0 \uc804\ub2ec\ud558\uc5ec \ub85c\uc9d3(logit)\uc744 \uc815\uaddc\ud654\ud558\uace0 \uc608\uce21 \uc624\ub958\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \uc190\uc2e4 \ud568\uc218\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.\nloss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \uc635\ud2f0\ub9c8\uc774\uc800(Optimizer)\n\n\ucd5c\uc801\ud654\ub294 \uac01 \ud559\uc2b5 \ub2e8\uacc4\uc5d0\uc11c \ubaa8\ub378\uc758 \uc624\ub958\ub97c \uc904\uc774\uae30 \uc704\ud574 \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc870\uc815\ud558\ub294 \uacfc\uc815\uc785\ub2c8\ub2e4. **\ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998**\\ \uc740 \uc774 \uacfc\uc815\uc774 \uc218\ud589\ub418\ub294 \ubc29\uc2dd(\uc5ec\uae30\uc5d0\uc11c\ub294 \ud655\ub960\uc801 \uacbd\uc0ac\ud558\uac15\ubc95(SGD; Stochastic Gradient Descent))\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.\n\ubaa8\ub4e0 \ucd5c\uc801\ud654 \uc808\ucc28(logic)\ub294 ``optimizer`` \uac1d\uccb4\uc5d0 \ucea1\uc290\ud654(encapsulate)\ub429\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 SGD \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\uace0 \uc788\uc73c\uba70, PyTorch\uc5d0\ub294 ADAM\uc774\ub098 RMSProp\uacfc \uac19\uc740 \ub2e4\ub978 \uc885\ub958\uc758 \ubaa8\ub378\uacfc \ub370\uc774\ud130\uc5d0\uc11c \ub354 \uc798 \ub3d9\uc791\ud558\ub294\n[\ub2e4\uc591\ud55c \uc635\ud2f0\ub9c8\uc774\uc800](https://pytorch.org/docs/stable/optim.html)\\ \uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ud559\uc2b5\ud558\ub824\ub294 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\uc640 \ud559\uc2b5\ub960(learning rate) \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ub4f1\ub85d\ud558\uc5ec \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud559\uc2b5 \ub2e8\uacc4(loop)\uc5d0\uc11c \ucd5c\uc801\ud654\ub294 \uc138\ub2e8\uacc4\ub85c \uc774\ub904\uc9d1\ub2c8\ub2e4:\n * ``optimizer.zero_grad()``\\ \ub97c \ud638\ucd9c\ud558\uc5ec \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\uc758 \ubcc0\ud654\ub3c4\ub97c \uc7ac\uc124\uc815\ud569\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c \ubcc0\ud654\ub3c4\ub294 \ub354\ud574\uc9c0\uae30(add up) \ub54c\ubb38\uc5d0 \uc911\ubcf5 \uacc4\uc0b0\uc744 \ub9c9\uae30 \uc704\ud574 \ubc18\ubcf5\ud560 \ub54c\ub9c8\ub2e4 \uba85\uc2dc\uc801\uc73c\ub85c 0\uc73c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4.\n * ``loss.backwards()``\\ \ub97c \ud638\ucd9c\ud558\uc5ec \uc608\uce21 \uc190\uc2e4(prediction loss)\uc744 \uc5ed\uc804\ud30c\ud569\ub2c8\ub2e4. PyTorch\ub294 \uac01 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\n * \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud55c \ub4a4\uc5d0\ub294 ``optimizer.step()``\\ \uc744 \ud638\ucd9c\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c \uc218\uc9d1\ub41c \ubcc0\ud654\ub3c4\ub85c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc870\uc815\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## \uc804\uccb4 \uad6c\ud604\n\n\ucd5c\uc801\ud654 \ucf54\ub4dc\ub97c \ubc18\ubcf5\ud558\uc5ec \uc218\ud589\ud558\ub294 ``train_loop``\\ \uc640 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub85c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud558\ub294 ``test_loop``\\ \ub97c \uc815\uc758\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # \ubaa8\ub378\uc744 \ud559\uc2b5(train) \ubaa8\ub4dc\ub85c \uc124\uc815\ud569\ub2c8\ub2e4 - \ubc30\uce58 \uc815\uaddc\ud654(Batch Normalization) \ubc0f \ub4dc\ub86d\uc544\uc6c3(Dropout) \ub808\uc774\uc5b4\ub4e4\uc5d0 \uc911\uc694\ud569\ub2c8\ub2e4.\n    # \uc774 \uc608\uc2dc\uc5d0\uc11c\ub294 \uc5c6\uc5b4\ub3c4 \ub418\uc9c0\ub9cc, \ubaa8\ubc94 \uc0ac\ub840\ub97c \uc704\ud574 \ucd94\uac00\ud574\ub450\uc5c8\uc2b5\ub2c8\ub2e4.\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # \uc608\uce21(prediction)\uacfc \uc190\uc2e4(loss) \uacc4\uc0b0\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # \uc5ed\uc804\ud30c\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    # \ubaa8\ub378\uc744 \ud3c9\uac00(eval) \ubaa8\ub4dc\ub85c \uc124\uc815\ud569\ub2c8\ub2e4 - \ubc30\uce58 \uc815\uaddc\ud654(Batch Normalization) \ubc0f \ub4dc\ub86d\uc544\uc6c3(Dropout) \ub808\uc774\uc5b4\ub4e4\uc5d0 \uc911\uc694\ud569\ub2c8\ub2e4.\n    # \uc774 \uc608\uc2dc\uc5d0\uc11c\ub294 \uc5c6\uc5b4\ub3c4 \ub418\uc9c0\ub9cc, \ubaa8\ubc94 \uc0ac\ub840\ub97c \uc704\ud574 \ucd94\uac00\ud574\ub450\uc5c8\uc2b5\ub2c8\ub2e4.\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # torch.no_grad()\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud14c\uc2a4\ud2b8 \uc2dc \ubcc0\ud654\ub3c4(gradient)\ub97c \uacc4\uc0b0\ud558\uc9c0 \uc54a\ub3c4\ub85d \ud569\ub2c8\ub2e4.\n    # \uc774\ub294 requires_grad=True\ub85c \uc124\uc815\ub41c \ud150\uc11c\ub4e4\uc758 \ubd88\ud544\uc694\ud55c \ubcc0\ud654\ub3c4 \uc5f0\uc0b0 \ubc0f \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \ub610\ud55c \uc904\uc5ec\uc90d\ub2c8\ub2e4.\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc190\uc2e4 \ud568\uc218\uc640 \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \ucd08\uae30\ud654\ud558\uace0 ``train_loop``\\ \uc640 ``test_loop``\\ \uc5d0 \uc804\ub2ec\ud569\ub2c8\ub2e4.\n\ubaa8\ub378\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc54c\uc544\ubcf4\uae30 \uc704\ud574 \uc790\uc720\ub86d\uac8c \uc5d0\ud3ed(epoch) \uc218\ub97c \uc99d\uac00\uc2dc\ucf1c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ub354 \uc77d\uc5b4\ubcf4\uae30\n- [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n- [torch.optim](https://pytorch.org/docs/stable/optim.html)\n- [Warmstart Training a Model](https://tutorials.pytorch.kr/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}