{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec(Profiler)\n============================\n\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 \uc5b4\ub5bb\uac8c PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \ubaa8\ub378\uc758\n\uc5f0\uc0b0\uc790\ub4e4\uc774 \uc18c\ube44\ud558\ub294 \uba54\ubaa8\ub9ac\uc640 \uc2dc\uac04\uc744 \uce21\uc815\ud558\ub294 \ubc29\ubc95\uc744 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uac1c\uc694\n----\n\nPyTorch\ub294 \uc0ac\uc6a9\uc790\uac00 \ubaa8\ub378 \ub0b4\uc758 \uc5f0\uc0b0 \ube44\uc6a9\uc774 \ud070(expensive) \uc5f0\uc0b0\uc790\ub4e4\uc774\n\ubb34\uc5c7\uc778\uc9c0 \uc54c\uace0\uc2f6\uc744 \ub54c \uc720\uc6a9\ud558\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uac04\ub2e8\ud55c \ud504\ub85c\ud30c\uc77c\ub7ec API\ub97c\n\ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5(performance)\uc744 \ubd84\uc11d\ud558\ub824\uace0 \ud560 \ub54c \uc5b4\ub5bb\uac8c\n\ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud574\uc57c \ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \uac04\ub2e8\ud55c ResNet \ubaa8\ub378\uc744\n\uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc124\uc815(Setup)\n-----------\n\n`torch` \uc640 `torchvision` \uc744 \uc124\uce58\ud558\uae30 \uc704\ud574\uc11c \uc544\ub798\uc758 \ucee4\ub9e8\ub4dc\ub97c \uc785\ub825\ud569\ub2c8\ub2e4:\n\n``` {.sh}\npip install torch torchvision\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub2e8\uacc4(Steps)\n===========\n\n1.  \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4 \ubd88\ub7ec\uc624\uae30\n2.  \uac04\ub2e8\ud55c ResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654 \ud558\uae30\n3.  \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud589\uc2dc\uac04 \ubd84\uc11d\ud558\uae30\n4.  \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba54\ubaa8\ub9ac \uc18c\ube44 \ubd84\uc11d\ud558\uae30\n5.  \ucd94\uc801\uae30\ub2a5 \uc0ac\uc6a9\ud558\uae30\n6.  Examining stack traces\n7.  Using profiler to analyze long-running jobs\n\n1. \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4 \ubd88\ub7ec\uc624\uae30\n-------------------------------\n\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 `torch` \uc640 `torchvision.models`, \uadf8\ub9ac\uace0 `profiler`\n\ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision.models as models\nfrom torch.profiler import profile, record_function, ProfilerActivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \uac04\ub2e8\ud55c ResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654 \ud558\uae30\n=====================================\n\nResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ub97c \ub9cc\ub4e4\uace0 \uc785\ub825\uac12\uc744 \uc900\ube44\ud569\ub2c8\ub2e4 :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud589\uc2dc\uac04 \ubd84\uc11d\ud558\uae30\n==========================================\n\nPyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ucee8\ud14d\uc2a4\ud2b8 \uba54\ub2c8\uc800(context manager)\ub97c \ud1b5\ud574 \ud65c\uc131\ud654\ub418\uace0,\n\uc5ec\ub7ec \ub9e4\uac1c\ubcc0\uc218\ub97c \ubc1b\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc720\uc6a9\ud55c \uba87 \uac00\uc9c0 \ub9e4\uac1c\ubcc0\uc218\ub294 \ub2e4\uc74c\uacfc\n\uac19\uc2b5\ub2c8\ub2e4:\n\n-   \n\n    `activities` - a list of activities to profile:\n\n    :   -   `ProfilerActivity.CPU` - PyTorch operators, TorchScript\n            functions and user-defined code labels (see\n            `record_function` below);\n        -   `ProfilerActivity.CUDA` - on-device CUDA kernels;\n\n-   `record_shapes` - \uc5f0\uc0ac\uc790 \uc785\ub825(input)\uc758 shape\uc744 \uae30\ub85d\ud560\uc9c0 \uc5ec\ubd80;\n\n-   `profile_memory` - \ubaa8\ub378\uc758 \ud150\uc11c(Tensor)\ub4e4\uc774 \uc18c\ube44\ud558\ub294 \uba54\ubaa8\ub9ac \uc591\uc744\n    \ubcf4\uace0(report)\ud560\uc9c0 \uc5ec\ubd80;\n\n-   `use_cuda` - CUDA \ucee4\ub110\uc758 \uc2e4\ud589\uc2dc\uac04\uc744 \uce21\uc815\ud560\uc9c0 \uc5ec\ubd80;\n\nNote: when using CUDA, profiler also shows the runtime CUDA events\noccuring on the host.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ub5bb\uac8c \uc2e4\ud589\uc2dc\uac04\uc744 \ubd84\uc11d\ud558\ub294\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`record_function` \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc784\uc758\uc758 \ucf54\ub4dc \ubc94\uc704\uc5d0 \uc0ac\uc6a9\uc790\uac00\n\uc9c0\uc815\ud55c \uc774\ub984\uc73c\ub85c \ub808\uc774\ube14(label)\uc744 \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc704 \uc608\uc81c\uc5d0\uc11c\ub294\n`model_inference` \ub97c \ub808\uc774\ube14\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.)\n\n\ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uba74 \ud504\ub85c\ud30c\uc77c\ub7ec \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub85c \uac10\uc2f8\uc9c4(wrap) \ucf54\ub4dc\n\ubc94\uc704\ub97c \uc2e4\ud589\ud558\ub294 \ub3d9\uc548 \uc5b4\ub5a4 \uc5f0\uc0b0\uc790\ub4e4\uc774 \ud638\ucd9c\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ub9cc\uc57d \uc5ec\ub7ec \ud504\ub85c\ud30c\uc77c\ub7ec\uc758 \ubc94\uc704\uac00 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \uacbd\uc6b0(\uc608. PyTorch \uc4f0\ub808\ub4dc\uac00\n\ubcd1\ub82c\ub85c \uc2e4\ud589 \uc911\uc778 \uacbd\uc6b0), \uac01 \ud504\ub85c\ud30c\uc77c\ub9c1 \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub294 \uac01\uac01\uc758 \ubc94\uc704 \ub0b4\uc758\n\uc5f0\uc0b0\uc790\ub4e4\ub9cc \ucd94\uc801(track)\ud569\ub2c8\ub2e4. \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ub610\ud55c `torch.jit._fork` \ub85c\n\uc2e4\ud589\ub41c \ube44\ub3d9\uae30 \uc791\uc5c5\uacfc (\uc5ed\uc804\ud30c \ub2e8\uacc4\uc758 \uacbd\uc6b0) `backward()` \uc758 \ud638\ucd9c\ub85c \uc2e4\ud589\ub41c\n\uc5ed\uc804\ud30c \uc5f0\uc0b0\uc790\ub4e4\ub3c4 \uc790\ub3d9\uc73c\ub85c \ud504\ub85c\ud30c\uc77c\ub9c1\ud569\ub2c8\ub2e4.\n\n\uc704 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud55c \ud1b5\uacc4\ub97c \ucd9c\ub825\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(\uba87\uba87 \uc5f4\uc744 \uc81c\uc678\ud558\uace0) \ucd9c\ub825\uac12\uc774 \uc774\ub807\uac8c \ubcf4\uc77c \uac83\uc785\ub2c8\ub2e4:\n\n``` {.sh}\n```\n\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- Name Self CPU CPU total CPU time avg \\# of Calls\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- model\\_inference 5.509ms 57.503ms 57.503ms 1\naten::conv2d 231.000us 31.931ms 1.597ms 20 aten::convolution 250.000us\n31.700ms 1.585ms 20 aten::\\_convolution 336.000us 31.450ms 1.573ms 20\naten::mkldnn\\_convolution 30.838ms 31.114ms 1.556ms 20 aten::batch\\_norm\n211.000us 14.693ms 734.650us 20 aten::\\_batch\\_norm\\_impl\\_index\n319.000us 14.482ms 724.100us 20 aten::native\\_batch\\_norm 9.229ms\n14.109ms 705.450us 20 aten::mean 332.000us 2.631ms 125.286us 21\naten::select 1.668ms 2.292ms 8.988us 255\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--Self CPU time total: 57.549ms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc608\uc0c1\ud588\ub358 \ub300\ub85c, \ub300\ubd80\ubd84\uc758 \uc2dc\uac04\uc774 \ud569\uc131\uacf1(convolution) \uc5f0\uc0b0(\ud2b9\ud788 `MKL-DNN`\n\uc744 \uc9c0\uc6d0\ud558\ub3c4\ub85d \ucef4\ud30c\uc77c\ub41c PyTorch\uc758 \uacbd\uc6b0\uc5d0\ub294 `mkldnn_convolution` )\uc5d0\uc11c\n\uc18c\uc694\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uacb0\uacfc \uc5f4\ub4e4 \uc911) Self CPU time\uacfc CPU\ntime\uc758 \ucc28\uc774\uc5d0 \uc720\uc758\ud574\uc57c \ud569\ub2c8\ub2e4 -\uc5f0\uc0b0\uc790\ub294 \ub2e4\ub978 \uc5f0\uc0b0\uc790\ub4e4\uc744 \ud638\ucd9c\ud560 \uc218\n\uc788\uc73c\uba70, Self CPU time\uc5d0\ub294 \ud558\uc704(child) \uc5f0\uc0b0\uc790 \ud638\ucd9c\uc5d0\uc11c \ubc1c\uc0dd\ud55c \uc2dc\uac04\uc744\n\uc81c\uc678\ud574\uc11c, Totacl CPU time\uc5d0\ub294 \ud3ec\ud568\ud574\uc11c \ud45c\uc2dc\ud569\ub2c8\ub2e4. You can choose to\nsort by the self cpu time by passing `sort_by=\"self_cpu_time_total\"`\ninto the `table` call.\n\n\ubcf4\ub2e4 \uc138\ubd80\uc801\uc778 \uacb0\uacfc \uc815\ubcf4 \ubc0f \uc5f0\uc0b0\uc790\uc758 \uc785\ub825 shape\uc744 \ud568\uaed8 \ubcf4\ub824\uba74\n`group_by_input_shape=True` \ub97c \uc778\uc790\ub85c \uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4 (note: this\nrequires running the profiler with `record_shapes=True`):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(\uba87\uba87 \uc5f4\uc744 \uc81c\uc678\ud558\uace0) \ucd9c\ub825\uac12\uc774 \uc774\ub807\uac8c \ubcf4\uc77c \uac83\uc785\ub2c8\ub2e4:\n\n``` {.sh}\n---------------------------------  ------------  -------------------------------------------\n                             Name     CPU total                                 Input Shapes\n---------------------------------  ------------  -------------------------------------------\n                  model_inference      57.503ms                                           []\n                     aten::conv2d       8.008ms      [5,64,56,56], [64,64,3,3], [], ..., []]\n                aten::convolution       7.956ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n               aten::_convolution       7.909ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n         aten::mkldnn_convolution       7.834ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n                     aten::conv2d       6.332ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n                aten::convolution       6.303ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n               aten::_convolution       6.273ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n         aten::mkldnn_convolution       6.233ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n                     aten::conv2d       4.751ms  [[5,256,14,14], [256,256,3,3], [], ..., []]\n---------------------------------  ------------  -------------------------------------------\nSelf CPU time total: 57.549ms\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the occurence of `aten::convolution` twice with different input\nshapes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Profiler can also be used to analyze performance of models executed on\nGPUs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18().cuda()\ninputs = torch.randn(5, 3, 224, 224).cuda()\n\nwith profile(activities=[\n        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Note: the first use of CUDA profiling may bring an extra overhead.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting table output:\n\n``` {.sh}\n-------------------------------------------------------  ------------  ------------\n                                                   Name     Self CUDA    CUDA total\n-------------------------------------------------------  ------------  ------------\n                                        model_inference       0.000us      11.666ms\n                                           aten::conv2d       0.000us      10.484ms\n                                      aten::convolution       0.000us      10.484ms\n                                     aten::_convolution       0.000us      10.484ms\n                             aten::_convolution_nogroup       0.000us      10.484ms\n                                      aten::thnn_conv2d       0.000us      10.484ms\n                              aten::thnn_conv2d_forward      10.484ms      10.484ms\nvoid at::native::im2col_kernel<float>(long, float co...       3.844ms       3.844ms\n                                      sgemm_32x32x32_NN       3.206ms       3.206ms\n                                  sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n-------------------------------------------------------  ------------  ------------\nSelf CPU time total: 23.015ms\nSelf CUDA time total: 11.666ms\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the occurence of on-device kernels in the output (e.g.\n`sgemm_32x32x32_NN`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba54\ubaa8\ub9ac \uc18c\ube44 \ubd84\uc11d\ud558\uae30\n=============================================\n\nPyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ubaa8\ub378\uc758 \uc5f0\uc0b0\uc790\ub4e4\uc744 \uc2e4\ud589\ud558\uba70 (\ubaa8\ub378\uc758 \ud150\uc11c\ub4e4\uc774\n\uc0ac\uc6a9\ud558\uba70) \ud560\ub2f9(\ub610\ub294 \ud574\uc81c)\ud55c \uba54\ubaa8\ub9ac\uc758 \uc591\ub3c4 \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798 \ucd9c\ub825\n\uacb0\uacfc\uc5d0\uc11c \\'Self\\' memory\ub294 \ud574\ub2f9 \uc5f0\uc0b0\uc790\uc5d0 \uc758\ud574 \ud638\ucd9c\ub41c \ud558\uc704(child)\n\uc5f0\uc0b0\uc790\ub4e4\uc744 \uc81c\uc678\ud55c, \uc5f0\uc0b0\uc790 \uc790\uccb4\uc5d0 \ud560\ub2f9(\ud574\uc81c)\ub41c \uba54\ubaa8\ub9ac\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4.\n\uba54\ubaa8\ub9ac \ud504\ub85c\ud30c\uc77c\ub9c1 \uae30\ub2a5\uc744 \ud65c\uc131\ud654\ud558\ub824\uba74 `profile_memory=True` \ub97c \uc778\uc790\ub85c\n\uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nwith profile(activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(\uba87\uba87 \uc5f4\uc740 \uc81c\uc678\ud558\uc600\uc2b5\ub2c8\ub2e4)\n\n``` {.sh}\n```\n\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\nName CPU Mem Self CPU Mem \\# of Calls\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\naten::empty 94.79 Mb 94.79 Mb 121 aten::max\\_pool2d\\_with\\_indices 11.48\nMb 11.48 Mb 1 aten::addmm 19.53 Kb 19.53 Kb 1 aten::empty\\_strided 572 b\n572 b 25 aten::[resize]() 240 b 240 b 6 aten::abs 480 b 240 b 4\naten::add 160 b 160 b 20 aten::masked\\_select 120 b 112 b 1 aten::ne 122\nb 53 b 6 aten::eq 60 b 30 b 2\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--Self CPU time total: 53.064ms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(\uba87\uba87 \uc5f4\uc744 \uc81c\uc678\ud558\uace0) \ucd9c\ub825\uac12\uc774 \uc774\ub807\uac8c \ubcf4\uc77c \uac83\uc785\ub2c8\ub2e4:\n\n``` {.sh}\n```\n\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\nName CPU Mem Self CPU Mem \\# of Calls\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\naten::empty 94.79 Mb 94.79 Mb 121 aten::batch\\_norm 47.41 Mb 0 b 20\naten::\\_batch\\_norm\\_impl\\_index 47.41 Mb 0 b 20\naten::native\\_batch\\_norm 47.41 Mb 0 b 20 aten::conv2d 47.37 Mb 0 b 20\naten::convolution 47.37 Mb 0 b 20 aten::\\_convolution 47.37 Mb 0 b 20\naten::mkldnn\\_convolution 47.37 Mb 0 b 20 aten::max\\_pool2d 11.48 Mb 0 b\n1 aten::max\\_pool2d\\_with\\_indices 11.48 Mb 11.48 Mb 1\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--Self CPU time total: 53.064ms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. \ucd94\uc801\uae30\ub2a5 \uc0ac\uc6a9\ud558\uae30\n====================\n\n\ud504\ub85c\ud30c\uc77c\ub9c1 \uacb0\uacfc\ub294 `.json` \ud615\ud0dc\uc758 \ucd94\uc801 \ud30c\uc77c(trace file)\ub85c \ucd9c\ub825\ud560 \uc218\n\uc788\uc2b5\ub2c8\ub2e4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18().cuda()\ninputs = torch.randn(5, 3, 224, 224).cuda()\n\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    model(inputs)\n\nprof.export_chrome_trace(\"trace.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc0ac\uc6a9\uc790\ub294 Chrome \ube0c\ub77c\uc6b0\uc800( `chrome://tracing` )\uc5d0\uc11c \ucd94\uc801 \ud30c\uc77c\uc744 \ubd88\ub7ec\uc640\n\ud504\ub85c\ud30c\uc77c\ub41c \uc77c\ub828\uc758 \uc5f0\uc0b0\uc790\ub4e4\uacfc CUDA \ucee4\ub110\uc744 \uac80\ud1a0\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n![image](https://tutorials.pytorch.kr/_static/img/trace_img.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Examining stack traces\n=========================\n\nProfiler can be used to analyze Python and TorchScript stack traces:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    with_stack=True,\n) as prof:\n    model(inputs)\n\n# Print aggregated stats\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output might look like this (omitting some columns):\n\n``` {.sh}\n-------------------------  -----------------------------------------------------------\n                     Name  Source Location\n-------------------------  -----------------------------------------------------------\naten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n                           .../torch/nn/modules/conv.py(443): forward\n                           .../torch/nn/modules/module.py(1051): _call_impl\n                           .../site-packages/torchvision/models/resnet.py(63): forward\n                           .../torch/nn/modules/module.py(1051): _call_impl\naten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n                           .../torch/nn/modules/conv.py(443): forward\n                           .../torch/nn/modules/module.py(1051): _call_impl\n                           .../site-packages/torchvision/models/resnet.py(59): forward\n                           .../torch/nn/modules/module.py(1051): _call_impl\n-------------------------  -----------------------------------------------------------\nSelf CPU time total: 34.016ms\nSelf CUDA time total: 11.659ms\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the two convolutions and the two call sites in\n`torchvision/models/resnet.py` script.\n\n(Warning: stack tracing adds an extra profiling overhead.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Using profiler to analyze long-running jobs\n==============================================\n\nPyTorch profiler offers an additional API to handle long-running jobs\n(such as training loops). Tracing all of the execution can be slow and\nresult in very large trace files. To avoid this, use optional arguments:\n\n-   `schedule` - specifies a function that takes an integer argument\n    (step number) as an input and returns an action for the profiler,\n    the best way to use this parameter is to use\n    `torch.profiler.schedule` helper function that can generate a\n    schedule for you;\n-   `on_trace_ready` - specifies a function that takes a reference to\n    the profiler as an input and is called by the profiler each time the\n    new trace is ready.\n\nTo illustrate how the API works, let\\'s first consider the following\nexample with `torch.profiler.schedule` helper function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.profiler import schedule\n\nmy_schedule = schedule(\n    skip_first=10,\n    wait=5,\n    warmup=1,\n    active=3,\n    repeat=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Profiler assumes that the long-running job is composed of steps,\nnumbered starting from zero. The example above defines the following\nsequence of actions for the profiler:\n\n1.  Parameter `skip_first` tells profiler that it should ignore the\n    first 10 steps (default value of `skip_first` is zero);\n2.  After the first `skip_first` steps, profiler starts executing\n    profiler cycles;\n3.  Each cycle consists of three phases:\n    -   idling (`wait=5` steps), during this phase profiler is not\n        active;\n    -   warming up (`warmup=1` steps), during this phase profiler starts\n        tracing, but the results are discarded; this phase is used to\n        discard the samples obtained by the profiler at the beginning of\n        the trace since they are usually skewed by an extra overhead;\n    -   active tracing (`active=3` steps), during this phase profiler\n        traces and records data;\n4.  An optional `repeat` parameter specifies an upper bound on the\n    number of cycles. By default (zero value), profiler will execute\n    cycles as long as the job runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thus, in the example above, profiler will skip the first 15 steps, spend\nthe next step on the warm up, actively record the next 3 steps, skip\nanother 5 steps, spend the next step on the warm up, actively record\nanother 3 steps. Since the `repeat=2` parameter value is specified, the\nprofiler will stop the recording after the first two cycles.\n\nAt the end of each cycle profiler calls the specified `on_trace_ready`\nfunction and passes itself as an argument. This function is used to\nprocess the new trace - either by obtaining the table output or by\nsaving the output on disk as a trace file.\n\nTo send the signal to the profiler that the next step has started, call\n`prof.step()` function. The current profiler step is stored in\n`prof.step_num`.\n\nThe following example shows how to use all of the concepts above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n) as p:\n    for idx in range(8):\n        model(inputs)\n        p.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub354 \uc54c\uc544\ubcf4\uae30\n===========\n\n\ub2e4\uc74c \ub808\uc2dc\ud53c\uc640 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc77d\uc73c\uba70 \ud559\uc2b5\uc744 \uacc4\uc18d\ud574\ubcf4\uc138\uc694:\n\n-   `/recipes/recipes/benchmark`{.interpreted-text role=\"doc\"}\n-   `/intermediate/tensorboard_profiler_tutorial`{.interpreted-text\n    role=\"doc\"} \ud29c\ud1a0\ub9ac\uc5bc\n-   `/intermediate/tensorboard_tutorial`{.interpreted-text role=\"doc\"}\n    \ud29c\ud1a0\ub9ac\uc5bc\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}