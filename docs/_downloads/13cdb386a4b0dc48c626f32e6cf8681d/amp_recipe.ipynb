{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc790\ub3d9 \ud63c\ud569 \uc815\ubc00\ub3c4(Automatic Mixed Precision) \uac00\uc774\ub4dc\n==================================================\n\n**\uc800\uc790**: [Michael Carilli](https://github.com/mcarilli) **\uc5ed\uc790**:\n[\uc624\uc655\ud0dd](https://github.com/ohkingtaek)\n\n[torch.cuda.amp](https://pytorch.org/docs/stable/amp.html) \uc5d0\uc11c\ub294 \ud63c\ud569\n\uc815\ubc00\ub3c4(mixed precision)\ub97c \uc704\ud574 \ud3b8\ub9ac\ud55c \uba54\uc18c\ub4dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \ubc29\uc2dd\uc5d0\uc11c\ub294\n\uc77c\ubd80 \uc5f0\uc0b0\uc774 `torch.float32` (`float`) \ub370\uc774\ud130 \ud0c0\uc785\uc744 \uc0ac\uc6a9\ud558\uace0, \ub2e4\ub978\n\uc5f0\uc0b0\uc740 `torch.float16` (`half`) \uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc120\ud615 \uacc4\uce35\uc774\ub098\n\ud569\uc131\uacf1 \uac19\uc740 \uc5f0\uc0b0\uc740 `float16` \ub610\ub294 `bfloat16` \uc5d0\uc11c \ud6e8\uc52c \ube60\ub974\uac8c\n\uc2e4\ud589\ub429\ub2c8\ub2e4. \ubc18\uba74, \ud569\uacc4(sum), \ud3c9\uade0(mean), \ucd5c\ub300/\ucd5c\uc18c\uac12 \uc5f0\uc0b0\uacfc \uac19\uc740\nreduction \uc5f0\uc0b0\uc740 \uac12\uc758 \ubc94\uc704 \ubcc0\ub3d9\uc774 \ud06c\ubbc0\ub85c \ub354 \ub113\uc740 \ub3d9\uc801 \ubc94\uc704\ub97c \uc81c\uacf5\ud558\ub294\n`float32` \uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \ud63c\ud569 \uc815\ubc00\ub3c4\ub294 \uac01 \uc5f0\uc0b0\uc5d0 \uc801\ud569\ud55c \ub370\uc774\ud130 \ud0c0\uc785\uc744\n\ub9e4\uce6d\ud558\uc5ec, \ub124\ud2b8\uc6cc\ud06c\uc758 \uc2e4\ud589 \uc2dc\uac04\uacfc \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc904\uc774\ub294 \ub370 \ub3c4\uc6c0\uc744\n\uc90d\ub2c8\ub2e4.\n\n\uc77c\ubc18\uc801\uc73c\ub85c \\\"\uc790\ub3d9 \ud63c\ud569 \uc815\ubc00\ub3c4 \ud559\uc2b5\\\"\uc740\n[torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast)\n\uc640\n[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)\n\ub97c \ud568\uaed8 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\uc774 \uac00\uc774\ub4dc\uc5d0\uc11c\ub294 \uae30\ubcf8 \uc815\ubc00\ub3c4\uc5d0\uc11c \uac04\ub2e8\ud55c \ub124\ud2b8\uc6cc\ud06c\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud55c \ud6c4,\n`autocast` \uc640 `GradScaler` \ub97c \ucd94\uac00\ud558\uc5ec \ub3d9\uc77c\ud55c \uc2e0\uacbd\ub9dd\uc744 \ud63c\ud569 \uc815\ubc00\ub3c4\ub85c\n\uc2e4\ud589\ud574 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uacfc\uc815\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n\uc774 \uac00\uc774\ub4dc\ub294 \ud30c\uc774\uc36c \uc2a4\ud06c\ub9bd\ud2b8\ub85c \ub2e4\uc6b4\ub85c\ub4dc\ud558\uc5ec \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud544\uc694\ud55c\n\uc694\uad6c \uc0ac\ud56d\uc740 PyTorch 1.6 \uc774\uc0c1\uacfc CUDA\ub97c \uc9c0\uc6d0\ud558\ub294 GPU\uc785\ub2c8\ub2e4.\n\n\ud63c\ud569 \uc815\ubc00\ub3c4\ub294 \uc8fc\ub85c Tensor Core\uac00 \uc9c0\uc6d0\ub418\ub294 \uc544\ud0a4\ud14d\ucc98(Volta, Turing,\nAmpere)\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ub0c5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c\ub294 2\\~3\ubc30\uc758 \uc131\ub2a5\n\ud5a5\uc0c1\uc774 \ub098\ud0c0\ub0a0 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc804 \uc544\ud0a4\ud14d\ucc98(Kepler, Maxwell, Pascal)\uc5d0\uc11c\ub294\n\uc57d\uac04\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. GPU\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ud655\uc778\ud558\ub824\uba74\n`nvidia-smi` \uba85\ub839\uc744 \uc2e4\ud589\ud558\uc138\uc694.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch, time, gc\n\n# \uc2dc\uac04 \ucc98\ub9ac\uc5d0 \uc0ac\uc6a9\ud560 \ud568\uc218\ub4e4\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    torch.cuda.synchronize()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac04\ub2e8\ud55c \uc2e0\uacbd\ub9dd\n=============\n\n\ub2e4\uc74c\uacfc \uac19\uc740 \uc120\ud615 \uacc4\uce35\uacfc ReLU \uc5f0\uc0b0\uc758 \uc5f0\uc18d\uc740 \ud63c\ud569 \uc815\ubc00\ub3c4\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c\n\uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc904 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model(in_size, out_size, num_layers):\n    layers = []\n    for _ in range(num_layers - 1):\n        layers.append(torch.nn.Linear(in_size, in_size))\n        layers.append(torch.nn.ReLU())\n    layers.append(torch.nn.Linear(in_size, out_size))\n    return torch.nn.Sequential(*tuple(layers)).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`batch_size`, `in_size`, `out_size`, \uadf8\ub9ac\uace0 `num_layers` \ub294 GPU\uc5d0 \ucda9\ubd84\ud55c\n\uc791\uc5c5\ub7c9\uc744 \ubd80\uc5ec\ud558\uae30 \uc704\ud574 \ud06c\uac8c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c GPU\uac00 \ucda9\ubd84\ud788 \uc0ac\uc6a9\ub420\n\ub54c \ud63c\ud569 \uc815\ubc00\ub3c4\uac00 \uac00\uc7a5 \ud070 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc791\uc740 \uc2e0\uacbd\ub9dd\uc740 CPU\uc5d0\n\uc758\ud574 \uc81c\uc57d\uc744 \ubc1b\uc744 \uc218 \uc788\uc73c\uba70, \uc774 \uacbd\uc6b0 \ud63c\ud569 \uc815\ubc00\ub3c4\ub294 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uc9c0 \ubabb\ud560\n\uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc120\ud615 \uacc4\uce35\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ucc28\uc6d0\ub4e4\uc740 8\uc758 \ubc30\uc218\ub85c \uc124\uc815\ub418\uc5b4\nTensor Core\ub97c \uc9c0\uc6d0\ud558\ub294 GPU\uc5d0\uc11c Tensor Core\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c\n\uad6c\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4. (\uc544\ub798\n`Troubleshooting \ud574\uacb0 <troubleshooting>`{.interpreted-text role=\"ref\"}\n\ucc38\uc870)\n\n\uc5f0\uc2b5 \ubb38\uc81c: \uc0ac\uc6a9\ud560 \uc0ac\uc774\uc988\ub97c \ub2e4\uc591\ud558\uac8c \uc124\uc815\ud558\uc5ec \ud63c\ud569 \uc815\ubc00\ub3c4 \uc0ac\uc6a9 \uc2dc \uc131\ub2a5\n\ud5a5\uc0c1\uc774 \uc5b4\ub5bb\uac8c \ub2ec\ub77c\uc9c0\ub294\uc9c0 \ud655\uc778\ud574 \ubcf4\uc138\uc694.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 512 # 128, 256, 513\ub3c4 \uc2dc\ub3c4\ud574\ubcf4\uc138\uc694.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.set_default_device(device)\n\n# \uae30\ubcf8 \uc815\ubc00\ub3c4\ub85c \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n# \uc544\ub798\uc5d0\uc11c \uae30\ubcf8 \uc815\ubc00\ub3c4\uc640 \ud63c\ud569 \uc815\ubc00\ub3c4 \uc2e4\ud5d8 \ubaa8\ub450\uc5d0\uc11c \ub3d9\uc77c\ud55c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n# \ud63c\ud569 \uc815\ubc00\ub3c4\ub97c \ud65c\uc131\ud654\ud560 \ub54c \uc785\ub825\uc758 ``dtype`` \uc744 \uc218\ub3d9\uc73c\ub85c \ubcc0\uacbd\ud560 \ud544\uc694\ub294 \uc5c6\uc2b5\ub2c8\ub2e4.\ndata = [torch.randn(batch_size, in_size) for _ in range(num_batches)]\ntargets = [torch.randn(batch_size, out_size) for _ in range(num_batches)]\n\nloss_fn = torch.nn.MSELoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uae30\ubcf8 \uc815\ubc00\ub3c4\n===========\n\n`torch.cuda.amp` \uc5c6\uc774, \uc544\ub798\uc758 \uac04\ub2e8\ud55c \uc2e0\uacbd\ub9dd\uc740 \ubaa8\ub4e0 \uc5f0\uc0b0\uc744 \uae30\ubcf8 \uc815\ubc00\ub3c4\n(`torch.float32`) \ub85c \uc2e4\ud589\ud569\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        output = net(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nend_timer_and_print(\"Default precision:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`torch.autocast` \ucd94\uac00\ud558\ub294 \ubc29\ubc95\n==============================\n\n[torch.autocast](https://pytorch.org/docs/stable/amp.html#autocasting)\n\uc758 \uc778\uc2a4\ud134\uc2a4\ub294 \uc2a4\ud06c\ub9bd\ud2b8\uc758 \uc77c\ubd80 \uc601\uc5ed\uc744 \ud63c\ud569 \uc815\ubc00\ub3c4\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d,\n\ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub85c \uc791\ub3d9\ud569\ub2c8\ub2e4. \uc774 \uc601\uc5ed\uc5d0\uc11c CUDA \uc5f0\uc0b0\uc740 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uba74\uc11c\n\uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uae30 \uc704\ud574 `autocast` \uac00 \uc120\ud0dd\ud55c `dtype` \uc73c\ub85c \uc2e4\ud589\ub429\ub2c8\ub2e4. \uac01\n\uc5f0\uc0b0\uc5d0 \ub300\ud574 `autocast` \uac00 \uc120\ud0dd\ud558\ub294 \uc815\ubc00\ub3c4\uc640 \ud574\ub2f9 \uc815\ubc00\ub3c4\ub97c \uc120\ud0dd\ud558\ub294\n\uc0c1\ud669\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 [Autocast Op\nReference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference)\n\ub97c \ucc38\uc870\ud558\uc138\uc694.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(0): # 0 epoch\uc73c\ub85c \uc124\uc815\ud55c \uac83\uc740 \uc774 \uc139\uc158\uc744 \uc124\uba85\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n    for input, target in zip(data, targets):\n        # ``autocast`` \uc544\ub798\uc5d0\uc11c \uc21c\uc804\ud30c\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            # output\uc740 float16\uc785\ub2c8\ub2e4. \uc774\ub294 \uc120\ud615 \uacc4\uce35\uc774 ``autocast`` \uc5d0 \uc758\ud574 float16\uc73c\ub85c \ubcc0\ud658\ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss\ub294 float32\uc785\ub2c8\ub2e4. \uc774\ub294 ``mse_loss`` \uacc4\uce35\uc774 ``autocast`` \uc5d0 \uc758\ud574 float32\ub85c \ubcc0\ud658\ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n            assert loss.dtype is torch.float32\n\n        # ``autocast`` \uc5d0\uc11c backward() \uc804\uc5d0 \uc885\ub8cc\ud569\ub2c8\ub2e4.\n        # ``autocast`` \uc5d0\uc11c \uc5ed\uc804\ud30c\ub97c \uc2e4\ud589\ud558\ub294 \uac83\uc740 \uad8c\uc7a5\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n        # \uc5ed\uc804\ud30c \uc5f0\uc0b0\uc740 \ud574\ub2f9 \uc21c\uc804\ud30c \uc5f0\uc0b0\uc744 \uc704\ud574 ``autocast`` \uac00 \uc120\ud0dd\ud55c \uac83\uacfc \ub3d9\uc77c\ud55c ``dtype`` \uc73c\ub85c \uc2e4\ud589\ub429\ub2c8\ub2e4.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`GradScaler` \ucd94\uac00\ud558\ub294 \ubc29\ubc95\n==========================\n\n[Gradient\nscaling](https://pytorch.org/docs/stable/amp.html#gradient-scaling) \uc740\n\ud63c\ud569 \uc815\ubc00\ub3c4\ub85c \ud559\uc2b5\ud560 \ub54c \uc791\uc740 \ud06c\uae30\uc758 \ubcc0\ud654\ub3c4\uac00 0\uc73c\ub85c \uc0ac\ub77c\uc9c0\ub294\n(\\\"underflowing\\\") \ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.\n[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)\n\ub294 \ubcc0\ud654\ub3c4 \uc2a4\ucf00\uc77c\ub9c1 \ub2e8\uacc4\ub97c \ud3b8\ub9ac\ud558\uac8c \uc218\ud589\ud569\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \uc218\ub834 \uc2e4\ud589\uc758 \uc2dc\uc791\uc5d0\uc11c \uae30\ubcf8 \uc778\uc790\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud55c \ubc88 ``scaler`` \ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n# \uc2e0\uacbd\ub9dd\uc774 \uae30\ubcf8 ``GradScaler`` \uc778\uc218\ub85c \uc218\ub834\ud558\uc9c0 \uc54a\ub294\ub2e4\uba74, \uc774\uc288\ub97c \uc81c\ucd9c\ud574\uc8fc\uc138\uc694.\n# \uc218\ub834 \uc2e4\ud589 \uc804\uccb4\uc5d0\uc11c \ub3d9\uc77c\ud55c ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n# \ub3d9\uc77c\ud55c \uc2a4\ud06c\ub9bd\ud2b8\uc5d0\uc11c \uc5ec\ub7ec \ubc88\uc758 \uc218\ub834 \uc2e4\ud589\uc744 \uc218\ud589\ud560 \uacbd\uc6b0, \uac01 \uc2e4\ud589\uc740 \uc804\uc6a9\uc758 \uc0c8\ub85c\uc6b4 ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n# ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub294 \uac00\ubccd\uc2b5\ub2c8\ub2e4.\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(0): # 0 epoch\uc73c\ub85c \uc124\uc815\ud55c \uac83\uc740 \uc774 \uc139\uc158\uc744 \uc124\uba85\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n\n        # \uc190\uc2e4\uc744 \uc870\uc815\ud569\ub2c8\ub2e4. \uc870\uc815\ub41c \uc190\uc2e4\uc5d0 \ub300\ud574 ``backward()`` \ub97c \ud638\ucd9c\ud558\uc5ec \uc870\uc815\ub41c \ubcc0\ud654\ub3c4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n        scaler.scale(loss).backward()\n\n        # ``scaler.step()`` \uc740 \uba3c\uc800 \uc635\ud2f0\ub9c8\uc774\uc800\uc5d0 \ud560\ub2f9\ub41c \ub9e4\uac1c\ubcc0\uc218\uc758 \ubcc0\ud654\ub3c4\ub97c \ubcf5\uc6d0\ud569\ub2c8\ub2e4.\n        # \uc774 \ubcc0\ud654\ub3c4\uc5d0 ``inf`` \ub098 ``NaN`` \uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc73c\uba74 optimizer.step()\uc774 \ud638\ucd9c\ub429\ub2c8\ub2e4.\n        # \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 optimizer.step()\uc744 \uac74\ub108\ub701\ub2c8\ub2e4.\n        scaler.step(opt)\n\n        # \ub2e4\uc74c \ubc18\ubcf5\uc744 \uc704\ud574 \uc870\uc815\ub41c \uac12\uc744 \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.\n        scaler.update()\n\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\\"Automatic Mixed Precision\\\" \uc18c\uac1c\ud55c \uac83\ub4e4 \uac19\uc774 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\n============================================================\n\n(\ub2e4\uc74c \uc608\uc2dc\ub294 `autocast` \uc640 `GradScaler` \uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ud3b8\ub9ac\ud55c \uc778\uc790\uc778\n`enabled` \ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub9cc\uc57d False\ub85c \uc124\uc815\ub418\uba74, `autocast` \uc640\n`GradScaler` \uc758 \ud638\ucd9c\uc774 \ubb34\ud6a8\ud654\ub429\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 if/else \ubb38 \uc5c6\uc774 \uae30\ubcf8\n\uc815\ubc00\ub3c4\uc640 \ud63c\ud569 \uc815\ubc00\ub3c4 \uac04 \uc804\ud658\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nend_timer_and_print(\"Mixed precision:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubcc0\ud654\ub3c4 \ud655\uc778/\uc218\uc815\ud558\uae30 (\uc608: \ud074\ub9ac\ud551)\n=================================\n\n`scaler.scale(loss).backward()` \ub85c \uc0dd\uc131\ub41c \ubaa8\ub4e0 \ubcc0\ud654\ub3c4\ub294 \uc870\uc815\ub429\ub2c8\ub2e4. \ub9cc\uc57d\n`backward()` \uc640 `scaler.step(optimizer)` \uc0ac\uc774\uc5d0\uc11c \ud30c\ub77c\ubbf8\ud130\uc758 `.grad`\n\uc18d\uc131\uc744 \uc218\uc815\ud558\uac70\ub098 \ud655\uc778\ud558\uace0 \uc2f6\ub2e4\uba74, \uba3c\uc800\n[scaler.unscale\\_(optimizer)](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.unscale_)\n\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubcc0\ud654\ub3c4\ub97c \ubcf5\uc6d0\ud574\uc57c \ud569\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(0): # 0 epoch\uc73c\ub85c \uc124\uc815\ud55c \uac83\uc740 \uc774 \uc139\uc158\uc744 \uc124\uba85\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # \uc635\ud2f0\ub9c8\uc774\uc800\uc5d0 \ud560\ub2f9\ub41c \ud30c\ub77c\ubbf8\ud130\uc758 \ubcc0\ud654\ub3c4\ub97c \uc81c\uc790\ub9ac\uc5d0\uc11c \ubcf5\uc6d0\ud569\ub2c8\ub2e4.\n        scaler.unscale_(opt)\n\n        # \uc635\ud2f0\ub9c8\uc774\uc800\uc5d0 \ud560\ub2f9\ub41c \ud30c\ub77c\ubbf8\ud130\uc758 \ubcc0\ud654\ub3c4\uac00 \uc774\uc81c \ubcf5\uc6d0\ub418\uc5c8\uc73c\ubbc0\ub85c, \ud3c9\uc18c\uc640 \uac19\uc774 \ud074\ub9ac\ud551\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n        # \uc774\ub54c \ud074\ub9ac\ud551\uc5d0 \uc0ac\uc6a9\ud558\ub294 max_norm \uac12\uc740 \ubcc0\ud654\ub3c4 \uc870\uc815\uc774 \uc5c6\uc744 \ub54c\uc640 \ub3d9\uc77c\ud558\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc800\uc7a5/\uc7ac\uac1c\ud558\ub294 \ubc95\n================\n\nAmp\uac00 \ud65c\uc131\ud654 \uc0c1\ud0dc\uc5d0\uc11c \ube44\ud2b8 \ub2e8\uc704\uc758 \uc815\ud655\ub3c4\ub85c \uc800\uc7a5/\uc7ac\uac1c\ud558\ub824\uba74,\n[scaler.state\\_dict](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.state_dict)\n\ub098\n[scaler.load\\_state\\_dict](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.load_state_dict)\n\ub97c \uc0ac\uc6a9\ud558\uc138\uc694.\n\n\uc800\uc7a5\ud560 \ub54c\ub294, \uc77c\ubc18\uc801\uc778 \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \uc0c1\ud0dc\uc640 \ud568\uaed8 `scaler` \uc758 \uc0c1\ud0dc\ub3c4\n\uc800\uc7a5\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub97c \uac01 \ubc18\ubcf5\ud558\ub294 \uc2dc\uc791 \uc2dc\uc810, \uc989 \uc5b4\ub5a4 \uc21c\uc804\ud30c \uc804\uc5d0 \ud558\uac70\ub098,\n\ubc18\ubcf5\uc774 \ub05d\ub09c \ud6c4\uc5d0 `scaler.update()` \uc774\ud6c4\uc5d0 \uc218\ud589\ud558\uba74 \ub429\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "checkpoint = {\"model\": net.state_dict(),\n              \"optimizer\": opt.state_dict(),\n              \"scaler\": scaler.state_dict()}\n# \uc608\ub97c \ub4e4\uc5b4 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc791\uc131\ud558\ub824\uba74,\n# torch.save(checkpoint, \"filename\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc7ac\uac1c\ud560 \ub54c\ub294, \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \uc0c1\ud0dc\uc640 \ud568\uaed8 `scaler` \uc758 \uc0c1\ud0dc\ub3c4\n\ub85c\ub4dc\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc77d\uc73c\ub824\uba74,\n\n``` {.}\ndev = torch.cuda.current_device()\ncheckpoint = torch.load(\"filename\",\n                        map_location = lambda storage, loc: storage.cuda(dev))\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uccb4\ud06c\ud3ec\uc778\ud2b8\uac00 Amp \uc5c6\uc774 \uc0dd\uc131\ub41c \uacbd\uc6b0, Amp\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\uc744 \uc7ac\uac1c\ud558\uace0\n\uc2f6\ub2e4\uba74, \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c \ud3c9\uc18c\ucc98\ub7fc \uccb4\ud06c\ud3ec\uc778\ud2b8\uc5d0\uc11c \ub85c\ub4dc\ud569\ub2c8\ub2e4. \uc774\n\uccb4\ud06c\ud3ec\uc778\ud2b8\uc5d0\ub294 \uc800\uc7a5\ub41c `scaler` \uc0c1\ud0dc\uac00 \uc5c6\uc73c\ubbc0\ub85c \uc0c8\ub85c\uc6b4 `GradScaler`\n\uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n\ubc18\ub300\ub85c \uccb4\ud06c\ud3ec\uc778\ud2b8\uac00 Amp\ub85c \uc0dd\uc131\ub41c \uacbd\uc6b0, `Amp` \ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \ud6c8\ub828\uc744\n\uc7ac\uac1c\ud558\ub824\uba74, \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c \uccb4\ud06c\ud3ec\uc778\ud2b8\uc5d0\uc11c \ud3c9\uc18c\ucc98\ub7fc \ub85c\ub4dc\ud558\uace0,\n\uc800\uc7a5\ub41c `scaler` \uc0c1\ud0dc\ub294 \ubb34\uc2dc\ud558\uba74 \ub429\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ucd94\ub860/\ud3c9\uac00\n=========\n\n`autocast` \ub294 \ub2e8\ub3c5\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ucd94\ub860 \ub610\ub294 \ud3c9\uac00\uc758 \uc21c\uc804\ud30c\ub97c \uac10\uc300 \uc218\n\uc788\uc2b5\ub2c8\ub2e4. \uc774 \uacbd\uc6b0 `GradScaler` \ub294 \ud544\uc694\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uace0\uae09 \uc8fc\uc81c {#advanced-topics}\n=========\n\n\uace0\uae09 \uc0ac\uc6a9 \uc0ac\ub840\uc5d0 \ub300\ud55c \ub0b4\uc6a9\uc740 [Automatic Mixed Precision\nExamples](https://pytorch.org/docs/stable/notes/amp_examples.html) \ub97c\n\ucc38\uc870\ud558\uc138\uc694. \uc608\uc2dc\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ub0b4\uc6a9\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4:\n\n-   \ubcc0\ud654\ub3c4 \ucd95\uc801\n-   \ubcc0\ud654\ub3c4 \ud398\ub110\ud2f0/\uc774\uc911 \uc5ed\uc804\ud30c\n-   \ub2e4\uc911 \ubaa8\ub378, \uc635\ud2f0\ub9c8\uc774\uc800 \ub610\ub294 \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\ub294 \uc2e0\uacbd\ub9dd\n-   \ub2e4\uc911 GPU (`torch.nn.DataParallel` \ub610\ub294\n    `torch.nn.parallel.DistributedDataParallel`)\n-   \uc0ac\uc6a9\uc790 \uc815\uc758 autograd \ud568\uc218 (`torch.autograd.Function` \uc758 \uc11c\ube0c\ud074\ub798\uc2a4)\n\n\ub3d9\uc77c\ud55c \uc2a4\ud06c\ub9bd\ud2b8\uc5d0\uc11c \uc5ec\ub7ec \ubc88\uc758 \uc218\ub834 \uc2e4\ud589\uc744 \uc218\ud589\ud558\ub294 \uacbd\uc6b0, \uac01 \uc2e4\ud589\uc740\n\uc0c8\ub85c\uc6b4 `GradScaler` \uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. `GradScaler` \uc778\uc2a4\ud134\uc2a4\ub294\n\uac00\ubccd\uc2b5\ub2c8\ub2e4.\n\n\ub9cc\uc57d \uc0ac\uc6a9\uc790 \uc815\uc758 C++ \uc5f0\uc0b0\uc744 \ub514\uc2a4\ud328\ucc98\uc5d0 \ub4f1\ub85d\ud558\ub824\uba74, \ub514\uc2a4\ud328\ucc98 \ud29c\ud1a0\ub9ac\uc5bc\uc758\n[autocast\nsection](https://tutorials.pytorch.kr/advanced/dispatcher.html#autocast)\n\uc744 \ucc38\uc870\ud558\uc138\uc694.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Troubleshooting \ud574\uacb0 {#troubleshooting}\n====================\n\nAmp\ub97c \uc0ac\uc6a9\ud55c \uc18d\ub3c4 \ud5a5\uc0c1\uc774 \ubbf8\ubbf8\ud55c \uacbd\uc6b0\n------------------------------------\n\n1.  \ub124\ud2b8\uc6cc\ud06c\uac00 GPU(\ub4e4)\uc5d0 \ucda9\ubd84\ud55c \uc791\uc5c5\uc744 \uc81c\uacf5\ud558\uc9c0 \ubabb\ud558\uace0 \uc788\uc5b4, CPU\uc5d0 \uc758\ud55c\n    \ubcd1\ubaa9 \ud604\uc0c1\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uacbd\uc6b0 Amp\uc758 GPU \uc131\ub2a5\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub294\n    \uc911\uc694\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n    -   GPU\ub97c \ud3ec\ud654 \uc0c1\ud0dc\ub85c \ub9cc\ub4e4\uae30 \uc704\ud55c \ub300\ub7b5\uc801\uc778 \ubc29\ubc95\uc740, \uba54\ubaa8\ub9ac\n        \ubd80\uc871(OOM)\uc774 \ubc1c\uc0dd\ud558\uc9c0 \uc54a\ub294 \uc120\uc5d0\uc11c \uac00\ub2a5\ud55c \ud55c \ubc30\uce58 \ud06c\uae30\ub098 \ub124\ud2b8\uc6cc\ud06c\n        \ud06c\uae30\ub97c \ub298\ub9ac\ub294 \uac83\uc785\ub2c8\ub2e4.\n    -   \uacfc\ub3c4\ud55c CPU-GPU \ub3d9\uae30\ud654(\uc608: `.item()` \ud638\ucd9c\uc774\ub098 CUDA \ud150\uc11c\uc5d0\uc11c \uac12\uc744\n        \ucd9c\ub825\ud558\ub294 \uac83)\ub97c \ud53c\ud574\uc57c \ud569\ub2c8\ub2e4.\n    -   \uc0ac\uc18c\ud55c\ub370 \ub9ce\uc740 CUDA \uc5f0\uc0b0\ub4e4\uc744 \ud53c\ud558\uace0, \uac00\ub2a5\ud55c \ud55c \uc774\ub7ec\ud55c \uc5f0\uc0b0\ub4e4\uc744 \uba87\n        \uac1c\uc758 \ud070 CUDA \uc5f0\uc0b0\uc73c\ub85c \ud569\uce58\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.\n2.  \uc2e0\uacbd\ub9dd\uc774 GPU \uacc4\uc0b0 \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uacaa\uace0 \uc788\uc744 \uc218 \uc788\uc9c0\ub9cc (\ub9ce\uc740 `matmul`\n    \ub610\ub294 \ud569\uc131\uacf1 \uc5f0\uc0b0), \uc0ac\uc6a9 \uc911\uc778 GPU\uc5d0 \ud150\uc11c \ucf54\uc5b4\uac00 \uc5c6\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\n    \uacbd\uc6b0 \uc18d\ub3c4 \ud5a5\uc0c1\uc774 \uc801\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n3.  `matmul` \ucc28\uc6d0\uc774 Tensor Core\uc5d0 \uce5c\ud654\uc801\uc774\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. `matmul`\n    \uc5d0 \ucc38\uc5ec\ud558\ub294 \uc0ac\uc774\uc988\uac00 8\uc758 \ubc30\uc218\uc778\uc9c0 \ud655\uc778\ud574\uc57c \ud569\ub2c8\ub2e4. (\uc778\ucf54\ub354/\ub514\ucf54\ub354\uac00\n    \uc788\ub294 NLP \ubaa8\ub378\uc758 \uacbd\uc6b0, \uc774 \uc810\uc774 \ubbf8\ubb18\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \ud569\uc131\uacf1\n    \uc5f0\uc0b0\ub3c4 Tensor Core \uc0ac\uc6a9\uc744 \uc704\ud574 \uc720\uc0ac\ud55c \ud06c\uae30 \uc81c\uc57d\uc744 \uac00\uc84c\uc5c8\uc9c0\ub9cc, CuDNN\n    7.3 \ubc84\uc804 \uc774\ud6c4\ub85c\ub294 \uc774\ub7ec\ud55c \uc81c\uc57d\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740\n    [\uc5ec\uae30](https://github.com/NVIDIA/apex/issues/221#issuecomment-478084841)\n    \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.)\n\n\uc190\uc2e4\uc774 inf/NaN\uc778 \uacbd\uc6b0\n---------------------\n\n\uba3c\uc800 \uc2e0\uacbd\ub9dd\uc774 `\uace0\uae09 \uc0ac\uc6a9 \uc0ac\ub840 <advanced-topics>`{.interpreted-text\nrole=\"ref\"} \uc5d0 \ud574\ub2f9\ud558\ub294\uc9c0 \ud655\uc778\ud558\uc138\uc694. \ub610\ud55c [Prefer\nbinary\\_cross\\_entropy\\_with\\_logits over\nbinary\\_cross\\_entropy](https://pytorch.org/docs/stable/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy)\n\uc744 \ucc38\uc870\ud558\uc138\uc694.\n\nAmp \uc0ac\uc6a9\uc774 \uc62c\ubc14\ub974\ub2e4\uace0 \ud655\uc2e0\ud55c\ub2e4\uba74, \uc774\uc288\ub97c \uc81c\uae30\ud574\uc57c \ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uadf8\n\uc804\uc5d0 \ub2e4\uc74c \uc815\ubcf4\ub97c \uc218\uc9d1\ud558\ub294 \uac83\uc774 \ub3c4\uc6c0\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n1.  `autocast` \ub610\ub294 `GradScaler` \ub97c \uac01\uac01 \ube44\ud65c\uc131\ud654 (`enabled=False` \ub97c\n    \uc0dd\uc131\uc790\uc5d0 \uc804\ub2ec)\ud558\uace0 `inf` \ub098 `NaN` \uc774 \uc5ec\uc804\ud788 \ubc1c\uc0dd\ud558\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4.\n2.  \uc2e0\uacbd\ub9dd\uc758 \uc77c\ubd80(\uc608: \ubcf5\uc7a1\ud55c \uc190\uc2e4 \ud568\uc218)\uac00 \uc624\ubc84\ud50c\ub85c\uc6b0\ub418\ub294 \uac83\uc774\n    \uc758\uc2ec\ub41c\ub2e4\uba74, \ud574\ub2f9 \uc21c\uc804\ud30c \uc601\uc5ed\uc744 `float32` \ub85c \uc2e4\ud589\ud558\uace0 `inf` \ub098 `NaN`\n    \uc774 \ubc1c\uc0dd\ud558\ub294\uc9c0 \ud655\uc778\ud558\uc138\uc694. [The autocast\n    docstring](https://pytorch.org/docs/stable/amp.html#torch.autocast)\n    \uc758 \ub9c8\uc9c0\ub9c9 \ucf54\ub4dc \ub2e8\ub77d\uc5d0\uc11c `autocast` \ub97c \ub85c\uceec\ub85c \ube44\ud65c\uc131\ud654\ud558\uace0 \ud558\uc704\n    \uc601\uc5ed\uc758 \uc785\ub825\uc744 \uce90\uc2a4\ud305\ud558\uc5ec \ud558\uc704 \uc601\uc5ed\uc744 `float32` \ub85c \uc2e4\ud589\ud558\ub294 \ubc29\ubc95\uc744\n    \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ud0c0\uc785 \ubd88\uc77c\uce58 \uc624\ub958 (`CUDNN_STATUS_BAD_PARAM` \uc73c\ub85c \ub098\ud0c0\ub0a0 \uc218 \uc788\uc74c)\n---------------------------------------------------------------\n\n`Autocast` \ub294 \uce90\uc2a4\ud305\uc73c\ub85c \uc774\ub4dd\uc744 \uc5bb\uac70\ub098 \ud544\uc694\ud55c \uc5f0\uc0b0\ub4e4\uc744 \ubaa8\ub450 \ucc98\ub9ac\ud558\ub824\uace0\n\ud569\ub2c8\ub2e4. [\uba85\uc2dc\uc801\uc73c\ub85c \ub2e4\ub8e8\uc5b4\uc9c4\n\uc5f0\uc0b0\ub4e4](https://pytorch.org/docs/stable/amp.html#autocast-op-reference)\n\uc740 \uc218\uce58\uc801 \ud2b9\uc131\ubfd0\ub9cc \uc544\ub2c8\ub77c \uacbd\ud5d8\uc5d0 \uae30\ubc18\ud558\uc5ec \uc120\ud0dd\ub418\uc5c8\uc2b5\ub2c8\ub2e4. `Autocast` \uac00\n\ud65c\uc131\ud654\ub41c \uc21c\uc804\ud30c \uc601\uc5ed\uc774\ub098 \uadf8 \uc601\uc5ed\uc744 \ub530\ub978 \uc5ed\uc804\ud30c\uc5d0\uc11c \ud0c0\uc785 \ubd88\uc77c\uce58 \uc624\ub958\uac00\n\ubc1c\uc0dd\ud55c\ub2e4\uba74, `autocast` \uac00 \uc5b4\ub5a4 \uc5f0\uc0b0\uc744 \ub193\ucce4\uc744 \uac00\ub2a5\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc624\ub958\n\ucd94\uc801\ub0b4\uc6a9\uacfc \ud568\uaed8 \uc774\uc288\ub97c \uc81c\uae30\ud558\uc138\uc694. \uc138\ubd80 \uc815\ubcf4 \uc81c\uacf5\uc744 \uc704\ud574 \uc2a4\ud06c\ub9bd\ud2b8\ub97c\n\uc2e4\ud589\ud558\uae30 \uc804\uc5d0 `export TORCH_SHOW_CPP_STACKTRACES=1` \uc744 \uc124\uc815\ud558\uc5ec \uc5b4\ub290\n\ubc31\uc5d4\ub4dc \uc5f0\uc0b0\uc5d0\uc11c \uc2e4\ud328\ud558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}