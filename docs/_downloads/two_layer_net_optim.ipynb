{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch: optim\n--------------\n\n\ud558\ub098\uc758 \uc740\ub2c9 \uacc4\uce35(Hidden Layer)\uc744 \uac16\ub294 \uc644\uc804\ud788 \uc5f0\uacb0\ub41c ReLU \uc2e0\uacbd\ub9dd\uc5d0 \uc720\ud074\ub9ac\ub4dc\n\uac70\ub9ac(Euclidean Distance)\uc758 \uc81c\uacf1\uc744 \ucd5c\uc18c\ud654\ud558\uc5ec x\ub85c\ubd80\ud130 y\ub97c \uc608\uce21\ud558\ub3c4\ub85d \ud559\uc2b5\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\nPyTorch\uc758 nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e0\uacbd\ub9dd\uc744 \uad6c\ud604\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub3d9\uc548 \ud574\uc654\ub358 \uac83\ucc98\ub7fc \uc9c1\uc811 \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud558\ub294 \ub300\uc2e0, optim \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec\n\uac00\uc911\uce58\ub97c \uac31\uc2e0\ud560 Optimizer\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. optim \ud328\ud0a4\uc9c0\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ub525\ub7ec\ub2dd\uc5d0 \uc0ac\uc6a9\ud558\ub294\nSGD+momentum, RMSProp, Adam \ub4f1\uacfc \uac19\uc740 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654(Optimization) \uc54c\uace0\ub9ac\uc998\uc744\n\uc815\uc758\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n# N\uc740 \ubc30\uce58 \ud06c\uae30\uc774\uba70, D_in\uc740 \uc785\ub825\uc758 \ucc28\uc6d0\uc785\ub2c8\ub2e4;\n# H\ub294 \uc740\ub2c9 \uacc4\uce35\uc758 \ucc28\uc6d0\uc774\uba70, D_out\uc740 \ucd9c\ub825 \ucc28\uc6d0\uc785\ub2c8\ub2e4.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \uc785\ub825\uacfc \ucd9c\ub825\uc744 \uc800\uc7a5\ud558\uae30 \uc704\ud574 \ubb34\uc791\uc704 \uac12\uc744 \uac16\ub294 Tensor\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uacfc \uc190\uc2e4 \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\nloss_fn = torch.nn.MSELoss(size_average=False)\n\n# optim \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud560 Optimizer\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n# \uc5ec\uae30\uc11c\ub294 Adam\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4; optim \ud328\ud0a4\uc9c0\ub294 \ub2e4\ub978 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc744\n# \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. Adam \uc0dd\uc131\uc790\uc758 \uccab\ubc88\uc9f8 \uc778\uc790\ub294 \uac31\uc2e0\ud574\uc57c \ud558\ub294 Tensor\ub97c\n# Optimizer\uc5d0 \uc54c\ub824\uc90d\ub2c8\ub2e4.\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nfor t in range(500):\n    # \uc21c\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc5d0 x\ub97c \uc804\ub2ec\ud558\uc5ec \uc608\uc0c1\ud558\ub294 y \uac12\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n    y_pred = model(x)\n\n    # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4.\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n\n    # \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc804\uc5d0, Optimizer \uac1d\uccb4\ub97c \uc0ac\uc6a9\ud558\uc5ec (\ubaa8\ub378\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \uac00\uc911\uce58\uc778)\n    # \uac31\uc2e0\ud560 Variable\ub4e4\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \ubcc0\ud654\ub3c4\ub97c 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774\ub294 \uae30\ubcf8\uc801\uc73c\ub85c,\n    # .backward()\ub97c \ud638\ucd9c\ud560 \ub54c\ub9c8\ub2e4 \ubcc0\ud654\ub3c4\uac00 \ubc84\ud37c(Buffer)\uc5d0 (\ub36e\uc5b4\uc4f0\uc9c0 \uc54a\uace0) \ub204\uc801\ub418\uae30\n    # \ub54c\ubb38\uc785\ub2c8\ub2e4. \ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 torch.autograd.backward\uc5d0 \ub300\ud55c \ubb38\uc11c\ub97c \ucc38\uc870\ud558\uc138\uc694.\n    optimizer.zero_grad()\n\n    # \uc5ed\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n    loss.backward()\n\n    # Optimizer\uc758 step \ud568\uc218\ub97c \ud638\ucd9c\ud558\uba74 \ub9e4\uac1c\ubcc0\uc218\uac00 \uac31\uc2e0\ub429\ub2c8\ub2e4.\n    optimizer.step()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}