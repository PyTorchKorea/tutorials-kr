{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nnn \ud328\ud0a4\uc9c0\n==========\n\nautograd\uc5d0 \uc644\ubcbd\ud788 \ud1b5\ud569\ub418\uac8c \ud558\uae30 \uc704\ud574 nn \ud328\ud0a4\uc9c0\ub97c \uc7ac\uc124\uacc4\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\uc774\uc81c \ubb34\uc5c7\uc774 \ubcc0\uacbd\ub418\uc5c8\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n**\ucee8\ud14c\uc774\ub108\ub97c autograd\ub85c \uad50\uccb4:**\n\n    \uc774\uc81c \ub354\uc774\uc0c1 ``ConcatTable`` \uac19\uc740 \ucee8\ud14c\uc774\ub108\ub098 ``CAddTable`` \uac19\uc740 \ubaa8\ub4c8, \ub610\ub294\n    nngraph\ub97c \uc774\uc6a9\ud558\uac70\ub098 \ub514\ubc84\uae45\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub300\uc2e0 autograd\ub97c \uc774\uc6a9\ud558\uc5ec \ub354 \uae54\ub054\ud558\uac8c\n    \uc2e0\uacbd\ub9dd\uc744 \uc815\uc758\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4,\n\n    * ``output = nn.CAddTable():forward({input1, input2})`` \ub300\uc2e0,\n      ``output = input1 + input2`` \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n    * ``output = nn.MulConstant(0.5):forward(input)`` \ub300\uc2e0,\n      ``output = input * 0.5`` \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n**\uc0c1\ud0dc(state)\ub294 \ubaa8\ub4c8 \ub0b4\uc5d0 \uc800\uc7a5\ub418\uc9c0 \uc54a\uace0, \uc2e0\uacbd\ub9dd \uadf8\ub798\ud504 \uc0c1\uc5d0 \uc874\uc7ac\ud569\ub2c8\ub2e4:**\n\n    \ub355\ubd84\uc5d0 \uc21c\ud658\uc2e0\uacbd\ub9dd\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc774 \ub354 \uac04\ub2e8\ud574\uc84c\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc21c\ud658\uc2e0\uacbd\ub9dd\uc744 \ub9cc\ub4e4\n    \ub54c, \ub354 \uc774\uc0c1 \uac00\uc911\uce58(weight)\ub97c \uacf5\uc720\ud558\ub294 \uac83\uc5d0 \ub300\ud574\uc11c\ub294 \uc0dd\uac01\ud560 \ud544\uc694\uc5c6\uc774 \ub3d9\uc77c\ud55c\n    Linear \uacc4\uce35\uc744 \uc5ec\ub7ec \ucc28\ub840 \ud638\ucd9c\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n    .. figure:: /_static/img/torch-nn-vs-pytorch-nn.png\n       :alt: torch-nn-vs-pytorch-nn\n\n       torch-nn-vs-pytorch-nn\n\n**\uac04\uc18c\ud654\ub41c \ub514\ubc84\uae45:**\n\n    \ub514\ubc84\uae45\uc740 Python\uc758 pdb \ub514\ubc84\uac70\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc9c1\uad00\uc801\uc774\uba70,\n    **\ub514\ubc84\uac70\uc640 \uc2a4\ud0dd \ucd94\uc801(stack trace)\uc740 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud55c \uacf3\uc5d0\uc11c \uc815\ud655\ud788 \uba48\ucda5\ub2c8\ub2e4.**\n    \uc774\uc81c \ubcf4\uc774\ub294 \ub300\ub85c \uc5bb\uc744 \uac83\uc785\ub2c8\ub2e4. (What you see is what you get.)\n\n\uc608\uc81c1: \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd(ConvNet)\n-----------------------------\n\n\uc774\uc81c \uc5b4\ub5bb\uac8c \uc791\uc740 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc744 \ub9cc\ub4dc\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ubaa8\ub4e0 \uc2e0\uacbd\ub9dd\uc740 \uae30\ubcf8 \ud074\ub798\uc2a4\uc778 ``nn.Module`` \ub85c\ubd80\ud130 \ud30c\uc0dd\ub429\ub2c8\ub2e4:\n\n-  \uc0dd\uc131\uc790\uc5d0\uc11c\ub294 \uc0ac\uc6a9\ud560 \ubaa8\ub4e0 \uacc4\uce35\uc744 \uc120\uc5b8\ud569\ub2c8\ub2e4.\n-  \uc21c\uc804\ud30c \ud568\uc218\uc5d0\uc11c\ub294 \uc2e0\uacbd\ub9dd \ubaa8\ub378\uc774 \uc785\ub825\uc5d0\uc11c \ucd9c\ub825\uae4c\uc9c0 \uc5b4\ub5bb\uac8c \uc2e4\ud589\ub418\ub294\uc9c0\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MNISTConvNet(nn.Module):\n\n    def __init__(self):\n        # \uc5ec\uae30\uc5d0 \ubaa8\ub4e0 \ubaa8\ub4c8\uc744 \uc0dd\uc131\ud574\ub450\uace0,\n        # \ub098\uc911\uc5d0 \uc5ec\uae30\uc5d0\uc11c \uc120\uc5b8\ud574\ub454 \uc774\ub984\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n        super(MNISTConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, 5)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(10, 20, 5)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    # \uc21c\uc804\ud30c \ud568\uc218\uc5d0\uc11c\ub294 \uc2e0\uacbd\ub9dd\uc758 \uad6c\uc870\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n    # \uc5ec\uae30\uc5d0\uc11c\ub294 \ub2e8 \ud558\ub098\uc758 \uc785\ub825\ub9cc \ubc1b\uc9c0\ub9cc, \ud544\uc694\ud558\uba74 \ub354 \ubc1b\ub3c4\ub85d \ubcc0\uacbd\ud558\uba74 \ub429\ub2c8\ub2e4.\n    def forward(self, input):\n        x = self.pool1(F.relu(self.conv1(input)))\n        x = self.pool2(F.relu(self.conv2(x)))\n\n        # \ubaa8\ub378 \uad6c\uc870\ub97c \uc815\uc758\ud560 \ub54c\ub294 \uc5b4\ub5a4 Python \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud574\ub3c4 \uad1c\ucc2e\uc2b5\ub2c8\ub2e4.\n        # \ubaa8\ub4e0 \ucf54\ub4dc\ub294 autograd\uc5d0 \uc758\ud574 \uc62c\ubc14\ub974\uace0 \uc644\ubcbd\ud558\uac8c \ucc98\ub9ac\ub420 \uac83\uc785\ub2c8\ub2e4.\n        # if x.gt(0) > x.numel() / 2:\n        #      ...\n        #\n        # \uc2ec\uc9c0\uc5b4 \ubc18\ubcf5\ubb38\uc744 \ub9cc\ub4e4\uace0 \uadf8 \uc548\uc5d0\uc11c \ub3d9\uc77c\ud55c \ubaa8\ub4c8\uc744 \uc7ac\uc0ac\uc6a9\ud574\ub3c4 \ub429\ub2c8\ub2e4.\n        # \ubaa8\ub4c8\uc740 \ub354 \uc774\uc0c1 \uc77c\uc2dc\uc801\uc778 \uc0c1\ud0dc\ub97c \uac16\uace0 \uc788\uc9c0 \uc54a\uc73c\ubbc0\ub85c,\n        # \uc21c\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c \uc5ec\ub7ec \ubc88 \uc0ac\uc6a9\ud574\ub3c4 \uad1c\ucc2e\uc2b5\ub2c8\ub2e4.\n        # while x.norm(2) < 10:\n        #    x = self.conv1(x)\n\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \uc815\uc758\ud55c \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc744 \uc0ac\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\uba3c\uc800 \ud074\ub798\uc2a4\uc758 \uc778\uc2a4\ud134\uc2a4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = MNISTConvNet()\nprint(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` \uc740 \ubbf8\ub2c8-\ubc30\uce58(mini-batch)\ub9cc \uc9c0\uc6d0\ud569\ub2c8\ub2e4. ``torch.nn`` \ud328\ud0a4\uc9c0\n    \uc804\uccb4\ub294 \ud558\ub098\uc758 \uc0d8\ud50c\uc774 \uc544\ub2cc, \uc0d8\ud50c\ub4e4\uc758 \ubbf8\ub2c8-\ubc30\uce58\ub9cc\uc744 \uc785\ub825\uc73c\ub85c \ubc1b\uc2b5\ub2c8\ub2e4.\n\n    \uc608\ub97c \ub4e4\uc5b4, ``nnConv2D`` \ub294 ``nSamples x nChannels x Height x Width`` \uc758\n    4\ucc28\uc6d0 Tensor\ub97c \uc785\ub825\uc73c\ub85c \ud569\ub2c8\ub2e4.\n\n    \ub9cc\uc57d \ud558\ub098\uc758 \uc0d8\ud50c\ub9cc \uc788\ub2e4\uba74, ``input.unsqueeze(0)`` \uc744 \uc0ac\uc6a9\ud574\uc11c \uac00\uc9dc \ucc28\uc6d0\uc744\n    \ucd94\uac00\ud569\ub2c8\ub2e4.</p></div>\n\n\ubb34\uc791\uc704 \uac12\uc744 \uac16\ub294 \ud558\ub098\uc758 \ubbf8\ub2c8-\ubc30\uce58\ub97c \ub9cc\ub4e4\uc5b4\uc11c \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc5d0 \ubcf4\ub0b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input = torch.randn(1, 1, 28, 28)\nout = net(input)\nprint(out.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac00\uc9dc(dummy)\ub85c \uc815\ub2f5(target)\uc744 \ud558\ub098 \ub9cc\ub4e4\uace0,\n\uc190\uc2e4 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc624\ucc28(error)\ub97c \uacc4\uc0b0\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target = torch.tensor([3], dtype=torch.long)\nloss_fn = nn.CrossEntropyLoss()  # LogSoftmax + ClassNLL Loss\nerr = loss_fn(out, target)\nerr.backward()\n\nprint(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc758 \ucd9c\ub825 ``out`` \uc740 ``Tensor`` \uc774\uba70, \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc624\ucc28\ub97c\n\uacc4\uc0b0\ud558\uace0 \uacb0\uacfc\ub97c ``Tensor`` \uc778 ``err`` \uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4.\n``err`` \uc758 ``.backward`` \ub97c \ud638\ucd9c\ud558\uba74 \ubcc0\ud654\ub3c4\uac00 \uc804\uccb4 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc758\n\uac00\uc911\uce58\uc5d0 \uc804\ud30c\ub429\ub2c8\ub2e4.\n\n\uc774\uc81c \uac1c\ubcc4 \uacc4\uce35\uc758 \uac00\uc911\uce58\uc640 \ubcc0\ud654\ub3c4\uc5d0 \uc811\uadfc\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(net.conv1.weight.grad.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(net.conv1.weight.data.norm())  # norm of the weight\nprint(net.conv1.weight.grad.data.norm())  # norm of the gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc21c\uc804\ud30c/\uc5ed\uc804\ud30c \ud568\uc218 \ud6c5(Hook)\n-----------------------------------\n\n\uc9c0\uae08\uae4c\uc9c0 \uac00\uc911\uce58\uc640 \ubcc0\ud654\ub3c4\uc5d0 \ub300\ud574\uc11c \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4. \uadf8\ub807\ub2e4\uba74 \uacc4\uce35\uc758 \ucd9c\ub825\uc774\ub098\ngrad_output \uc744 \uc0b4\ud3b4\ubcf4\uac70\ub098 \uc218\uc815\ud558\ub824\uba74 \uc5b4\ub5bb\uac8c \ud574\uc57c \ud560\uae4c\uc694?\n\n\uc774\ub7f0 \ubaa9\uc801\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 **\ud6c5(Hook)** \uc744 \uc18c\uac1c\ud569\ub2c8\ub2e4.\n\n``Module`` \uc774\ub098 ``Tensor`` \uc5d0 \ud568\uc218\ub97c \ub4f1\ub85d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ud6c5\uc740 \uc21c\uc804\ud30c \ud6c5\uacfc \uc5ed\uc804\ud30c \ud6c5\uc774 \uc788\ub294\ub370, \uc21c\uc804\ud30c \ud6c5\uc740 \uc21c\uc804\ud30c\uac00 \uc77c\uc5b4\ub0a0 \ub54c,\n\uc5ed\uc804\ud30c \ud6c5\uc740 \uc5ed\uc804\ud30c\uac00 \uc77c\uc5b4\ub0a0 \ub54c \uc2e4\ud589\ub429\ub2c8\ub2e4.\n\uc608\uc81c\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\nconv2\uc5d0 \uc21c\uc804\ud30c \ud6c5\uc744 \ub4f1\ub85d\ud558\uace0 \uba87 \uac00\uc9c0 \uc815\ubcf4\ub97c \ucd9c\ub825\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def printnorm(self, input, output):\n    # input is a tuple of packed inputs\n    # output is a Tensor. output.data is the Tensor we are interested\n    print('Inside ' + self.__class__.__name__ + ' forward')\n    print('')\n    print('input: ', type(input))\n    print('input[0]: ', type(input[0]))\n    print('output: ', type(output))\n    print('')\n    print('input size:', input[0].size())\n    print('output size:', output.data.size())\n    print('output norm:', output.data.norm())\n\n\nnet.conv2.register_forward_hook(printnorm)\n\nout = net(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "conv2\uc5d0 \uc5ed\uc804\ud30c \ud6c5\uc744 \ub4f1\ub85d\ud558\uace0 \uba87 \uac00\uc9c0 \uc815\ubcf4\ub97c \ucd9c\ub825\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def printgradnorm(self, grad_input, grad_output):\n    print('Inside ' + self.__class__.__name__ + ' backward')\n    print('Inside class:' + self.__class__.__name__)\n    print('')\n    print('grad_input: ', type(grad_input))\n    print('grad_input[0]: ', type(grad_input[0]))\n    print('grad_output: ', type(grad_output))\n    print('grad_output[0]: ', type(grad_output[0]))\n    print('')\n    print('grad_input size:', grad_input[0].size())\n    print('grad_output size:', grad_output[0].size())\n    print('grad_input norm:', grad_input[0].norm())\n\n\nnet.conv2.register_backward_hook(printgradnorm)\n\nout = net(input)\nerr = loss_fn(out, target)\nerr.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc2e4\uc81c\ub85c \ub3d9\uc791\ud558\ub294 MNIST \uc804\uccb4 \uc608\uc81c\ub294 \uc5ec\uae30\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nhttps://github.com/pytorch/examples/tree/master/mnist\n\n\uc608\uc81c2: \uc21c\ud658 \uc2e0\uacbd\ub9dd(Recurrent Nets)\n------------------------------------\n\n\ub2e4\uc74c\uc73c\ub85c PyTorch\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc21c\ud658 \uc2e0\uacbd\ub9dd\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc2e0\uacbd\ub9dd\uc758 \uc0c1\ud0dc\ub294 \uac01 \uacc4\uce35\uc774 \uc544\ub2cc \uadf8\ub798\ud504\uc5d0 \uc800\uc7a5\ub418\ubbc0\ub85c, \uac04\ub2e8\ud788 nn.Linear\uc744\n\uc0dd\uc131\ud55c \ud6c4 \uc21c\ud658\ud560 \ub54c\ub9c8\ub2e4 \uacc4\uc18d\ud574\uc11c \uc7ac\uc0ac\uc6a9\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n\n    # you can also accept arguments in your model constructor\n    def __init__(self, data_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n\n        self.hidden_size = hidden_size\n        input_size = data_size + hidden_size\n\n        self.i2h = nn.Linear(input_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n\n    def forward(self, data, last_hidden):\n        input = torch.cat((data, last_hidden), 1)\n        hidden = self.i2h(input)\n        output = self.h2o(hidden)\n        return hidden, output\n\n\nrnn = RNN(50, 20, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LSTM\uacfc Penn Tree-bank\ub97c \uc0ac\uc6a9\ud55c \uc880 \ub354 \uc644\ubcbd\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uc608\uc81c\ub294\n`\uc5ec\uae30 <https://github.com/pytorch/examples/tree/master/word\\_language\\_model>`_\n\uc5d0 \uc788\uc2b5\ub2c8\ub2e4.\n\nPyTorch\ub294 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uacfc \uc21c\ud658 \uc2e0\uacbd\ub9dd\uc5d0 CuDNN \uc5f0\ub3d9\uc744 \uae30\ubcf8\uc801\uc73c\ub85c \uc9c0\uc6d0\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n\nbatch_size = 10\nTIMESTEPS = 5\n\n# Create some fake data\nbatch = torch.randn(batch_size, 50)\nhidden = torch.zeros(batch_size, 20)\ntarget = torch.zeros(batch_size, 10)\n\nloss = 0\nfor t in range(TIMESTEPS):\n    # yes! you can reuse the same network several times,\n    # sum up the losses, and call backward!\n    hidden, output = rnn(batch, hidden)\n    loss += loss_fn(output, target)\nloss.backward()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}