{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTorchText\ub85c \uc5b8\uc5b4 \ubc88\uc5ed\ud558\uae30\n===================================\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 ``torchtext`` \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc601\uc5b4\uc640 \ub3c5\uc77c\uc5b4 \ubb38\uc7a5\ub4e4\uc774 \ud3ec\ud568\ub41c \uc798 \uc54c\ub824\uc9c4 \ub370\uc774\ud130\uc14b\uc744 \uc804\ucc98\ub9ac(preprocess)\ud558\uace0\n\uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub3c5\uc77c\uc5b4 \ubb38\uc7a5\uc744 \uc601\uc5b4\ub85c \ubc88\uc5ed\ud558\ub294 \uc2dc\ud000\uc2a4-\ud22c-\uc2dc\ud000\uc2a4(sequence-to-sequence, seq2seq) \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \ubc29\ubc95\uc744\n\uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740\nPyTorch \ucee4\ubba4\ub2c8\ud2f0 \uba64\ubc84\uc778 `Ben Trevett <https://github.com/bentrevett>`__ \uc774 \uc791\uc131\ud55c\n`\ud29c\ud1a0\ub9ac\uc5bc <https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__\n\uc5d0 \uae30\ucd08\ud558\uace0 \uc788\uc73c\uba70 Ben\uc758 \ud5c8\ub77d\uc744 \ubc1b\uace0 \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4. \uba87\uba87 \uae30\uc874 \ucf54\ub4dc\ub4e4\uc744 \uc81c\uac70\ud558\uace0 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ud1b5\ud574 NLP \ubaa8\ub378\ub9c1\uc744 \uc704\ud574 \ubb38\uc7a5\ub4e4\uc744 \ud150\uc11c(tensor)\ub85c \uc804\ucc98\ub9ac\ud558\uace0, \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uace0 \uac80\uc99d\ud558\uae30 \uc704\ud574\n`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub370\uc774\ud130 \ucc98\ub9ac\ud558\uae30\n--------------------------------\n\n``torchtext`` \uc5d0\ub294 \uc5b8\uc5b4 \ubcc0\ud658 \ubaa8\ub378\uc744 \ub9cc\ub4e4 \ub54c \uc27d\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub370\uc774\ud130\uc14b\uc744 \ub9cc\ub4e4\uae30 \uc801\ud569\ud55c \ub2e4\uc591\ud55c \ub3c4\uad6c\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774 \uc608\uc81c\uc5d0\uc11c\ub294 \uac00\uacf5\ub418\uc9c0 \uc54a\uc740 \ud14d\uc2a4\ud2b8 \ubb38\uc7a5(raw text sentence)\uc744 \ud1a0\ud070\ud654(tokenize)\ud558\uace0, \uc5b4\ud718\uc9d1(vocabulary)\uc744 \ub9cc\ub4e4\uace0,\n\ud1a0\ud070\uc744 \ud150\uc11c\ub85c \uc22b\uc790\ud654(numericalize)\ud558\ub294 \ubc29\ubc95\uc744 \uc54c\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ucc38\uace0 : \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\uc758 \ud1a0\ud070\ud654(tokenization)\uc5d0\ub294 `Spacy <https://spacy.io>`__ \uac00 \ud544\uc694\ud569\ub2c8\ub2e4.\nSpacy\ub294 \uc601\uc5b4 \uc774 \uc678\uc758 \ub2e4\ub978 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uac15\ub825\ud55c \ud1a0\ud070\ud654 \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uae30 \ub54c\ubb38\uc5d0 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. ``torchtext`` \ub294\n`basic_english`` \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc81c\uacf5\ud560 \ubfd0 \uc544\ub2c8\ub77c \uc601\uc5b4\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub2e4\ub978 \ud1a0\ud06c\ub098\uc774\uc800\ub4e4(\uc608\ucee8\ub370\n`Moses <https://bitbucket.org/luismsgomes/mosestokenizer/src/default/>`__ )\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4\ub9cc, \uc5b8\uc5b4 \ubc88\uc5ed\uc744 \uc704\ud574\uc11c\ub294 \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub97c\n\ub2e4\ub8e8\uc5b4\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 Spacy\uac00 \uac00\uc7a5 \uc801\ud569\ud569\ub2c8\ub2e4.\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2e4\ud589\ud558\ub824\uba74, \uc6b0\uc120 ``pip`` \ub098 ``conda`` \ub85c ``spacy`` \ub97c \uc124\uce58\ud558\uc138\uc694. \uadf8 \ub2e4\uc74c,\nSpacy \ud1a0\ud06c\ub098\uc774\uc800\uac00 \uc4f8 \uc601\uc5b4\uc640 \ub3c5\uc77c\uc5b4\uc5d0 \ub300\ud55c \ub370\uc774\ud130\ub97c \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uc2b5\ub2c8\ub2e4.\n\n::\n\n   python -m spacy download en\n   python -m spacy download de\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchtext\nimport torch\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\nfrom torchtext.utils import download_from_url, extract_archive\nimport io\n\nurl_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\ntrain_urls = ('train.de.gz', 'train.en.gz')\nval_urls = ('val.de.gz', 'val.en.gz')\ntest_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n\ntrain_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\nval_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\ntest_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n\nde_tokenizer = get_tokenizer('spacy', language='de')\nen_tokenizer = get_tokenizer('spacy', language='en')\n\ndef build_vocab(filepath, tokenizer):\n  counter = Counter()\n  with io.open(filepath, encoding=\"utf8\") as f:\n    for string_ in f:\n      counter.update(tokenizer(string_))\n  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n\nde_vocab = build_vocab(train_filepaths[0], de_tokenizer)\nen_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n\ndef data_process(filepaths):\n  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n  data = []\n  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n                            dtype=torch.long)\n    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n                            dtype=torch.long)\n    data.append((de_tensor_, en_tensor_))\n  return data\n\ntrain_data = data_process(train_filepaths)\nval_data = data_process(val_filepaths)\ntest_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``DataLoader``\n--------------------\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc0ac\uc6a9\ud574 \ubcfc ``torch`` \uc5d0 \ud2b9\ud654\ub41c \uae30\ub2a5\uc740 \ubc14\ub85c ``DataLoader`` \ub85c,\n\uccab \ubc88\uc9f8 \uc778\uc790\ub85c \ub370\uc774\ud130\ub97c \uc804\ub2ec\ubc1b\uae30 \ub54c\ubb38\uc5d0 \uc0ac\uc6a9\ud558\uae30\uac00 \uc27d\uc2b5\ub2c8\ub2e4. \ubb38\uc11c\uc5d0\uc11c\ub3c4 \ubcfc \uc218 \uc788\ub4ef\uc774,\n``DataLoader \ub294 \ub370\uc774\ud130\uc14b\uacfc \uc0d8\ud50c\ub7ec\ub97c \uacb0\ud569\ud558\uace0, \uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130\uc14b\uc5d0 \ubc18\ubcf5 \uae30\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n``DataLoader`` \ub294 \ub9f5 \ud615\ud0dc(map-style)\uacfc \uc21c\ud68c \uac00\ub2a5\ud55c \ud615\ud0dc(iterable-style) \ub370\uc774\ud130\uc14b\uc744 \ubaa8\ub450 \uc9c0\uc6d0\ud558\uba70,\n\ub2e8\uc77c \ub610\ub294 \ub2e4\uc911 \ud504\ub85c\uc138\uc2a4\ub85c \ubd88\ub7ec\uc624\uac70\ub098, \ubd88\ub7ec\uc624\ub294 \uc21c\uc11c\ub97c \uc870\uc815(customize)\ud558\uac70\ub098\n\uc120\ud0dd\uc801 \uc790\ub3d9 \uc77c\uad04 \ucc98\ub9ac(optional automatic batching), \uba54\ubaa8\ub9ac \ud53c\ub2dd(memory pinning)\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.\n\n\uc0d8\ud50c \ubaa9\ub85d\uc744 \ubcd1\ud569(merge)\ud558\uc5ec Tensor\uc758 \ubbf8\ub2c8\ubc30\uce58\ub97c \uad6c\uc131\ud558\ub294 ``collate_fn`` (\uc120\ud0dd \uc0ac\ud56d)\uc744 \uc0b4\ud3b4\ubcf4\uc2ed\uc2dc\uc624.\n\ub9f5 \ud615\ud0dc(map-style) \ub370\uc774\ud130\uc14b\uc744 \uc77c\uad04\ub85c \ubd88\ub7ec\uc62c \ub54c \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nBATCH_SIZE = 128\nPAD_IDX = de_vocab['<pad>']\nBOS_IDX = de_vocab['<bos>']\nEOS_IDX = de_vocab['<eos>']\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\ndef generate_batch(data_batch):\n  de_batch, en_batch = [], []\n  for (de_item, en_item) in data_batch:\n    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n  return de_batch, en_batch\n\ntrain_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\nvalid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\ntest_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n                       shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``nn.Module`` \uacfc ``Optimizer`` \uc815\uc758\ud558\uae30\n------------------------------------------\n\ub300\ubd80\ubd84\uc740 ``torchtext`` \uac00 \uc54c\uc544\uc11c \ud574\uc90d\ub2c8\ub2e4 : \ub370\uc774\ud130\uc14b\uc774 \ub9cc\ub4e4\uc5b4\uc9c0\uace0 \ubc18\ubcf5\uc790\uac00 \uc815\uc758\ub418\uba74, \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\n\uc6b0\ub9ac\uac00 \ud574\uc57c \ud560 \uc77c\uc774\ub77c\uace0\ub294 \uadf8\uc800 ``nn.Module`` \uc640 ``Optimizer`` \ub97c \ubaa8\ub378\ub85c\uc11c \uc815\uc758\ud558\uace0 \ud6c8\ub828\uc2dc\ud0a4\ub294 \uac83\uc774 \uc804\ubd80\uc785\ub2c8\ub2e4.\n\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc0ac\uc6a9\ud560 \ubaa8\ub378\uc740 `\uc774\uacf3 <https://arxiv.org/abs/1409.0473>`__ \uc5d0\uc11c \uc124\uba85\ud558\uace0 \uc788\ub294 \uad6c\uc870\ub97c \ub530\ub974\uace0 \uc788\uc73c\uba70,\n\ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 `\uc5ec\uae30 <https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb>`__\n\ub97c \ucc38\uace0\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.\n\n\ucc38\uace0 : \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc740 \uc5b8\uc5b4 \ubc88\uc5ed\uc744 \uc704\ud574 \uc0ac\uc6a9\ud560 \uc608\uc2dc \ubaa8\ub378\uc785\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc740\n\uc774 \uc791\uc5c5\uc5d0 \uc801\ub2f9\ud55c \ud45c\uc900 \ubaa8\ub378\uc774\uae30 \ub54c\ubb38\uc774\uc9c0, \ubc88\uc5ed\uc5d0 \uc801\ud569\ud55c \ubaa8\ub378\uc774\uae30 \ub54c\ubb38\uc740 \uc544\ub2d9\ub2c8\ub2e4. \uc5ec\ub7ec\ubd84\uc774 \ucd5c\uc2e0 \uae30\uc220 \ud2b8\ub80c\ub4dc\ub97c\n\uc798 \ub530\ub77c\uac00\uace0 \uc788\ub2e4\uba74 \uc798 \uc544\uc2dc\uaca0\uc9c0\ub9cc, \ud604\uc7ac \ubc88\uc5ed\uc5d0\uc11c \uac00\uc7a5 \ub6f0\uc5b4\ub09c \ubaa8\ub378\uc740 Transformers\uc785\ub2c8\ub2e4. PyTorch\uac00\nTransformer \ub808\uc774\uc5b4\ub97c \uad6c\ud604\ud55c \ub0b4\uc6a9\uc740 `\uc5ec\uae30 <https://pytorch.org/docs/stable/nn.html#transformer-layers>`__\n\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ubaa8\ub378\uc774 \uc0ac\uc6a9\ud558\ub294 \"attention\" \uc740 Transformer \ubaa8\ub378\uc5d0\uc11c \uc81c\uc548\ud558\ub294\n\uba40\ud2f0 \ud5e4\ub4dc \uc140\ud504 \uc5b4\ud150\uc158(multi-headed self-attention) \uacfc\ub294 \ub2e4\ub974\ub2e4\ub294 \uc810\uc744 \uc54c\ub824\ub4dc\ub9bd\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nfrom typing import Tuple\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass Encoder(nn.Module):\n    def __init__(self,\n                 input_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: float):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.dropout = dropout\n\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n\n        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n\n        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self,\n                src: Tensor) -> Tuple[Tensor]:\n\n        embedded = self.dropout(self.embedding(src))\n\n        outputs, hidden = self.rnn(embedded)\n\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n\n        return outputs, hidden\n\n\nclass Attention(nn.Module):\n    def __init__(self,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 attn_dim: int):\n        super().__init__()\n\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n\n        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n\n        self.attn = nn.Linear(self.attn_in, attn_dim)\n\n    def forward(self,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -> Tensor:\n\n        src_len = encoder_outputs.shape[0]\n\n        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        energy = torch.tanh(self.attn(torch.cat((\n            repeated_decoder_hidden,\n            encoder_outputs),\n            dim = 2)))\n\n        attention = torch.sum(energy, dim=2)\n\n        return F.softmax(attention, dim=1)\n\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 output_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: int,\n                 attention: nn.Module):\n        super().__init__()\n\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.output_dim = output_dim\n        self.dropout = dropout\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n\n        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n\n        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n\n    def _weighted_encoder_rep(self,\n                              decoder_hidden: Tensor,\n                              encoder_outputs: Tensor) -> Tensor:\n\n        a = self.attention(decoder_hidden, encoder_outputs)\n\n        a = a.unsqueeze(1)\n\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n\n        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n\n        return weighted_encoder_rep\n\n\n    def forward(self,\n                input: Tensor,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -> Tuple[Tensor]:\n\n        input = input.unsqueeze(0)\n\n        embedded = self.dropout(self.embedding(input))\n\n        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n                                                          encoder_outputs)\n\n        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n\n        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n\n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n\n        output = self.out(torch.cat((output,\n                                     weighted_encoder_rep,\n                                     embedded), dim = 1))\n\n        return output, decoder_hidden.squeeze(0)\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self,\n                 encoder: nn.Module,\n                 decoder: nn.Module,\n                 device: torch.device):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                teacher_forcing_ratio: float = 0.5) -> Tensor:\n\n        batch_size = src.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n\n        encoder_outputs, hidden = self.encoder(src)\n\n        # \ub514\ucf54\ub354\ub85c\uc758 \uccab \ubc88\uc9f8 \uc785\ub825\uc740 <sos> \ud1a0\ud070\uc785\ub2c8\ub2e4.\n        output = trg[0,:]\n\n        for t in range(1, max_len):\n            output, hidden = self.decoder(output, hidden, encoder_outputs)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.max(1)[1]\n            output = (trg[t] if teacher_force else top1)\n\n        return outputs\n\n\nINPUT_DIM = len(de_vocab)\nOUTPUT_DIM = len(en_vocab)\n# ENC_EMB_DIM = 256\n# DEC_EMB_DIM = 256\n# ENC_HID_DIM = 512\n# DEC_HID_DIM = 512\n# ATTN_DIM = 64\n# ENC_DROPOUT = 0.5\n# DEC_DROPOUT = 0.5\n\nENC_EMB_DIM = 32\nDEC_EMB_DIM = 32\nENC_HID_DIM = 64\nDEC_HID_DIM = 64\nATTN_DIM = 8\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n\nattn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n\nmodel = Seq2Seq(enc, dec, device).to(device)\n\n\ndef init_weights(m: nn.Module):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n\n\nmodel.apply(init_weights)\n\noptimizer = optim.Adam(model.parameters())\n\n\ndef count_parameters(model: nn.Module):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ucc38\uace0 : \uc5b8\uc5b4 \ubc88\uc5ed\uc758 \uc131\ub2a5 \uc810\uc218\ub97c \uae30\ub85d\ud558\ub824\uba74, ``nn.CrossEntropyLoss`` \ud568\uc218\uac00 \ub2e8\uc21c\ud55c\n\ud328\ub529\uc744 \ucd94\uac00\ud558\ub294 \ubd80\ubd84\uc744 \ubb34\uc2dc\ud560 \uc218 \uc788\ub3c4\ub85d \ud574\ub2f9 \uc0c9\uc778\ub4e4\uc744 \uc54c\ub824\uc918\uc57c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PAD_IDX = en_vocab.stoi['<pad>']\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc774 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uace0 \ud3c9\uac00\ud569\ub2c8\ub2e4 :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\nimport time\n\n\ndef train(model: nn.Module,\n          iterator: torch.utils.data.DataLoader,\n          optimizer: optim.Optimizer,\n          criterion: nn.Module,\n          clip: float):\n\n    model.train()\n\n    epoch_loss = 0\n\n    for _, (src, trg) in enumerate(iterator):\n        src, trg = src.to(device), trg.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(src, trg)\n\n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg[1:].view(-1)\n\n        loss = criterion(output, trg)\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef evaluate(model: nn.Module,\n             iterator: torch.utils.data.DataLoader,\n             criterion: nn.Module):\n\n    model.eval()\n\n    epoch_loss = 0\n\n    with torch.no_grad():\n\n        for _, (src, trg) in enumerate(iterator):\n            src, trg = src.to(device), trg.to(device)\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg[1:].view(-1)\n\n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef epoch_time(start_time: int,\n               end_time: int):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\nN_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n\n    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iter, criterion)\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\ntest_loss = evaluate(model, test_iter, criterion)\n\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub2e4\uc74c \ub2e8\uacc4\n--------------\n\n- ``torchtext`` \ub97c \uc0ac\uc6a9\ud55c Ben Trevett\uc758 \ud29c\ud1a0\ub9ac\uc5bc\uc744 `\uc774\uacf3 <https://github.com/bentrevett/>`__ \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- ``nn.Transformer`` \uc640 ``torchtext`` \uc758 \ub2e4\ub978 \uae30\ub2a5\ub4e4\uc744 \uc774\uc6a9\ud55c \ub2e4\uc74c \ub2e8\uc5b4 \uc608\uce21\uc744 \ud1b5\ud55c \uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc0b4\ud3b4\ubcf4\uc138\uc694.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}