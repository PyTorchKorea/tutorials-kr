{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\uacf5\uac04 \ubcc0\ud615\uae30 \ub124\ud2b8\uc6cc\ud06c \ud29c\ud1a0\ub9ac\uc5bc(Spatial Transformer Networks Tutorial)\n==================================================================\n**Author**: `Ghassen HAMROUNI <https://github.com/GHamrouni>`_\n\n.. figure:: /_static/img/stn/FSeq.png\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uacf5\uac04 \ubcc0\ud615\uae30 \ub124\ud2b8\uc6cc\ud06c\ub85c \ubd88\ub9ac\ub294 \uc2dc\uac01 \uc5b4\ud150\uc158 \uba54\uce74\ub2c8\uc998\uc744 \n\uc774\uc6a9\ud55c \ub124\ud2b8\uc6cc\ud06c \uc0ac\uc6a9 \ubc29\ubc95\uc744 \ubc30\uc6c1\ub2c8\ub2e4. `DeepMind paper <https://arxiv.org/abs/1506.02025>`__\n\uc5d0\uc11c \uacf5\uac04 \ubcc0\ud615 \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uad00\ud574 \ub354 \ub9ce\uc740 \uac83\uc744 \uc77d\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uacf5\uac04 \ubcc0\ud615\uae30 \ub124\ud2b8\uc6cc\ud06c\ub294 \uc5b4\ub5a4 \uacf5\uac04 \ubcc0\ud615\uc5d0\ub3c4 \ubbf8\ubd84 \uac00\ub2a5\ud55c \uc5b4\ud150\uc158\uc758 \uc77c\ubc18\ud654\uc785\ub2c8\ub2e4.\n\uacf5\uac04 \ubcc0\ud615\uae30 \ub124\ud2b8\uc6cc\ud06c(STN)\ub294 \uc2e0\uacbd\ub9dd\uc774 \ubaa8\ub378\uc758 \uae30\ud558\ud558\uc801 \ubd88\ubcc0\uc131\uc744 \uac15\ud654\ud558\uae30 \uc704\ud574\uc11c \n\uc5b4\ub5bb\uac8c \uc785\ub825 \uc774\ubbf8\uc9c0 \uacf5\uac04 \ubcc0\ud615\uc744 \uc218\ud589\ud574\uc57c \ud558\ub294\uc9c0 \ubc30\uc6b0\uac8c \ud569\ub2c8\ub2e4. \n\uc608\ub97c \ub4e4\uc5b4\uc11c \uc774\ubbf8\uc9c0\uc758 \uad00\uc2ec \uc601\uc5ed\uc744 \uc798\ub974\uace0 \ud06c\uae30\ub97c \uc870\uc815\ud558\uace0 \ubc29\ud5a5\uc744 \uc218\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nCNN\uc774 \ud68c\uc804\uacfc \ud06c\uae30 \uadf8\ub9ac\uace0 \ub354 \uc77c\ubc18\uc801\uc778 Affine \ubcc0\ud615\uc5d0 \ubd88\ubcc0\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0\n(\ubbfc\uac10\ud558\uae30 \ub54c\ubb38\uc5d0) \uc774\uac83\uc740 \ub9e4\uc6b0 \uc720\uc6a9\ud55c \uba54\uce74\ub2c8\uc998 \uc785\ub2c8\ub2e4.\n\nSTN\uc758 \uac00\uc7a5 \uc88b\uc740 \uc810 \uc911 \ud558\ub098\ub294 \uac70\uc758 \uc218\uc815\ud558\uc9c0 \uc54a\uace0 \uae30\uc874\uc758 CNN\uc5d0 \n\uac04\ub2e8\ud788 \uc5f0\uacb0\ud560 \uc218 \uc788\ub294 \uc810 \uc785\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ub77c\uc774\uc13c\uc2a4: BSD\n# Author: Ghassen Hamrouni\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub370\uc774\ud130 \ub85c\ub529\n----------------\n\n\n\uc774 \ud3ec\uc2a4\ud2b8\uc5d0\uc11c \uace0\uc804\uc801\uc778 MNIST \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc2e4\ud5d8\ud569\ub2c8\ub2e4.\n\uacf5\uac04 \ubcc0\ud615\uae30 \ub124\ud2b8\uc6cc\ud06c\ub85c \ubcf4\uac15\ub41c \ud45c\uc900 CN(convolutional network)\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# \ud559\uc2b5 \ub370\uc774\ud130\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uacf5\uac04 \ubcc0\ud615\uae30 \ub124\ud2b8\uc6cc\ud06c \uc124\uba85\n--------------------------------------\n\n \uacf5\uac04 \ubcc0\ud615\uae30 \ub124\ud2b8\uc6cc\ud06c\ub294 3\uac00\uc9c0 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\ub85c \uc694\uc57d\ub429\ub2c8\ub2e4:\n\n-  \uc704\uce58 \uacb0\uc815 \ub124\ud2b8\uc6cc\ud06c(localization network)\ub294 \ubcc0\ud615 \ud30c\ub77c\ubbf8\ud130\ub97c \ud68c\uadc0\uc2dc\ud0a4\ub294\n   \uc77c\ubc18\uc801\uc778 CNN \uc785\ub2c8\ub2e4. \ubcc0\ud615\uc740 \uc774 \ub370\uc774\ud130\uc14b\uc5d0 \uba85\uc2dc\uc801\uc73c\ub85c \ud559\uc2b5\ub418\uc9c0 \uc54a\uc73c\uba70\n   \ub124\ud2b8\uc6cc\ud06c\ub294 \uc804\uccb4 \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\ud558\ub294 \uacf5\uac04 \ubcc0\ud615\uc744 \uc790\ub3d9\uc73c\ub85c \ubc30\uc6c1\ub2c8\ub2e4.\n-  \uadf8\ub9ac\ub4dc \uc0dd\uc131\uae30(grid generator)\ub294 \ucd9c\ub825 \uc774\ubbf8\uc9c0\ub85c\uc758 \uac01 \ud53d\uc140\uc5d0 \ub300\uc751\ud558\ub294 \n   \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc88c\ud45c \uadf8\ub9ac\ub4dc\ub97c \uc0dd\uc131\ud55c\ub2e4.   \n-  \uc0d8\ud50c\ub7ec\ub294 \ubcc0\ud615\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0 \uc801\uc6a9\ud569\ub2c8\ub2e4.  \n\n.. figure:: /_static/img/stn/stn-arch.png\n\n.. Note::\n   affine_grid \ubc0f grid_sample \ubaa8\ub4c8\uc774 \ud3ec\ud568 \ub41c PyTorch\uc758 \ucd5c\uc2e0 \ubc84\uc804\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n        # Spatial transformer localization-network\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = nn.Sequential(\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n\n        # Initialize the weights/bias with identity transformation\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n    def forward(self, x):\n        # transform the input\n        x = self.stn(x)\n\n        # Perform the usual forward pass\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nmodel = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \ud559\uc2b5\n------------------\n\n\uc774\uc81c SGD \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ucf1c \ubd05\uc2dc\ub2e4.\n\ub124\ud2b8\uc6cc\ud06c\ub294 \uac10\ub3c5 \ubc29\uc2dd\uc73c\ub85c \ubd84\ub958 \uc791\uc5c5\uc744 \ud559\uc2b5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\ub3d9\uc2dc\uc5d0 \ubaa8\ub378\uc740 STN\uc744 \uc790\ub3d9\uc73c\ub85c end-to-end \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n#\n# MNIST\uc5d0\uc11c STN\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud558\ub294 \uac04\ub2e8\ud55c \ud14c\uc2a4\ud2b8 \uc808\ucc28.\n#\n\n\ndef test():\n    with torch.no_grad():\n        model.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(test_loader.dataset),\n                      100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "STN \uacb0\uacfc \uc2dc\uac01\ud654\n---------------------------\n\n\uc774\uc81c \ud559\uc2b5 \ub41c \ube44\uc8fc\uc5bc \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uacb0\uacfc\ub97c \uac80\uc0ac \ud560 \uac83\uc785\ub2c8\ub2e4.\n\n\ud6c8\ub828 \ub3c4\uc911 \ubcc0\ud615\uc744 \uc2dc\uac01\ud654\ud558\uae30 \uc704\ud574 \uc791\uc740 \ud5ec\ud37c \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# \ud559\uc2b5 \ud6c4\uc5d0 \uacf5\uac04 \ubcc0\ud615\uae30 \ub808\uc774\uc5b4\uc758 \ucd9c\ub825\uc744 \uc2dc\uac01\ud654\ud558\uae30 \uc704\ud574 STN\uc744 \n# \uc0ac\uc6a9\ud558\uc5ec \uc785\ub825 \uc774\ubbf8\uc9c0 \ubc30\uce58\uc640 \ud574\ub2f9 \ubcc0\ud615 \ubc30\uce58\ub97c \uc2dc\uac01\ud654\ud569\ub2c8\ub2e4.\n\n\ndef visualize_stn():\n    with torch.no_grad():\n        # Get a batch of training data\n        data = next(iter(test_loader))[0].to(device)\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            torchvision.utils.make_grid(input_tensor))\n\n        out_grid = convert_image_np(\n            torchvision.utils.make_grid(transformed_input_tensor))\n\n        # Plot the results side-by-side\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# \uc77c\ubd80 \uc785\ub825 \ubc30\uce58\uc5d0\uc11c STN \ubcc0\ud615 \uc2dc\uac01\ud654\nvisualize_stn()\n\nplt.ioff()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}