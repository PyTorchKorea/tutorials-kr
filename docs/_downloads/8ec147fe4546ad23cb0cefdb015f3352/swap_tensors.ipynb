{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uc2e4 \ub54c\uc5d0\ub294 \n# https://tutorials.pytorch.kr/beginner/colab \ub97c \ucc38\uace0\ud558\uc138\uc694.\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Extension points in ``nn.Module`` for ``load_state_dict`` and tensor subclasses\n**Author:** [Mikayla Gawarecki](https://github.com/mikaylagawarecki)\n\nThis recipe introduces a new utility function ``torch.utils.swap_tensors``\nas well as two new extension points where it has been integrated in\n``nn.Module``:\n\n* ``nn.Module.to()`` and related methods\n* ``nn.Module.load_state_dict()``\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This recipe requires PyTorch 2.3.0 or later.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``torch.utils.swap_tensors``\n``torch.utils.swap_tensors`` (hereafter referred to as ``swap_tensors``) is a\nutility function that takes in two Python tensors and swaps them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nt1 = torch.arange(2)\nt2 = torch.arange(3)\nprint(f\"Before swapping, t1: {t1}, t2: {t2}\")\ntorch.utils.swap_tensors(t1, t2)\nprint(f\"After swapping, t1: {t1}, t2: {t2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More specifically, ``swap_tensors`` swaps the Python ``__class__``, ``__dict__``\nand ``__slots__`` of the two tensors, as well as their associated ``at::Tensor``.\n\n\n## Application to ``nn.Module``\nThis utility is pertinent to ``nn.Module`` when a Python object outside\nof the module holds a reference to parameters of the module. If an ``nn.Module``\nmodifies any of its parameters out of place, the object holding references to\nthe parameters will not see the change. A classic example of this is the\noptimizer, which holds a reference to the parameters of the ``nn.Module``.\nThis leads to a silent correctness issue where the ``optimizer.step()`` will\nrun without error but the weights of the ``nn.Module`` will not be updated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = torch.nn.Linear(1, 2, bias=False)\noptimizer = torch.optim.SGD(mod.parameters())\nprint(f\"weight in mod: {mod.weight}\")\nprint(f\"weight in optimizer: {optimizer.param_groups[0]['params']}\")\nmod.weight = torch.nn.Parameter(2 * mod.weight)\nprint(f\"weight in mod: {mod.weight}\")\nprint(f\"weight in optimizer: {optimizer.param_groups[0]['params']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``nn.Module.to()`` and related methods\nThis includes methods that change the device of the module (such as ``nn.Module.cpu()``),\nmethods that change the ``dtype`` of the module (such as ``nn.Module.float()``)\nas well as methods that allow the module to be materialized\n(such as ``nn.Module.to_empty()``).\n\nAt first glance, it might be non-intuitive that these methods are able to\nmodify the parameters of the module in-place. The existing approach has been\nto use a nasty hack dating back from the first days of PyTorch.\n\nNotably, the existing approach does not work in these cases:\n\n* when using ``__torch_dispatch__`` subclasses\n* when ``param`` and ``new_param`` do not have the same Python ``type()``\n* For tensors with special C++ representations (such as sparse tensors and ``XLA`` tensors)\n\nIn the following part of this recipe, we will define a toy ``__torch_dispatch__``\nsubclass ``MyQuantizedLinearWeight`` that represents quantized linear weights.\nThis subclass will be used for illustration purposes throughout the rest of\nthe tutorial. For brevity, we omit most of the ``__torch_dispatch__``\nimplementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aten = torch.ops.aten\n\nclass MyQuantizedLinearWeight(torch.Tensor):\n    @staticmethod\n    def __new__(cls, elem, scale):\n        return torch.Tensor._make_wrapper_subclass(\n            cls,\n            elem.shape,\n            dtype=elem.dtype,\n            layout=elem.layout,\n            device=elem.device,\n            strides=elem.stride(),\n            storage_offset=elem.storage_offset())\n\n    def __init__(self, elem: torch.Tensor, scale: float):\n        self.elem = elem\n        self.scale = scale\n\n    def __repr__(self):\n        return f\"MyQuantizedLinearWeight({self.elem}, scale={self.scale})\"\n\n    @classmethod\n    def __torch_dispatch__(cls, func, types, args, kwargs):\n        if func in (aten.detach.default, aten._to_copy.default):\n            new_elem = func(args[0].elem, *args[1:], **kwargs)\n            return cls(new_elem, args[0].scale)\n        # Implementations for certain ops would be added to ``OP_TABLE``.\n        # We omit this for brevity.\n        OP_TABLE = dict()\n        if func in OP_TABLE:\n          return OP_TABLE[func](func, args, kwargs)\n        raise NotImplementedError(f\"Unsupported function {func}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us create an ``nn.Linear`` layer of ``dtype`` ``torch.float32`` where the weight is\na ``MyQuantizedLinearWeight`` and try to convert it to ``torch.bfloat16``.\nObserve that the weight's ``dtype`` changes as expected. However, the ``dtype``\nof the subclass' payload (``elem``) does not change.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m = nn.Linear(3, 5, dtype=torch.float32)\nm.weight = torch.nn.Parameter(MyQuantizedLinearWeight(m.weight, 0.5))\nprint(f\"Before: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nm.bfloat16()\nprint(f\"After: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nprint(f\"m.weight.dtype: {m.weight.dtype}\")\nprint(f\"m.weight.elem.dtype: {m.weight.elem.dtype}\")\nprint(f\"m.bias.dtype: {m.bias.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To this end, we introduce a global config\n``torch.__future__.set_swap_module_params_on_conversion`` that will use\n``swap_tensors`` to swap the parameters of the module while preserving\nreferences in place of ``.data`` setting. When this config is set,\n``swap_tensors`` will be used during the conversion, which ensures that\nthe ``dtype`` of the payload is properly converted.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.__future__.set_swap_module_params_on_conversion(True)\nm = nn.Linear(3, 5, dtype=torch.float32)\nm.weight = torch.nn.Parameter(MyQuantizedLinearWeight(m.weight, 0.5))\nprint(f\"Before: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nm.bfloat16()\nprint(f\"After: id(m.weight)={id(m.weight)}, id(m.bias)={id(m.bias)}\")\nprint(f\"m.weight.dtype: {m.weight.dtype}\")\nprint(f\"m.weight.elem.dtype: {m.weight.elem.dtype}\")\nprint(f\"m.bias.dtype: {m.bias.dtype}\")\ntorch.__future__.set_swap_module_params_on_conversion(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``nn.Module.load_state_dict()``\nDepending on the value of the ``assign`` keyword argument passed\nto ``load_state_dict()``, there are two ways to load the ``state_dict``:\n\n* ``assign=False``: preserves the properties of ``module.param`` and only takes the values\n  from ``state_dict['param_name']``\n* ``assign=True``: preserves the properties and values of ``state_dict['param_name']``.\n\n\nPreviously, these were implemented with in-place ``copy_`` and ``__setattr__`` respectively.\nWith the existing implementation, each approach had its own limitations -- ``assign=False``\nimposes the constraint that the type of the parameter in the ``state_dict`` must\nbe the same as the type of the parameter in the module while ``assign=True`` imposes\nthe constraint that anything that holds references to the module's parameters must\nbe initialized after ``nn.Module.load_state_dict()``.\n\nNow, we address both constraints by adding a ``swap_tensors`` path to ``load_state_dict()``\nand introducing a new extension point ``torch.Tensor.module_load(self, other, assign=False)``.\nWhen the ``swap_tensors`` path is enabled via the ``__future__`` mentioned above,\nwe can use a ``__torch_function__`` handler for ``module_load`` to apply a\ncustom transformation to the value in the ``state_dict``. The result of this\ntransformation will be swapped with the parameter in the module.\n\nIn the following example, we will use the ``MyQuantizedLinearWeight`` subclass\ndefined above to illustrate how we can use these features to apply a\ncustom quantization scheme to the weights of a linear layer when\nloading the ``state_dict``.\n\nRecall that the ``__torch_function__`` handler for ``module_load`` will be\ninvoked if either ``self`` or ``other`` (in this case ``param`` or\n``state_dict[param_key]``) are ``MyQuantizedLinearWeight`` subclasses.\n\nAssume that we expect the ``state_dict`` to contain plain tensors and the\nmodule to contain ``MyQuantizedLinearWeight`` parameters where we want the\ntensors in the ``state_dict`` to be transformed into the subclass. Then we\ncan define a ``__torch_function__`` handler for ``torch.Tensor.module_load``\nas such:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@classmethod\ndef custom_torch_function(cls, func, types, args=(), kwargs=None):\n    kwargs = {} if kwargs is None else kwargs\n\n    if func is torch.Tensor.module_load:\n        dest, src = args[0], args[1]\n        assert type(dest) == cls and type(src) == torch.Tensor\n        return MyQuantizedLinearWeight(src, dest.scale)\n    else:\n        with torch._C.DisableTorchFunctionSubclass():\n                return func(*args, **kwargs)\n\nMyQuantizedLinearWeight.__torch_function__ = custom_torch_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let us create a skeleton of a model on the meta device to avoid\nmaterializing storages. We convert all weights in the modules to\n``MyQuantizedLinearWeight`` subclasses while leaving biases intact.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fn(m):\n    if isinstance(m, nn.Linear):\n        requires_grad = m.weight.requires_grad\n        m.weight = torch.nn.Parameter(\n                    MyQuantizedLinearWeight(m.weight, 0.5), requires_grad=requires_grad\n                   )\n\nwith torch.device(\"meta\"):\n    m = nn.Linear(3, 5)\n    m.apply(fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then load the ``state_dict``. Observe that we use ``assign=True`` because\nfor biases, we want to preserve the properties of the tensor in the ``state_dict``\n(for example, we do not want the bias to be on the ``meta`` device after loading).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.__future__.set_swap_module_params_on_conversion(True)\nprint(f\"Before: id(weight)={id(m.weight)}, id(bias)={id(m.bias)}\")\nprint(f\"m.state_dict() before load_state_dict():\\n {m.state_dict()}\")\nstate_dict = nn.Linear(3, 5).state_dict()\nprint(f\"state_dict:\\n {state_dict}\")\nm.load_state_dict(state_dict, assign=True)\nprint(f\"After: id(weight)={id(m.weight)}, id(bias)={id(m.bias)}\")\nprint(f\"m.state_dict() after load_state_dict():\\n {m.state_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above is a toy example of how we can use the new extension point in\n``nn.Module.load_state_dict()``. One can also imagine alternate scenarios such\nas when we have tensor subclasses in the ``state_dict`` and plain ``nn.Parameters``/\ntensors in the module or when both are tensor subclasses. Based on the use\ncase, we can define the ``__torch_function__`` handler for ``module_load``\nto apply the transforms as needed.\n\n## Conclusion\nIn this recipe, we learned about ``swap_tensors``, the importance\nof preserving references for parameters in ``nn.Module`` as well as how to\nuse the two new extension points that are gated by\n``torch.__future__.set_swap_module_params_on_conversion``.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}