


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ko" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ko" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta property="og:title" content="Performance Tuning Guide" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/recipes/recipes/tuning_guide.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Szymon Migacz Performance Tuning Guide is a set of optimizations and best practices which can accelerate training and inference of deep learning models in PyTorch. Presented techniques often can be implemented by changing only a few lines of code and can be applied to a wide range of deep..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Szymon Migacz Performance Tuning Guide is a set of optimizations and best practices which can accelerate training and inference of deep learning models in PyTorch. Presented techniques often can be implemented by changing only a few lines of code and can be applied to a wide range of deep..." />
<meta property="og:ignore_canonical" content="true" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Performance Tuning Guide &mdash; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>




    <link rel="shortcut icon" href="../../_static/favicon.ico"/>












  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom2.css" type="text/css" />
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />
    <link rel="next" title="(beta) Compiling the optimizer with torch.compile" href="../compiling_optimizer.html" />
    <link rel="prev" title="Automatic Mixed Precision" href="amp_recipe.html" />


  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.kr/get-started">시작하기</a>
          </li>

          <li class="active">
            <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
          </li>

          <li>
            <a href="https://pytorch.kr/hub">허브</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">커뮤니티</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">





    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">





                <div class="version">
                  2.3.1+cu121
                </div>









<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


          </div>







              <p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 레시피</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../recipes_index.html">모든 레시피 보기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prototype/prototype_index.html">모든 프로토타입 레시피 보기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 시작하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/intro.html">파이토치(PyTorch) 기본 익히기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/quickstart_tutorial.html">빠른 시작(Quickstart)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/tensorqs_tutorial.html">텐서(Tensor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/data_tutorial.html">Dataset과 DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/transforms_tutorial.html">변형(Transform)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/buildmodel_tutorial.html">신경망 모델 구성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/autogradqs_tutorial.html"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code>를 사용한 자동 미분</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/optimization_tutorial.html">모델 매개변수 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/saveloadrun_tutorial.html">모델 저장하고 불러오기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch on YouTube</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt.html">PyTorch 소개 - YouTube 시리즈</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/introyt1_tutorial.html">PyTorch 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/tensors_deeper_tutorial.html">Pytorch Tensor 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 배우기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">이미지/비디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dcgan_faces_tutorial.html">DCGAN 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/vt_tutorial.html">배포를 위해 비전 트랜스포머(Vision Transformer) 모델 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tiatoolbox_tutorial.html">Whole Slide Image Classification Using PyTorch and TIAToolbox</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">오디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_datasets_tutorial.html">Audio Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/text_to_speech_with_torchaudio.html">Text-to-speech with Tacotron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forced_alignment_with_torchaudio_tutorial.html">wav2vec2을 이용한 강제 정렬</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">텍스트</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/bettertransformer_tutorial.html">Fast Transformer Inference with Better Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html">기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/text_sentiment_ngrams_tutorial.html">torchtext 라이브러리로 텍스트 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/translation_transformer.html"><code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> 와 torchtext로 언어 번역하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/torchtext_custom_dataset_tutorial.html">Preprocess custom text dataset using Torchtext</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">백엔드</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">강화학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/mario_rl_tutorial.html">마리오 게임 RL 에이전트로 학습하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 모델을 프로덕션 환경에 배포하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/flask_rest_api_tutorial.html">Flask를 사용하여 Python에서 PyTorch를 REST API로 배포하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/Intro_to_TorchScript_tutorial.html">TorchScript 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_export.html">C++에서 TorchScript 모델 로딩하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/super_resolution_with_onnxruntime.html">(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/realtime_rpi.html">Raspberry Pi 4 에서 실시간 추론(Inference) (30fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 프로파일링</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hta_intro_tutorial.html">Introduction to Holistic Trace Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hta_trace_diff_tutorial.html">Trace Diff using Holistic Trace Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_conv_bn_fuser.html">(베타) FX에서 합성곱/배치 정규화(Convolution/Batch Norm) 결합기(Fuser) 만들기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">프론트엔드 API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ensembling.html">모델 앙상블</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_frontend.html">PyTorch C++ 프론트엔드 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch-script-parallelism.html">TorchScript의 동적 병렬 처리(Dynamic Parallelism)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_autograd.html">C++ 프론트엔드의 자동 미분 (autograd)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 확장하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_classes.html">커스텀 C++ 클래스로 TorchScript 확장하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">모델 최적화</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_profiler_tutorial.html">텐서보드를 이용한 파이토치 프로파일러</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hyperparameter_tuning_tutorial.html">Ray Tune을 사용한 하이퍼파라미터 튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/vt_tutorial.html">배포를 위해 비전 트랜스포머(Vision Transformer) 모델 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dynamic_quantization_tutorial.html">(베타) LSTM 기반 단어 단위 언어 모델의 동적 양자화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dynamic_quantization_bert_tutorial.html">(베타) BERT 모델 동적 양자화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/quantized_transfer_learning_tutorial.html">(베타) 컴퓨터 비전 튜토리얼을 위한 양자화된 전이학습(Quantized Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/static_quantization_tutorial.html">(베타) PyTorch에서 Eager Mode를 이용한 정적 양자화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex.html">Grokking PyTorch Intel CPU performance from first principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex_2.html">Grokking PyTorch Intel CPU performance from first principles (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#torch-compile-sdpa"><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> 과 함께 SDPA 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#sdpa-atteition-bias">SDPA를 <code class="docutils literal notranslate"><span class="pre">atteition.bias</span></code> 하위 클래스와 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#id8">결론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">병렬 및 분산 학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../distributed/home.html">Distributed and Parallel Training Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/model_parallel_tutorial.html">단일 머신을 사용한 모델 병렬화 모범 사례</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ddp_tutorial.html">분산 데이터 병렬 처리 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_tuto.html">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel(FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_adavnced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/TP_tutorial.html">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/process_group_cpp_extension_tutorial.html">Cpp 확장을 사용한 프로세스 그룹 백엔드 사용자 정의</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/rpc_ddp_tutorial.html">분산 데이터 병렬(DDP)과 분산 RPC 프레임워크 결합</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/ddp_pipeline.html">분산 데이터 병렬 처리와 병렬 처리 파이프라인을 사용한 트랜스포머 모델 학습</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Edge with ExecuTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html">Exporting to ExecuTorch Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href=" https://pytorch.org/executorch/stable/running-a-model-cpp-tutorial.html">Running an ExecuTorch Model in C++ Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/sdk-integration-tutorial.html">Using the ExecuTorch SDK to Profile a Model</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/demo-apps-ios.html">Building an ExecuTorch iOS Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/demo-apps-android.html">Building an ExecuTorch Android Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/examples-end-to-end-to-lower-model-to-delegate.html">Lowering a Model as a Delegate</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">추천 시스템</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchrec_tutorial.html">TorchRec 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multimodality</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/flava_finetuning_tutorial.html">TorchMultimodal 튜토리얼: FLAVA 미세조정</a></li>
</ul>



        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">

      <li>
        <a href="../../index.html">

            Tutorials

        </a> &gt;
      </li>


          <li><a href="../recipes_index.html">PyTorch Recipes</a> &gt;</li>

      <li>Performance Tuning Guide</li>


      <li class="pytorch-breadcrumbs-aside">


            <a href="../../_sources/recipes/recipes/tuning_guide.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>


      </li>

  </ul>


</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">



          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">recipes/recipes/tuning_guide</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>




          <div class="rst-content">

            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">

  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-recipes-recipes-tuning-guide-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="performance-tuning-guide">
<span id="sphx-glr-recipes-recipes-tuning-guide-py"></span><h1>Performance Tuning Guide<a class="headerlink" href="#performance-tuning-guide" title="이 제목에 대한 퍼머링크">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/szmigacz">Szymon Migacz</a></p>
<p>Performance Tuning Guide is a set of optimizations and best practices which can
accelerate training and inference of deep learning models in PyTorch. Presented
techniques often can be implemented by changing only a few lines of code and can
be applied to a wide range of deep learning models across all domains.</p>
<div class="section" id="general-optimizations">
<h2>General optimizations<a class="headerlink" href="#general-optimizations" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="enable-asynchronous-data-loading-and-augmentation">
<h3>Enable asynchronous data loading and augmentation<a class="headerlink" href="#enable-asynchronous-data-loading-and-augmentation" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a>
supports asynchronous data loading and data augmentation in separate worker
subprocesses. The default setting for <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> is <code class="docutils literal notranslate"><span class="pre">num_workers=0</span></code>,
which means that the data loading is synchronous and done in the main process.
As a result the main training process has to wait for the data to be available
to continue the execution.</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> enables asynchronous data loading and overlap
between the training and data loading. <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> should be tuned
depending on the workload, CPU, GPU, and location of training data.</p>
<p><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> accepts <code class="docutils literal notranslate"><span class="pre">pin_memory</span></code> argument, which defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.
When using a GPU it’s better to set <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code>, this instructs
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> to use pinned memory and enables faster and asynchronous memory
copy from the host to the GPU.</p>
</div>
<div class="section" id="disable-gradient-calculation-for-validation-or-inference">
<h3>Disable gradient calculation for validation or inference<a class="headerlink" href="#disable-gradient-calculation-for-validation-or-inference" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>PyTorch saves intermediate buffers from all operations which involve tensors
that require gradients. Typically gradients aren’t needed for validation or
inference.
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad">torch.no_grad()</a>
context manager can be applied to disable gradient calculation within a
specified block of code, this accelerates execution and reduces the amount of
required memory.
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad">torch.no_grad()</a>
can also be used as a function decorator.</p>
</div>
<div class="section" id="disable-bias-for-convolutions-directly-followed-by-a-batch-norm">
<h3>Disable bias for convolutions directly followed by a batch norm<a class="headerlink" href="#disable-bias-for-convolutions-directly-followed-by-a-batch-norm" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">torch.nn.Conv2d()</a>
has <code class="docutils literal notranslate"><span class="pre">bias</span></code> parameter which defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> (the same is true for
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d">Conv1d</a>
and
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d">Conv3d</a>
).</p>
<p>If a <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> layer is directly followed by a <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d</span></code> layer,
then the bias in the convolution is not needed, instead use
<code class="docutils literal notranslate"><span class="pre">nn.Conv2d(...,</span> <span class="pre">bias=False,</span> <span class="pre">....)</span></code>. Bias is not needed because in the first
step <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> subtracts the mean, which effectively cancels out the
effect of bias.</p>
<p>This is also applicable to 1d and 3d convolutions as long as <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> (or
other normalization layer) normalizes on the same dimension as convolution’s
bias.</p>
<p>Models available from <a class="reference external" href="https://github.com/pytorch/vision">torchvision</a>
already implement this optimization.</p>
</div>
<div class="section" id="use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad">
<h3>Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()<a class="headerlink" href="#use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Instead of calling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="c1"># or</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>to zero out gradients, use the following method instead:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The second code snippet does not zero the memory of each individual parameter,
also the subsequent backward pass uses assignment instead of addition to store
gradients, this reduces the number of memory operations.</p>
<p>Setting gradient to <code class="docutils literal notranslate"><span class="pre">None</span></code> has a slightly different numerical behavior than
setting it to zero, for more details refer to the
<a class="reference external" href="https://pytorch.org/docs/master/optim.html#torch.optim.Optimizer.zero_grad">documentation</a>.</p>
<p>Alternatively, starting from PyTorch 1.7, call <code class="docutils literal notranslate"><span class="pre">model</span></code> or
<code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad(set_to_none=True)</span></code>.</p>
</div>
<div class="section" id="fuse-operations">
<h3>Fuse operations<a class="headerlink" href="#fuse-operations" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Pointwise operations such as elementwise addition, multiplication, and math
functions like <cite>sin()</cite>, <cite>cos()</cite>, <cite>sigmoid()</cite>, etc., can be combined into a
single kernel. This fusion helps reduce memory access and kernel launch times.
Typically, pointwise operations are memory-bound; PyTorch eager-mode initiates
a separate kernel for each operation, which involves loading data from memory,
executing the operation (often not the most time-consuming step), and writing
the results back to memory.</p>
<p>By using a fused operator, only one kernel is launched for multiple pointwise
operations, and data is loaded and stored just once. This efficiency is
particularly beneficial for activation functions, optimizers, and custom RNN cells etc.</p>
<p>PyTorch 2 introduces a compile-mode facilitated by TorchInductor, an underlying compiler
that automatically fuses kernels. TorchInductor extends its capabilities beyond simple
element-wise operations, enabling advanced fusion of eligible pointwise and reduction
operations for optimized performance.</p>
<p>In the simplest case fusion can be enabled by applying
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html">torch.compile</a>
decorator to the function definition, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mf">1.41421</span><span class="p">))</span>
</pre></div>
</div>
<p>Refer to
<a class="reference external" href="https://tutorials.pytorch.kr/intermediate/torch_compile_tutorial.html">Introduction to torch.compile</a>
for more advanced use cases.</p>
</div>
<div class="section" id="enable-channels-last-memory-format-for-computer-vision-models">
<h3>Enable channels_last memory format for computer vision models<a class="headerlink" href="#enable-channels-last-memory-format-for-computer-vision-models" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>PyTorch 1.5 introduced support for <code class="docutils literal notranslate"><span class="pre">channels_last</span></code> memory format for
convolutional networks. This format is meant to be used in conjunction with
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html">AMP</a> to further accelerate
convolutional neural networks with
<a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">Tensor Cores</a>.</p>
<p>Support for <code class="docutils literal notranslate"><span class="pre">channels_last</span></code> is experimental, but it’s expected to work for
standard computer vision models (e.g. ResNet-50, SSD). To convert models to
<code class="docutils literal notranslate"><span class="pre">channels_last</span></code> format follow
<a class="reference external" href="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html">Channels Last Memory Format Tutorial</a>.
The tutorial includes a section on
<a class="reference external" href="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html#converting-existing-models">converting existing models</a>.</p>
</div>
<div class="section" id="checkpoint-intermediate-buffers">
<h3>Checkpoint intermediate buffers<a class="headerlink" href="#checkpoint-intermediate-buffers" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Buffer checkpointing is a technique to mitigate the memory capacity burden of
model training. Instead of storing inputs of all layers to compute upstream
gradients in backward propagation, it stores the inputs of a few layers and
the others are recomputed during backward pass. The reduced memory
requirements enables increasing the batch size that can improve utilization.</p>
<p>Checkpointing targets should be selected carefully. The best is not to store
large layer outputs that have small re-computation cost. The example target
layers are activation functions (e.g. <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>, <code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">Tanh</span></code>),
up/down sampling and matrix-vector operations with small accumulation depth.</p>
<p>PyTorch supports a native
<a class="reference external" href="https://pytorch.org/docs/stable/checkpoint.html">torch.utils.checkpoint</a>
API to automatically perform checkpointing and recomputation.</p>
</div>
<div class="section" id="disable-debugging-apis">
<h3>Disable debugging APIs<a class="headerlink" href="#disable-debugging-apis" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Many PyTorch APIs are intended for debugging and should be disabled for
regular training runs:</p>
<ul class="simple">
<li><p>anomaly detection:
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly">torch.autograd.detect_anomaly</a>
or
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly">torch.autograd.set_detect_anomaly(True)</a></p></li>
<li><p>profiler related:
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx">torch.autograd.profiler.emit_nvtx</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile">torch.autograd.profiler.profile</a></p></li>
<li><p>autograd <code class="docutils literal notranslate"><span class="pre">gradcheck</span></code>:
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck">torch.autograd.gradcheck</a>
or
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck">torch.autograd.gradgradcheck</a></p></li>
</ul>
</div>
</div>
<div class="section" id="cpu-specific-optimizations">
<h2>CPU specific optimizations<a class="headerlink" href="#cpu-specific-optimizations" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="utilize-non-uniform-memory-access-numa-controls">
<h3>Utilize Non-Uniform Memory Access (NUMA) Controls<a class="headerlink" href="#utilize-non-uniform-memory-access-numa-controls" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>NUMA or non-uniform memory access is a memory layout design used in data center machines meant to take advantage of locality of memory in multi-socket machines with multiple memory controllers and blocks. Generally speaking, all deep learning workloads, training or inference, get better performance without accessing hardware resources across NUMA nodes. Thus, inference can be run with multiple instances, each instance runs on one socket, to raise throughput. For training tasks on single node, distributed training is recommended to make each training process run on one socket.</p>
<p>In general cases the following command executes a PyTorch script on cores on the Nth node only, and avoids cross-socket memory access to reduce memory access overhead.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>numactl<span class="w"> </span>--cpunodebind<span class="o">=</span>N<span class="w"> </span>--membind<span class="o">=</span>N<span class="w"> </span>python<span class="w"> </span>&lt;pytorch_script&gt;
</pre></div>
</div>
<p>More detailed descriptions can be found <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html">here</a>.</p>
</div>
<div class="section" id="utilize-openmp">
<h3>Utilize OpenMP<a class="headerlink" href="#utilize-openmp" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>OpenMP is utilized to bring better performance for parallel computation tasks.
<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> is the easiest switch that can be used to accelerate computations. It determines number of threads used for OpenMP computations.
CPU affinity setting controls how workloads are distributed over multiple cores. It affects communication overhead, cache line invalidation overhead, or page thrashing, thus proper setting of CPU affinity brings performance benefits. <code class="docutils literal notranslate"><span class="pre">GOMP_CPU_AFFINITY</span></code> or <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> determines how to bind OpenMP* threads to physical processing units. Detailed information can be found <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html">here</a>.</p>
<p>With the following command, PyTorch run the task on N OpenMP threads.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>N
</pre></div>
</div>
<p>Typically, the following environment variables are used to set for CPU affinity with GNU OpenMP implementation. <code class="docutils literal notranslate"><span class="pre">OMP_PROC_BIND</span></code> specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions. <code class="docutils literal notranslate"><span class="pre">OMP_SCHEDULE</span></code> determines how OpenMP threads are scheduled. <code class="docutils literal notranslate"><span class="pre">GOMP_CPU_AFFINITY</span></code> binds threads to specific CPUs.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_SCHEDULE</span><span class="o">=</span>STATIC
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PROC_BIND</span><span class="o">=</span>CLOSE
<span class="nb">export</span><span class="w"> </span><span class="nv">GOMP_CPU_AFFINITY</span><span class="o">=</span><span class="s2">&quot;N-M&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="intel-openmp-runtime-library-libiomp">
<h3>Intel OpenMP Runtime Library (<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)<a class="headerlink" href="#intel-openmp-runtime-library-libiomp" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>By default, PyTorch uses GNU OpenMP (GNU <code class="docutils literal notranslate"><span class="pre">libgomp</span></code>) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library (<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>) provides OpenMP API specification support. It sometimes brings more performance benefits compared to <code class="docutils literal notranslate"><span class="pre">libgomp</span></code>. Utilizing environment variable <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> can switch OpenMP library to <code class="docutils literal notranslate"><span class="pre">libiomp</span></code>:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>&lt;path&gt;/libiomp5.so:<span class="nv">$LD_PRELOAD</span>
</pre></div>
</div>
<p>Similar to CPU affinity settings in GNU OpenMP, environment variables are provided in <code class="docutils literal notranslate"><span class="pre">libiomp</span></code> to control CPU affinity settings.
<code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> binds OpenMP threads to physical processing units. <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. In most cases, setting <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> to 1 or 0 yields good performances.
The following commands show a common settings with Intel OpenMP Runtime Library.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">KMP_AFFINITY</span><span class="o">=</span><span class="nv">granularity</span><span class="o">=</span>fine,compact,1,0
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_BLOCKTIME</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</div>
<div class="section" id="switch-memory-allocator">
<h3>Switch Memory allocator<a class="headerlink" href="#switch-memory-allocator" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>For deep learning workloads, <code class="docutils literal notranslate"><span class="pre">Jemalloc</span></code> or <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> can get better performance by reusing memory as much as possible than default <code class="docutils literal notranslate"><span class="pre">malloc</span></code> function. <a class="reference external" href="https://github.com/jemalloc/jemalloc">Jemalloc</a> is a general purpose <code class="docutils literal notranslate"><span class="pre">malloc</span></code> implementation that emphasizes fragmentation avoidance and scalable concurrency support. <a class="reference external" href="https://google.github.io/tcmalloc/overview.html">TCMalloc</a> also features a couple of optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated.
Use environment variable <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> to take advantage of one of them.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>&lt;jemalloc.so/tcmalloc.so&gt;:<span class="nv">$LD_PRELOAD</span>
</pre></div>
</div>
</div>
<div class="section" id="use-onednn-graph-with-torchscript-for-inference">
<h3>Use oneDNN Graph with TorchScript for inference<a class="headerlink" href="#use-onednn-graph-with-torchscript-for-inference" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>oneDNN Graph can significantly boost inference performance. It fuses some compute-intensive operations such as convolution, matmul with their neighbor operations.
In PyTorch 2.0, it is supported as a beta feature for <code class="docutils literal notranslate"><span class="pre">Float32</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code> data-types.
oneDNN Graph receives the model’s graph and identifies candidates for operator-fusion with respect to the shape of the example input.
A model should be JIT-traced using an example input.
Speed-up would then be observed after a couple of warm-up iterations for inputs with the same shape as the example input.
The example code-snippets below are for resnet50, but they can very well be extended to use oneDNN Graph with custom models as well.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Only this extra line of code is required to use oneDNN Graph</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">enable_onednn_fusion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the oneDNN Graph API requires just one extra line of code for inference with Float32.
If you are using oneDNN Graph, please avoid calling <code class="docutils literal notranslate"><span class="pre">torch.jit.optimize_for_inference</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample input should be of the same shape as expected inputs</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)]</span>
<span class="c1"># Using resnet50 from torchvision in this example for illustrative purposes,</span>
<span class="c1"># but the line below can indeed be modified to use custom models as well.</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">)()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># Tracing the model with example input</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample_input</span><span class="p">)</span>
<span class="c1"># Invoking torch.jit.freeze</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
</pre></div>
</div>
<p>Once a model is JIT-traced with a sample input, it can then be used for inference after a couple of warm-up runs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># a couple of warm-up runs</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
    <span class="c1"># speedup would be observed after warm-up runs</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
</pre></div>
</div>
<p>While the JIT fuser for oneDNN Graph also supports inference with <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code> datatype,
performance benefit with oneDNN Graph is only exhibited by machines with AVX512_BF16
instruction set architecture (ISA).
The following code snippets serves as an example of using <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code> datatype for inference with oneDNN Graph:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_autocast_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">cache_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">):</span>
    <span class="c1"># Conv-BatchNorm folding for CNN-based Vision Models should be done with ``torch.fx.experimental.optimization.fuse`` when AMP is used</span>
    <span class="kn">import</span> <span class="nn">torch.fx.experimental.optimization</span> <span class="k">as</span> <span class="nn">optimization</span>
    <span class="c1"># Please note that optimization.fuse need not be called when AMP is not used</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">optimization</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">example_input</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="c1"># a couple of warm-up runs</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
    <span class="c1"># speedup would be observed in subsequent runs.</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="train-a-model-on-cpu-with-pytorch-distributeddataparallel-ddp-functionality">
<h3>Train a model on CPU with PyTorch <a href="#id3"><span class="problematic" id="id4">``</span></a>DistributedDataParallel``(DDP) functionality<a class="headerlink" href="#train-a-model-on-cpu-with-pytorch-distributeddataparallel-ddp-functionality" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>For small scale models or memory-bound models, such as DLRM, training on CPU is also a good choice. On a machine with multiple sockets, distributed training brings a high-efficient hardware resource usage to accelerate the training process. <a class="reference external" href="https://github.com/intel/torch-ccl">Torch-ccl</a>, optimized with Intel(R) <code class="docutils literal notranslate"><span class="pre">oneCCL</span></code> (collective communications library) for efficient distributed deep learning training implementing such collectives like <code class="docutils literal notranslate"><span class="pre">allreduce</span></code>, <code class="docutils literal notranslate"><span class="pre">allgather</span></code>, <code class="docutils literal notranslate"><span class="pre">alltoall</span></code>, implements PyTorch C10D <code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code> API and can be dynamically loaded as external <code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code>. Upon optimizations implemented in PyTorch DDP module, <code class="docutils literal notranslate"><span class="pre">torch-ccl</span></code> accelerates communication operations. Beside the optimizations made to communication kernels, <code class="docutils literal notranslate"><span class="pre">torch-ccl</span></code> also features simultaneous computation-communication functionality.</p>
</div>
</div>
<div class="section" id="gpu-specific-optimizations">
<h2>GPU specific optimizations<a class="headerlink" href="#gpu-specific-optimizations" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="enable-cudnn-auto-tuner">
<h3>Enable cuDNN auto-tuner<a class="headerlink" href="#enable-cudnn-auto-tuner" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a> supports many algorithms
to compute a convolution. Autotuner runs a short benchmark and selects the
kernel with the best performance on a given hardware for a given input size.</p>
<p>For convolutional networks (other types currently not supported), enable cuDNN
autotuner before launching the training loop by setting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<ul class="simple">
<li><p>the auto-tuner decisions may be non-deterministic; different algorithm may
be selected for different runs.  For more details see
<a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism">PyTorch: Reproducibility</a></p></li>
<li><p>in some rare cases, such as with highly variable input sizes,  it’s better
to run convolutional networks with autotuner disabled to avoid the overhead
associated with algorithm selection for each input size.</p></li>
</ul>
</div>
<div class="section" id="avoid-unnecessary-cpu-gpu-synchronization">
<h3>Avoid unnecessary CPU-GPU synchronization<a class="headerlink" href="#avoid-unnecessary-cpu-gpu-synchronization" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Avoid unnecessary synchronizations, to let the CPU run ahead of the
accelerator as much as possible to make sure that the accelerator work queue
contains many operations.</p>
<p>When possible, avoid operations which require synchronizations, for example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">print(cuda_tensor)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_tensor.item()</span></code></p></li>
<li><p>memory copies: <code class="docutils literal notranslate"><span class="pre">tensor.cuda()</span></code>,  <code class="docutils literal notranslate"><span class="pre">cuda_tensor.cpu()</span></code> and equivalent
<code class="docutils literal notranslate"><span class="pre">tensor.to(device)</span></code> calls</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_tensor.nonzero()</span></code></p></li>
<li><p>python control flow which depends on results of operations performed on CUDA
tensors e.g. <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">(cuda_tensor</span> <span class="pre">!=</span> <span class="pre">0).all()</span></code></p></li>
</ul>
</div>
<div class="section" id="create-tensors-directly-on-the-target-device">
<h3>Create tensors directly on the target device<a class="headerlink" href="#create-tensors-directly-on-the-target-device" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Instead of calling <code class="docutils literal notranslate"><span class="pre">torch.rand(size).cuda()</span></code> to generate a random tensor,
produce the output directly on the target device:
<code class="docutils literal notranslate"><span class="pre">torch.rand(size,</span> <span class="pre">device='cuda')</span></code>.</p>
<p>This is applicable to all functions which create new tensors and accept
<code class="docutils literal notranslate"><span class="pre">device</span></code> argument:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand">torch.rand()</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros">torch.zeros()</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.full.html#torch.full">torch.full()</a>
and similar.</p>
</div>
<div class="section" id="use-mixed-precision-and-amp">
<h3>Use mixed precision and AMP<a class="headerlink" href="#use-mixed-precision-and-amp" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Mixed precision leverages
<a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">Tensor Cores</a>
and offers up to 3x overall speedup on Volta and newer GPU architectures. To
use Tensor Cores AMP should be enabled and matrix/tensor dimensions should
satisfy requirements for calling kernels that use Tensor Cores.</p>
<p>To use Tensor Cores:</p>
<ul class="simple">
<li><p>set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)</p>
<ul>
<li><p>see
<a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance">Deep Learning Performance Documentation</a>
for more details and guidelines specific to layer type</p></li>
<li><p>if layer size is derived from other parameters rather than fixed, it can
still be explicitly padded e.g. vocabulary size in NLP models</p></li>
</ul>
</li>
<li><p>enable AMP</p>
<ul>
<li><p>Introduction to Mixed Precision Training and AMP:
<a class="reference external" href="https://www.youtube.com/watch?v=jF4-_ZK_tyc&amp;feature=youtu.be">video</a>,
<a class="reference external" href="https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dusan_stosic-training-neural-networks-with-tensor-cores.pdf">slides</a></p></li>
<li><p>native PyTorch AMP is available starting from PyTorch 1.6:
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html">documentation</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples">examples</a>,
<a class="reference external" href="https://tutorials.pytorch.kr/recipes/recipes/amp_recipe.html">tutorial</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="preallocate-memory-in-case-of-variable-input-length">
<h3>Preallocate memory in case of variable input length<a class="headerlink" href="#preallocate-memory-in-case-of-variable-input-length" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Models for speech recognition or for NLP are often trained on input tensors
with variable sequence length. Variable length can be problematic for PyTorch
caching allocator and can lead to reduced performance or to unexpected
out-of-memory errors. If a batch with a short sequence length is followed by
an another batch with longer sequence length, then PyTorch is forced to
release intermediate buffers from previous iteration and to re-allocate new
buffers. This process is time consuming and causes fragmentation in the
caching allocator which may result in out-of-memory errors.</p>
<p>A typical solution is to implement preallocation. It consists of the
following steps:</p>
<ol class="arabic simple">
<li><p>generate a (usually random) batch of inputs with maximum sequence length
(either corresponding to max length in the training dataset or to some
predefined threshold)</p></li>
<li><p>execute a forward and a backward pass with the generated batch, do not
execute an optimizer or a learning rate scheduler, this step preallocates
buffers of maximum size, which can be reused in subsequent
training iterations</p></li>
<li><p>zero out gradients</p></li>
<li><p>proceed to regular training</p></li>
</ol>
</div>
</div>
<div class="section" id="distributed-optimizations">
<h2>Distributed optimizations<a class="headerlink" href="#distributed-optimizations" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="use-efficient-data-parallel-backend">
<h3>Use efficient data-parallel backend<a class="headerlink" href="#use-efficient-data-parallel-backend" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>PyTorch has two ways to implement data-parallel training:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel">torch.nn.DataParallel</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> offers much better performance and scaling to
multiple-GPUs. For more information refer to the
<a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel">relevant section of CUDA Best Practices</a>
from PyTorch documentation.</p>
</div>
<div class="section" id="skip-unnecessary-all-reduce-if-training-with-distributeddataparallel-and-gradient-accumulation">
<h3>Skip unnecessary all-reduce if training with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and gradient accumulation<a class="headerlink" href="#skip-unnecessary-all-reduce-if-training-with-distributeddataparallel-and-gradient-accumulation" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>By default
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a>
executes gradient all-reduce after every backward pass to compute the average
gradient over all workers participating in the training. If training uses
gradient accumulation over N steps, then all-reduce is not necessary after
every training step, it’s only required to perform all-reduce after the last
call to backward, just before the execution of the optimizer.</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> provides
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync">no_sync()</a>
context manager which disables gradient all-reduce for particular iteration.
<code class="docutils literal notranslate"><span class="pre">no_sync()</span></code> should be applied to first <code class="docutils literal notranslate"><span class="pre">N-1</span></code> iterations of gradient
accumulation, the last iteration should follow the default execution and
perform the required gradient all-reduce.</p>
</div>
<div class="section" id="match-the-order-of-layers-in-constructors-and-during-the-execution-if-using-distributeddataparallel-find-unused-parameters-true">
<h3>Match the order of layers in constructors and during the execution if using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel(find_unused_parameters=True)</span></code><a class="headerlink" href="#match-the-order-of-layers-in-constructors-and-during-the-execution-if-using-distributeddataparallel-find-unused-parameters-true" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a>
with <code class="docutils literal notranslate"><span class="pre">find_unused_parameters=True</span></code> uses the order of layers and parameters
from model constructors to build buckets for <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>
gradient all-reduce. <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> overlaps all-reduce with the
backward pass. All-reduce for a particular bucket is asynchronously triggered
only when all gradients for parameters in a given bucket are available.</p>
<p>To maximize the amount of overlap, the order in model constructors should
roughly match the order during the execution. If the order doesn’t match, then
all-reduce for the entire bucket waits for the gradient which is the last to
arrive, this may reduce the overlap between backward pass and all-reduce,
all-reduce may end up being exposed, which slows down the training.</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> with <code class="docutils literal notranslate"><span class="pre">find_unused_parameters=False</span></code> (which is
the default setting) relies on automatic bucket formation based on order of
operations encountered during the backward pass. With
<code class="docutils literal notranslate"><span class="pre">find_unused_parameters=False</span></code> it’s not necessary to reorder layers or
parameters to achieve optimal performance.</p>
</div>
<div class="section" id="load-balance-workload-in-a-distributed-setting">
<h3>Load-balance workload in a distributed setting<a class="headerlink" href="#load-balance-workload-in-a-distributed-setting" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>Load imbalance typically may happen for models processing sequential data
(speech recognition, translation, language models etc.). If one device
receives a batch of data with sequence length longer than sequence lengths for
the remaining devices, then all devices wait for the worker which finishes
last. Backward pass functions as an implicit synchronization point in a
distributed setting with
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel</a>
backend.</p>
<p>There are multiple ways to solve the load balancing problem. The core idea is
to distribute workload over all workers as uniformly as possible within each
global batch. For example Transformer solves imbalance by forming batches with
approximately constant number of tokens (and variable number of sequences in a
batch), other models solve imbalance by bucketing samples with similar
sequence length or even by sorting dataset by sequence length.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-recipes-tuning-guide-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/8c82db84c10318a94cbe213adb618139/tuning_guide.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tuning_guide.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/38991cbc7763ed7e0f1b711da737b391/tuning_guide.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tuning_guide.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</div>


             </article>

            </div>
            <footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">

        <a href="../compiling_optimizer.html" class="btn btn-neutral float-right" title="(beta) Compiling the optimizer with torch.compile" accesskey="n" rel="next">Next <img src="../../_static/images/chevron-right-orange.svg" class="next-page"></a>


        <a href="amp_recipe.html" class="btn btn-neutral" title="Automatic Mixed Precision" accesskey="p" rel="prev"><img src="../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>

    </div>



    <hr class="community-hr hr-top" />
    <div class="community-container">
      <div class="community-prompt">더 궁금하시거나 개선할 내용이 있으신가요? 커뮤니티에 참여해보세요!</div>
      <div class="community-link"><a href="https://discuss.pytorch.kr/" aria-label="PyTorchKoreaCommunity">한국어 커뮤니티 바로가기</a></div>
    </div>
    <hr class="community-hr hr-bottom"/>

    <hr class="rating-hr hr-top" />
    <div class="rating-container">
      <div class="rating-prompt">이 튜토리얼이 어떠셨나요? 평가해주시면 이후 개선에 참고하겠습니다! :)</div>
      <div class="stars-outer">
        <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
        <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
        <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
        <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
        <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
      </div>
    </div>
    <hr class="rating-hr hr-bottom"/>


  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2024, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorch Korea User Group).

    </p>
  </div>

      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>


</footer>

          </div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> 이 튜토리얼은 프로토타입(prototype) 기능들에 대해서 설명하고 있습니다. 프로토타입 기능은 일반적으로 피드백 및 테스트용으로, 런타임 플래그 없이는 PyPI나 Conda로 배포되는 바이너리에서는 사용할 수 없습니다.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Performance Tuning Guide</a><ul>
<li><a class="reference internal" href="#general-optimizations">General optimizations</a><ul>
<li><a class="reference internal" href="#enable-asynchronous-data-loading-and-augmentation">Enable asynchronous data loading and augmentation</a></li>
<li><a class="reference internal" href="#disable-gradient-calculation-for-validation-or-inference">Disable gradient calculation for validation or inference</a></li>
<li><a class="reference internal" href="#disable-bias-for-convolutions-directly-followed-by-a-batch-norm">Disable bias for convolutions directly followed by a batch norm</a></li>
<li><a class="reference internal" href="#use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad">Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()</a></li>
<li><a class="reference internal" href="#fuse-operations">Fuse operations</a></li>
<li><a class="reference internal" href="#enable-channels-last-memory-format-for-computer-vision-models">Enable channels_last memory format for computer vision models</a></li>
<li><a class="reference internal" href="#checkpoint-intermediate-buffers">Checkpoint intermediate buffers</a></li>
<li><a class="reference internal" href="#disable-debugging-apis">Disable debugging APIs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cpu-specific-optimizations">CPU specific optimizations</a><ul>
<li><a class="reference internal" href="#utilize-non-uniform-memory-access-numa-controls">Utilize Non-Uniform Memory Access (NUMA) Controls</a></li>
<li><a class="reference internal" href="#utilize-openmp">Utilize OpenMP</a></li>
<li><a class="reference internal" href="#intel-openmp-runtime-library-libiomp">Intel OpenMP Runtime Library (<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)</a></li>
<li><a class="reference internal" href="#switch-memory-allocator">Switch Memory allocator</a></li>
<li><a class="reference internal" href="#use-onednn-graph-with-torchscript-for-inference">Use oneDNN Graph with TorchScript for inference</a></li>
<li><a class="reference internal" href="#train-a-model-on-cpu-with-pytorch-distributeddataparallel-ddp-functionality">Train a model on CPU with PyTorch ``DistributedDataParallel``(DDP) functionality</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu-specific-optimizations">GPU specific optimizations</a><ul>
<li><a class="reference internal" href="#enable-cudnn-auto-tuner">Enable cuDNN auto-tuner</a></li>
<li><a class="reference internal" href="#avoid-unnecessary-cpu-gpu-synchronization">Avoid unnecessary CPU-GPU synchronization</a></li>
<li><a class="reference internal" href="#create-tensors-directly-on-the-target-device">Create tensors directly on the target device</a></li>
<li><a class="reference internal" href="#use-mixed-precision-and-amp">Use mixed precision and AMP</a></li>
<li><a class="reference internal" href="#preallocate-memory-in-case-of-variable-input-length">Preallocate memory in case of variable input length</a></li>
</ul>
</li>
<li><a class="reference internal" href="#distributed-optimizations">Distributed optimizations</a><ul>
<li><a class="reference internal" href="#use-efficient-data-parallel-backend">Use efficient data-parallel backend</a></li>
<li><a class="reference internal" href="#skip-unnecessary-all-reduce-if-training-with-distributeddataparallel-and-gradient-accumulation">Skip unnecessary all-reduce if training with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> and gradient accumulation</a></li>
<li><a class="reference internal" href="#match-the-order-of-layers-in-constructors-and-during-the-execution-if-using-distributeddataparallel-find-unused-parameters-true">Match the order of layers in constructors and during the execution if using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel(find_unused_parameters=True)</span></code></a></li>
<li><a class="reference internal" href="#load-balance-workload-in-a-distributed-setting">Load-balance workload in a distributed setting</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>







       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
         <script src="../../_static/translations.js"></script>
         <script src="../../_static/katex.min.js"></script>
         <script src="../../_static/auto-render.min.js"></script>
         <script src="../../_static/katex_autorenderer.js"></script>
         <script src="../../_static/design-tabs.js"></script>



  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

<script>
// Helper function to make it easier to call dataLayer.push()
function gtag(){window.dataLayer.push(arguments);}

//add microsoft link
if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }

    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; display: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-LZRD6GXDLF"></script>
<script data-cfasync="false">
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-LZRD6GXDLF');   // GA4
  gtag('config', 'UA-71919972-3');  // UA
</script>


<script data-cfasync="false">
  $("[data-behavior='call-to-action-event']").on('click', function(){
    ga('send', {
      hitType: 'event',
      eventCategory: $(this).attr("data-response"),
      eventAction: 'click',
      eventLabel: window.location.href
    });

    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count"),
      'customEvent:Rating': $(this).attr("data-count") // send to GA custom dimension customEvent:Rating.
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }
</script>

<script type="text/javascript">
  var collapsedSections = ['파이토치(PyTorch) 레시피', '파이토치(PyTorch) 배우기', 'Introduction to PyTorch on YouTube', '이미지/비디오', '오디오', '텍스트', '백엔드', '강화학습', 'PyTorch 모델을 프로덕션 환경에 배포하기', 'PyTorch 프로파일링', 'Code Transforms with FX', '프론트엔드 API', 'PyTorch 확장하기', '모델 최적화', '병렬 및 분산 학습', 'Edge with ExecuTorch', '추천 시스템', 'Multimodality'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>PyTorchKorea @ GitHub</h2>
          <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
          <a class="with-right-arrow" href="https://github.com/PyTorchKorea" target="_blank">GitHub로 이동</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>한국어 튜토리얼</h2>
          <p>한국어로 번역 중인 PyTorch 튜토리얼입니다.</p>
          <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>커뮤니티</h2>
          <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
          <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.kr/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
            <li><a href="https://pytorch.kr//about">사용자 모임 소개</a></li>
            <li><a href="https://pytorch.kr//about/contributors">기여해주신 분들</a></li>
            <li><a href="https://pytorch.kr//resources">리소스</a></li>
            <li><a href="https://pytorch.kr//coc">행동 강령</a></li>
          </ul>
        </div>
      </div>
      <div class="trademark-disclaimer">
        <ul>
          <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 정책은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
        </ul>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.kr/get-started">시작하기</a>
          </li>

          <li class="active">
            <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
          </li>

          <li>
            <a href="https://pytorch.kr/hub">허브</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">커뮤니티</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>