


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>(experimental) Dynamic Quantization on BERT &mdash; PyTorch Tutorials 1.4.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Pruning Tutorial" href="pruning_tutorial.html" />
    <link rel="prev" title="(experimental) Quantized Transfer Learning for Computer Vision Tutorial" href="quantized_transfer_learning_tutorial.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
</head>

<a class="github-ribbon" href="https://github.com/9bow/PyTorch-tutorials-kr">
  <img class="ribbon-img" width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_red_aa0000.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1">
</a>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.4.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">시작하기 (Getting Started)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">파이토치(PyTorch)로 딥러닝하기: 60분만에 끝장내기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/data_loading_tutorial.html">사용자 정의 Dataset, Dataloader, Transforms 작성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
</ul>
<p class="caption"><span class="caption-text">이미지 (Image)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision 객체 검출 미세조정(Finetuning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/neural_style_tutorial.html">Neural Transfer Using PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
</ul>
<p class="caption"><span class="caption-text">오디오 (Audio)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_preprocessing_tutorial.html">torchaudio Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">텍스트 (Text)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_generation_tutorial.html">기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/text_sentiment_ngrams_tutorial.html">Text Classification with TorchText</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/torchtext_translation_tutorial.html">TorchText로 언어 번역하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transformer_tutorial.html">nn.Transformer 와 TorchText 로 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델링하기</a></li>
</ul>
<p class="caption"><span class="caption-text">Named Tensor (experimental)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="named_tensor_tutorial.html">(experimental) Introduction to Named Tensors in PyTorch</a></li>
</ul>
<p class="caption"><span class="caption-text">강화 학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
</ul>
<p class="caption"><span class="caption-text">PyTorch 모델을 운영환경에 배포하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="flask_rest_api_tutorial.html">Flask를 이용하여 Python에서 PyTorch를 REST API로 배포하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">TorchScript 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_export.html">C++에서 TorchScript 모델 로딩하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_onnxruntime.html">(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기</a></li>
</ul>
<p class="caption"><span class="caption-text">병렬 &amp; 분산 학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_parallel_tutorial.html">Single-Machine Model Parallel Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_tuto.html">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/aws_distributed_training_tutorial.html">(advanced) PyTorch 1.0 Distributed Trainer with Amazon AWS</a></li>
</ul>
<p class="caption"><span class="caption-text">PyTorch 확장하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html">Creating Extensions Using numpy and scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
</ul>
<p class="caption"><span class="caption-text">모델 최적화</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../advanced/dynamic_quantization_tutorial.html">(experimental) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_quantization_tutorial.html">(experimental) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_transfer_learning_tutorial.html">(experimental) Quantized Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">(experimental) Dynamic Quantization on BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">Pruning Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">다른 언어에서의 PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
</ul>
<p class="caption"><span class="caption-text">PyTorch Fundamentals In-Depth</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html">What is <cite>torch.nn</cite> <em>really</em>?</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>(experimental) Dynamic Quantization on BERT</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/intermediate/dynamic_quantization_bert_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">intermediate/dynamic_quantization_bert_tutorial</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="experimental-dynamic-quantization-on-bert">
<h1>(experimental) Dynamic Quantization on BERT<a class="headerlink" href="#experimental-dynamic-quantization-on-bert" title="Permalink to this headline">¶</a></h1>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To get the most of this tutorial, we suggest using this
<a class="reference external" href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/dynamic_quantization_bert_tutorial.ipynb">Colab Version</a>. This will allow you to experiment with the information presented below.</p>
</div>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/jianyuh">Jianyu Huang</a></p>
<p><strong>Reviewed by</strong>: <a class="reference external" href="https://github.com/raghuramank100">Raghuraman Krishnamoorthi</a></p>
<p><strong>Edited by</strong>: <a class="reference external" href="https://github.com/jlin27">Jessica Lin</a></p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we will apply the dynamic quantization on a BERT
model, closely following the BERT model from <a class="reference external" href="https://github.com/huggingface/transformers">the HuggingFace
Transformers examples</a>.
With this step-by-step journey, we would like to demonstrate how to
convert a well-known state-of-the-art model like BERT into dynamic
quantized model.</p>
<ul class="simple">
<li><p>BERT, or Bidirectional Embedding Representations from Transformers,
is a new method of pre-training language representations which
achieves the state-of-the-art accuracy results on many popular
Natural Language Processing (NLP) tasks, such as question answering,
text classification, and others. The original paper can be found
<a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">here</a>.</p></li>
<li><p>Dynamic quantization support in PyTorch converts a float model to a
quantized model with static int8 or float16 data types for the
weights and dynamic quantization for the activations. The activations
are quantized dynamically (per batch) to int8 when the weights are
quantized to int8. In PyTorch, we have <a class="reference external" href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic">torch.quantization.quantize_dynamic API</a>,
which replaces specified modules with dynamic weight-only quantized
versions and output the quantized model.</p></li>
<li><p>We demonstrate the accuracy and inference performance results on the
<a class="reference external" href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">Microsoft Research Paraphrase Corpus (MRPC) task</a>
in the General Language Understanding Evaluation benchmark <a class="reference external" href="https://gluebenchmark.com/">(GLUE)</a>. The MRPC (Dolan and Brockett, 2005) is
a corpus of sentence pairs automatically extracted from online news
sources, with human annotations of whether the sentences in the pair
are semantically equivalent. As the classes are imbalanced (68%
positive, 32% negative), we follow the common practice and report
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1 score</a>.
MRPC is a common NLP task for language pair classification, as shown
below.</p></li>
</ul>
<img alt="../_images/bert1.png" src="../_images/bert1.png" />
</div>
<div class="section" id="setup">
<h2>1. Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="section" id="install-pytorch-and-huggingface-transformers">
<h3>1.1 Install PyTorch and HuggingFace Transformers<a class="headerlink" href="#install-pytorch-and-huggingface-transformers" title="Permalink to this headline">¶</a></h3>
<p>To start this tutorial, let’s first follow the installation instructions
in PyTorch <a class="reference external" href="https://github.com/pytorch/pytorch/#installation">here</a> and HuggingFace Github Repo <a class="reference external" href="https://github.com/huggingface/transformers#installation">here</a>.
In addition, we also install <a class="reference external" href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a> package, as we will reuse its
built-in F1 score calculation helper function.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip install sklearn
pip install transformers
</pre></div>
</div>
<p>Because we will be using the experimental parts of the PyTorch, it is
recommended to install the latest version of torch and torchvision. You
can find the most recent instructions on local installation <a class="reference external" href="https://pytorch.org/get-started/locally/">here</a>. For example, to install on
Mac:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>yes y <span class="p">|</span> pip uninstall torch tochvision
yes y <span class="p">|</span> pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html
</pre></div>
</div>
</div>
<div class="section" id="import-the-necessary-modules">
<h3>1.2 Import the necessary modules<a class="headerlink" href="#import-the-necessary-modules" title="Permalink to this headline">¶</a></h3>
<p>In this step we import the necessary Python modules for the tutorial.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="p">(</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">SequentialSampler</span><span class="p">,</span>
                              <span class="n">TensorDataset</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span><span class="p">,)</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_compute_metrics</span> <span class="k">as</span> <span class="n">compute_metrics</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_output_modes</span> <span class="k">as</span> <span class="n">output_modes</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_processors</span> <span class="k">as</span> <span class="n">processors</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">glue_convert_examples_to_features</span> <span class="k">as</span> <span class="n">convert_examples_to_features</span>

<span class="c1"># Setup logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> - </span><span class="si">%(levelname)s</span><span class="s1"> - </span><span class="si">%(name)s</span><span class="s1"> -   </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
                    <span class="n">datefmt</span> <span class="o">=</span> <span class="s1">&#39;%m/</span><span class="si">%d</span><span class="s1">/%Y %H:%M:%S&#39;</span><span class="p">,</span>
                    <span class="n">level</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">)</span>

<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;transformers.modeling_utils&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span>
   <span class="n">logging</span><span class="o">.</span><span class="n">WARN</span><span class="p">)</span>  <span class="c1"># Reduce logging</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<p>We set the number of threads to compare the single thread performance between FP32 and INT8 performance.
In the end of the tutorial, the user can set other number of threads by building PyTorch with right parallel backend.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__config__</span><span class="o">.</span><span class="n">parallel_info</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="learn-about-helper-functions">
<h3>1.3 Learn about helper functions<a class="headerlink" href="#learn-about-helper-functions" title="Permalink to this headline">¶</a></h3>
<p>The helper functions are built-in in transformers library. We mainly use
the following helper functions: one for converting the text examples
into the feature vectors; The other one for measuring the F1 score of
the predicted result.</p>
<p>The <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py">glue_convert_examples_to_features</a> function converts the texts into input features:</p>
<ul class="simple">
<li><p>Tokenize the input sequences;</p></li>
<li><p>Insert [CLS] in the beginning;</p></li>
<li><p>Insert [SEP] between the first sentence and the second sentence, and
in the end;</p></li>
<li><p>Generate token type ids to indicate whether a token belongs to the
first sequence or the second sequence.</p></li>
</ul>
<p>The <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py">glue_compute_metrics</a>  function has the compute metrics with
the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1 score</a>, which
can be interpreted as a weighted average of the precision and recall,
where an F1 score reaches its best value at 1 and worst score at 0. The
relative contribution of precision and recall to the F1 score are equal.</p>
<ul class="simple">
<li><p>The equation for the F1 score is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[F1 = 2 * (\text{precision} * \text{recall}) / (\text{precision} + \text{recall})\]</div>
</div>
<div class="section" id="download-the-dataset">
<h3>1.4 Download the dataset<a class="headerlink" href="#download-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>Before running MRPC tasks we download the <a class="reference external" href="https://gluebenchmark.com/tasks">GLUE data</a> by running <a class="reference external" href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e">this script</a>
and unpack it to a directory <code class="docutils literal notranslate"><span class="pre">glue_data</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python download_glue_data.py --data_dir<span class="o">=</span><span class="s1">&#39;glue_data&#39;</span> --tasks<span class="o">=</span><span class="s1">&#39;MRPC&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="fine-tune-the-bert-model">
<h2>2. Fine-tune the BERT model<a class="headerlink" href="#fine-tune-the-bert-model" title="Permalink to this headline">¶</a></h2>
<p>The spirit of BERT is to pre-train the language representations and then
to fine-tune the deep bi-directional representations on a wide range of
tasks with minimal task-dependent parameters, and achieves
state-of-the-art results. In this tutorial, we will focus on fine-tuning
with the pre-trained BERT model to classify semantically equivalent
sentence pairs on MRPC task.</p>
<p>To fine-tune the pre-trained BERT model (<code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code> model in
HuggingFace transformers) for the MRPC task, you can follow the command
in <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples#mrpc">examples</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>export GLUE_DIR=./glue_data
export TASK_NAME=MRPC
export OUT_DIR=./$TASK_NAME/
python ./run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir $GLUE_DIR/$TASK_NAME \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --save_steps 100000 \
    --output_dir $OUT_DIR
</pre></div>
</div>
<p>We provide the fined-tuned BERT model for MRPC task <a class="reference external" href="https://download.pytorch.org/tutorial/MRPC.zip">here</a>.
To save time, you can download the model file (~400 MB) directly into your local folder <code class="docutils literal notranslate"><span class="pre">$OUT_DIR</span></code>.</p>
<div class="section" id="set-global-configurations">
<h3>2.1 Set global configurations<a class="headerlink" href="#set-global-configurations" title="Permalink to this headline">¶</a></h3>
<p>Here we set the global configurations for evaluating the fine-tuned BERT
model before and after the dynamic quantization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">configs</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">()</span>

<span class="c1"># The output directory for the fine-tuned model, $OUT_DIR.</span>
<span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;./MRPC/&quot;</span>

<span class="c1"># The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.</span>
<span class="n">configs</span><span class="o">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="s2">&quot;./glue_data/MRPC&quot;</span>

<span class="c1"># The model name or path for the pre-trained model.</span>
<span class="n">configs</span><span class="o">.</span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="c1"># The maximum length of an input sequence</span>
<span class="n">configs</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># Prepare GLUE task.</span>
<span class="n">configs</span><span class="o">.</span><span class="n">task_name</span> <span class="o">=</span> <span class="s2">&quot;MRPC&quot;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="n">configs</span><span class="o">.</span><span class="n">processor</span> <span class="o">=</span> <span class="n">processors</span><span class="p">[</span><span class="n">configs</span><span class="o">.</span><span class="n">task_name</span><span class="p">]()</span>
<span class="n">configs</span><span class="o">.</span><span class="n">output_mode</span> <span class="o">=</span> <span class="n">output_modes</span><span class="p">[</span><span class="n">configs</span><span class="o">.</span><span class="n">task_name</span><span class="p">]</span>
<span class="n">configs</span><span class="o">.</span><span class="n">label_list</span> <span class="o">=</span> <span class="n">configs</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">get_labels</span><span class="p">()</span>
<span class="n">configs</span><span class="o">.</span><span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;bert&quot;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="n">configs</span><span class="o">.</span><span class="n">do_lower_case</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Set the device, batch size, topology, and caching flags.</span>
<span class="n">configs</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">configs</span><span class="o">.</span><span class="n">per_gpu_eval_batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">configs</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">configs</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">configs</span><span class="o">.</span><span class="n">overwrite_cache</span> <span class="o">=</span> <span class="kc">False</span>


<span class="c1"># Set random seed for reproducibility.</span>
<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="load-the-fine-tuned-bert-model">
<h3>2.2 Load the fine-tuned BERT model<a class="headerlink" href="#load-the-fine-tuned-bert-model" title="Permalink to this headline">¶</a></h3>
<p>We load the tokenizer and fine-tuned BERT sequence classifier model
(FP32) from the <code class="docutils literal notranslate"><span class="pre">configs.output_dir</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="n">configs</span><span class="o">.</span><span class="n">do_lower_case</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">configs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="define-the-tokenize-and-evaluation-function">
<h3>2.3 Define the tokenize and evaluation function<a class="headerlink" href="#define-the-tokenize-and-evaluation-function" title="Permalink to this headline">¶</a></h3>
<p>We reuse the tokenize and evaluation function from <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py">Huggingface</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.</span>
<span class="c1"># Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="c1"># Loop to handle MNLI double evaluation (matched, mis-matched)</span>
    <span class="n">eval_task_names</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;mnli&quot;</span><span class="p">,</span> <span class="s2">&quot;mnli-mm&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">task_name</span> <span class="o">==</span> <span class="s2">&quot;mnli&quot;</span> <span class="k">else</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">task_name</span><span class="p">,)</span>
    <span class="n">eval_outputs_dirs</span> <span class="o">=</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">+</span> <span class="s1">&#39;-MM&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">task_name</span> <span class="o">==</span> <span class="s2">&quot;mnli&quot;</span> <span class="k">else</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">eval_task</span><span class="p">,</span> <span class="n">eval_output_dir</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">eval_task_names</span><span class="p">,</span> <span class="n">eval_outputs_dirs</span><span class="p">):</span>
        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">eval_task</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">evaluate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">)</span>

        <span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">per_gpu_eval_batch_size</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span><span class="p">)</span>
        <span class="c1"># Note that DistributedSampler samples randomly</span>
        <span class="n">eval_sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">)</span>
        <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">eval_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">)</span>

        <span class="c1"># multi-gpu eval</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">n_gpu</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="c1"># Eval!</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;***** Running evaluation </span><span class="si">{}</span><span class="s2"> *****&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Num examples = </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Batch size = </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">)</span>
        <span class="n">eval_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">nb_eval_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">out_label_ids</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Evaluating&quot;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;input_ids&#39;</span><span class="p">:</span>      <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                          <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                          <span class="s1">&#39;labels&#39;</span><span class="p">:</span>         <span class="n">batch</span><span class="p">[</span><span class="mi">3</span><span class="p">]}</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">model_type</span> <span class="o">!=</span> <span class="s1">&#39;distilbert&#39;</span><span class="p">:</span>
                    <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;token_type_ids&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bert&#39;</span><span class="p">,</span> <span class="s1">&#39;xlnet&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>  <span class="c1"># XLM, DistilBERT and RoBERTa don&#39;t use segment_ids</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">tmp_eval_loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

                <span class="n">eval_loss</span> <span class="o">+=</span> <span class="n">tmp_eval_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">nb_eval_steps</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">preds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">preds</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="n">out_label_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">out_label_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_label_ids</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">eval_loss</span> <span class="o">=</span> <span class="n">eval_loss</span> <span class="o">/</span> <span class="n">nb_eval_steps</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">output_mode</span> <span class="o">==</span> <span class="s2">&quot;classification&quot;</span><span class="p">:</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">output_mode</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">(</span><span class="n">eval_task</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">out_label_ids</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

        <span class="n">output_eval_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_output_dir</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="s2">&quot;eval_results.txt&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_eval_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;***** Eval results </span><span class="si">{}</span><span class="s2"> *****&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  </span><span class="si">%s</span><span class="s2"> = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">]))</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> = </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="n">key</span><span class="p">])))</span>

    <span class="k">return</span> <span class="n">results</span>


<span class="k">def</span> <span class="nf">load_and_cache_examples</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">evaluate</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">evaluate</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># Make sure only the first process in distributed training process the dataset, and the others will use the cache</span>

    <span class="n">processor</span> <span class="o">=</span> <span class="n">processors</span><span class="p">[</span><span class="n">task</span><span class="p">]()</span>
    <span class="n">output_mode</span> <span class="o">=</span> <span class="n">output_modes</span><span class="p">[</span><span class="n">task</span><span class="p">]</span>
    <span class="c1"># Load data features from cache or dataset file</span>
    <span class="n">cached_features_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;cached_</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="s1">&#39;dev&#39;</span> <span class="k">if</span> <span class="n">evaluate</span> <span class="k">else</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span>
        <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">model_name_or_path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)))</span><span class="o">.</span><span class="n">pop</span><span class="p">(),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">),</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">task</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">overwrite_cache</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading features from cached file </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cached_features_file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Creating features from dataset file at </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">)</span>
        <span class="n">label_list</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">get_labels</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">task</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;mnli&#39;</span><span class="p">,</span> <span class="s1">&#39;mnli-mm&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;roberta&#39;</span><span class="p">]:</span>
            <span class="c1"># HACK(label indices are swapped in RoBERTa pretrained model)</span>
            <span class="n">label_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label_list</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_list</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">examples</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">get_dev_examples</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">evaluate</span> <span class="k">else</span> <span class="n">processor</span><span class="o">.</span><span class="n">get_train_examples</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">convert_examples_to_features</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span>
                                                <span class="n">tokenizer</span><span class="p">,</span>
                                                <span class="n">label_list</span><span class="o">=</span><span class="n">label_list</span><span class="p">,</span>
                                                <span class="n">max_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">,</span>
                                                <span class="n">output_mode</span><span class="o">=</span><span class="n">output_mode</span><span class="p">,</span>
                                                <span class="n">pad_on_left</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;xlnet&#39;</span><span class="p">]),</span>                 <span class="c1"># pad on the left for xlnet</span>
                                                <span class="n">pad_token</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">])[</span><span class="mi">0</span><span class="p">],</span>
                                                <span class="n">pad_token_segment_id</span><span class="o">=</span><span class="mi">4</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;xlnet&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Saving features into cached file </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">cached_features_file</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">evaluate</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># Make sure only the first process in distributed training process the dataset, and the others will use the cache</span>

    <span class="c1"># Convert to Tensors and build dataset</span>
    <span class="n">all_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">input_ids</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">attention_mask</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="n">all_token_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">token_type_ids</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_mode</span> <span class="o">==</span> <span class="s2">&quot;classification&quot;</span><span class="p">:</span>
        <span class="n">all_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">label</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">output_mode</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span><span class="p">:</span>
        <span class="n">all_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">label</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">all_input_ids</span><span class="p">,</span> <span class="n">all_attention_mask</span><span class="p">,</span> <span class="n">all_token_type_ids</span><span class="p">,</span> <span class="n">all_labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="apply-the-dynamic-quantization">
<h2>3. Apply the dynamic quantization<a class="headerlink" href="#apply-the-dynamic-quantization" title="Permalink to this headline">¶</a></h2>
<p>We call <code class="docutils literal notranslate"><span class="pre">torch.quantization.quantize_dynamic</span></code> on the model to apply
the dynamic quantization on the HuggingFace BERT model. Specifically,</p>
<ul class="simple">
<li><p>We specify that we want the torch.nn.Linear modules in our model to
be quantized;</p></li>
<li><p>We specify that we want weights to be converted to quantized int8
values.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="check-the-model-size">
<h3>3.1 Check the model size<a class="headerlink" href="#check-the-model-size" title="Permalink to this headline">¶</a></h3>
<p>Let’s first check the model size. We can observe a significant reduction
in model size (FP32 total size: 438 MB; INT8 total size: 181 MB):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_size_of_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;temp.p&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Size (MB):&#39;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getsize</span><span class="p">(</span><span class="s2">&quot;temp.p&quot;</span><span class="p">)</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;temp.p&#39;</span><span class="p">)</span>

<span class="n">print_size_of_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">print_size_of_model</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The BERT model used in this tutorial (<code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>) has a
vocabulary size V of 30522. With the embedding size of 768, the total
size of the word embedding table is ~ 4 (Bytes/FP32) * 30522 * 768 =
90 MB. So with the help of quantization, the model size of the
non-embedding table part is reduced from 350 MB (FP32 model) to 90 MB
(INT8 model).</p>
</div>
<div class="section" id="evaluate-the-inference-accuracy-and-time">
<h3>3.2 Evaluate the inference accuracy and time<a class="headerlink" href="#evaluate-the-inference-accuracy-and-time" title="Permalink to this headline">¶</a></h3>
<p>Next, let’s compare the inference time as well as the evaluation
accuracy between the original FP32 model and the INT8 model after the
dynamic quantization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">time_model_evaluation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">configs</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">eval_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">configs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="n">eval_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">eval_duration_time</span> <span class="o">=</span> <span class="n">eval_end_time</span> <span class="o">-</span> <span class="n">eval_start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluate total time (seconds): </span><span class="si">{0:.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eval_duration_time</span><span class="p">))</span>

<span class="c1"># Evaluate the original FP32 BERT model</span>
<span class="n">time_model_evaluation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">configs</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># Evaluate the INT8 BERT model after the dynamic quantization</span>
<span class="n">time_model_evaluation</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">configs</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
<p>Running this locally on a MacBook Pro, without quantization, inference
(for all 408 examples in MRPC dataset) takes about 160 seconds, and with
quantization it takes just about 90 seconds. We summarize the results
for running the quantized BERT model inference on a Macbook Pro as the
follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|</span> <span class="n">Prec</span> <span class="o">|</span> <span class="n">F1</span> <span class="n">score</span> <span class="o">|</span> <span class="n">Model</span> <span class="n">Size</span> <span class="o">|</span> <span class="mi">1</span> <span class="n">thread</span> <span class="o">|</span> <span class="mi">4</span> <span class="n">threads</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">FP32</span> <span class="o">|</span>  <span class="mf">0.9019</span>  <span class="o">|</span>   <span class="mi">438</span> <span class="n">MB</span>   <span class="o">|</span> <span class="mi">160</span> <span class="n">sec</span>  <span class="o">|</span> <span class="mi">85</span> <span class="n">sec</span>    <span class="o">|</span>
<span class="o">|</span> <span class="n">INT8</span> <span class="o">|</span>  <span class="mf">0.8953</span>  <span class="o">|</span>   <span class="mi">181</span> <span class="n">MB</span>   <span class="o">|</span>  <span class="mi">90</span> <span class="n">sec</span>  <span class="o">|</span> <span class="mi">46</span> <span class="n">sec</span>    <span class="o">|</span>
</pre></div>
</div>
<p>We have 0.6% F1 score accuracy after applying the post-training dynamic
quantization on the fine-tuned BERT model on the MRPC task. As a
comparison, in a <a class="reference external" href="https://arxiv.org/pdf/1910.06188.pdf">recent paper</a> (Table 1),
it achieved 0.8788 by
applying the post-training dynamic quantization and 0.8956 by applying
the quantization-aware training. The main difference is that we support the
asymmetric quantization in PyTorch while that paper supports the
symmetric quantization only.</p>
<p>Note that we set the number of threads to 1 for the single-thread
comparison in this tutorial. We also support the intra-op
parallelization for these quantized INT8 operators. The users can now
set multi-thread by <code class="docutils literal notranslate"><span class="pre">torch.set_num_threads(N)</span></code> (<code class="docutils literal notranslate"><span class="pre">N</span></code> is the number of
intra-op parallelization threads). One preliminary requirement to enable
the intra-op parallelization support is to build PyTorch with the right
<a class="reference external" href="https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#build-options">backend</a>
such as OpenMP, Native or TBB.
You can use <code class="docutils literal notranslate"><span class="pre">torch.__config__.parallel_info()</span></code> to check the
parallelization settings. On the same MacBook Pro using PyTorch with
Native backend for parallelization, we can get about 46 seconds for
processing the evaluation of MRPC dataset.</p>
</div>
<div class="section" id="serialize-the-quantized-model">
<h3>3.3 Serialize the quantized model<a class="headerlink" href="#serialize-the-quantized-model" title="Permalink to this headline">¶</a></h3>
<p>We can serialize and save the quantized model for the future use.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quantized_output_dir</span> <span class="o">=</span> <span class="n">configs</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">+</span> <span class="s2">&quot;quantized/&quot;</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">)</span>
    <span class="n">quantized_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quantized_output_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we demonstrated how to demonstrate how to convert a
well-known state-of-the-art NLP model like BERT into dynamic quantized
model. Dynamic quantization can reduce the size of the model while only
having a limited implication on accuracy.</p>
<p>Thanks for reading! As always, we welcome any feedback, so please create
an issue <a class="reference external" href="https://github.com/pytorch/pytorch/issues">here</a> if you have
any.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, <a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding (2018)</a>.</p>
<p>[2] <a class="reference external" href="https://github.com/huggingface/transformers">HuggingFace Transformers</a>.</p>
<p>[3] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat (2019). <a class="reference external" href="https://arxiv.org/pdf/1910.06188.pdf">Q8BERT:
Quantized 8bit BERT</a>.</p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="pruning_tutorial.html" class="btn btn-neutral float-right" title="Pruning Tutorial" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="quantized_transfer_learning_tutorial.html" class="btn btn-neutral" title="(experimental) Quantized Transfer Learning for Computer Vision Tutorial" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr class="helpful-hr hr-top">
      <div class="helpful-container">
        <div class="helpful-question">Was this helpful?</div>
        <div class="helpful-question yes-link" data-behavior="was-this-helpful-event" data-response="yes">Yes</div>
        <div class="helpful-question no-link" data-behavior="was-this-helpful-event" data-response="no">No</div>
        <div class="was-helpful-thank-you">Thank you</div>
      </div>
    <hr class="helpful-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, PyTorch.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">(experimental) Dynamic Quantization on BERT</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#setup">1. Setup</a><ul>
<li><a class="reference internal" href="#install-pytorch-and-huggingface-transformers">1.1 Install PyTorch and HuggingFace Transformers</a></li>
<li><a class="reference internal" href="#import-the-necessary-modules">1.2 Import the necessary modules</a></li>
<li><a class="reference internal" href="#learn-about-helper-functions">1.3 Learn about helper functions</a></li>
<li><a class="reference internal" href="#download-the-dataset">1.4 Download the dataset</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fine-tune-the-bert-model">2. Fine-tune the BERT model</a><ul>
<li><a class="reference internal" href="#set-global-configurations">2.1 Set global configurations</a></li>
<li><a class="reference internal" href="#load-the-fine-tuned-bert-model">2.2 Load the fine-tuned BERT model</a></li>
<li><a class="reference internal" href="#define-the-tokenize-and-evaluation-function">2.3 Define the tokenize and evaluation function</a></li>
</ul>
</li>
<li><a class="reference internal" href="#apply-the-dynamic-quantization">3. Apply the dynamic quantization</a><ul>
<li><a class="reference internal" href="#check-the-model-size">3.1 Check the model size</a></li>
<li><a class="reference internal" href="#evaluate-the-inference-accuracy-and-time">3.2 Evaluate the inference accuracy and time</a></li>
<li><a class="reference internal" href="#serialize-the-quantized-model">3.3 Serialize the quantized model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71919972-3', 'auto');
  ga('send', 'pageview');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    ga('send', {
      hitType: 'event',
      eventCategory: 'Download',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   $("[data-behavior='was-this-helpful-event']").on('click', function(){
    $(".helpful-question").hide();
    $(".was-helpful-thank-you").show();
    ga('send', {
      hitType: 'event',
      eventCategory: 'Was this Helpful?',
      eventAction: 'click',
      eventLabel: $(this).attr("data-response")
    });
   });

   if (location.pathname == "/") {
     $(".helpful-container").hide();
     $(".hr-bottom").hide();
   }
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>