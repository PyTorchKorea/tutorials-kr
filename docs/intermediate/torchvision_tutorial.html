
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="TorchVision Object Detection Finetuning Tutorial" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="For this tutorial, we will be finetuning a pre-trained Mask R-CNN model on the Penn-Fudan Database for Pedestrian Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an ..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="For this tutorial, we will be finetuning a pre-trained Mask R-CNN model on the Penn-Fudan Database for Pedestrian Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an ..." />
<meta property="og:ignore_canonical" content="true" />

    <title>TorchVision Object Detection Finetuning Tutorial &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/torchvision_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)" href="../beginner/transfer_learning_tutorial.html" />
    <link rel="prev" title="Domains" href="../domains.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">공간 변형 네트워크(Spatial Transformer Networks) 튜토리얼</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="mario_rl_tutorial.html">마리오 게임 RL 에이전트로 학습하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torchrec_intro_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Domains</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable/index.html">See Audio tutorials on the audio website</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/index.html">See ExecuTorch tutorials on the ExecuTorch website</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../domains.html" class="nav-link">Domains</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">TorchVision...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../domains.html">
        <meta itemprop="name" content="Domains">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="TorchVision Object Detection Finetuning Tutorial">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/torchvision_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-torchvision-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="torchvision-object-detection-finetuning-tutorial">
<span id="sphx-glr-intermediate-torchvision-tutorial-py"></span><h1>TorchVision Object Detection Finetuning Tutorial<a class="headerlink" href="#torchvision-object-detection-finetuning-tutorial" title="Link to this heading">#</a></h1>
<p>For this tutorial, we will be finetuning a pre-trained <a class="reference external" href="https://arxiv.org/abs/1703.06870">Mask
R-CNN</a> model on the <a class="reference external" href="https://www.cis.upenn.edu/~jshi/ped_html/">Penn-Fudan
Database for Pedestrian Detection and
Segmentation</a>. It contains
170 images with 345 instances of pedestrians, and we will use it to
illustrate how to use the new features in torchvision in order to train
an object detection and instance segmentation model on a custom dataset.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>This tutorial works only with torchvision version &gt;=0.16 or nightly.
If you’re using torchvision&lt;=0.15, please follow
<a class="reference external" href="https://github.com/pytorchkorea/tutorials-kr/blob/d686b662932a380a58b7683425faa00c06bcf502/intermediate_source/torchvision_tutorial.rst">this tutorial instead</a>.</p>
</div>
<section id="defining-the-dataset">
<h2>Defining the Dataset<a class="headerlink" href="#defining-the-dataset" title="Link to this heading">#</a></h2>
<p>The reference scripts for training object detection, instance
segmentation and person keypoint detection allows for easily supporting
adding new custom datasets. The dataset should inherit from the standard
<a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code></a> class, and implement <code class="docutils literal notranslate"><span class="pre">__len__</span></code> and
<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>.</p>
<p>The only specificity that we require is that the dataset <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>
should return a tuple:</p>
<ul class="simple">
<li><p>image: <a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.Image</span></code></a> of shape <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">H,</span> <span class="pre">W]</span></code>, a pure tensor, or a PIL Image of size <code class="docutils literal notranslate"><span class="pre">(H,</span> <span class="pre">W)</span></code></p></li>
<li><p>target: a dict containing the following fields</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">boxes</span></code>, <a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.BoundingBoxes</span></code></a> of shape <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">4]</span></code>:
the coordinates of the <code class="docutils literal notranslate"><span class="pre">N</span></code> bounding boxes in <code class="docutils literal notranslate"><span class="pre">[x0,</span> <span class="pre">y0,</span> <span class="pre">x1,</span> <span class="pre">y1]</span></code> format, ranging from <code class="docutils literal notranslate"><span class="pre">0</span></code>
to <code class="docutils literal notranslate"><span class="pre">W</span></code> and <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">H</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">labels</span></code>, integer <a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> of shape <code class="docutils literal notranslate"><span class="pre">[N]</span></code>: the label for each bounding box.
<code class="docutils literal notranslate"><span class="pre">0</span></code> represents always the background class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">image_id</span></code>, int: an image identifier. It should be
unique between all the images in the dataset, and is used during
evaluation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">area</span></code>, float <a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> of shape <code class="docutils literal notranslate"><span class="pre">[N]</span></code>: the area of the bounding box. This is used
during evaluation with the COCO metric, to separate the metric
scores between small, medium and large boxes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">iscrowd</span></code>, uint8 <a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> of shape <code class="docutils literal notranslate"><span class="pre">[N]</span></code>: instances with <code class="docutils literal notranslate"><span class="pre">iscrowd=True</span></code> will be
ignored during evaluation.</p></li>
<li><p>(optionally) <code class="docutils literal notranslate"><span class="pre">masks</span></code>, <a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.Mask</span></code></a> of shape <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">H,</span> <span class="pre">W]</span></code>: the segmentation
masks for each one of the objects</p></li>
</ul>
</li>
</ul>
<p>If your dataset is compliant with above requirements then it will work for both
training and evaluation codes from the reference script. Evaluation code will use scripts from
<code class="docutils literal notranslate"><span class="pre">pycocotools</span></code> which can be installed with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pycocotools</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>For Windows, please install <code class="docutils literal notranslate"><span class="pre">pycocotools</span></code> from <a class="reference external" href="https://github.com/gautamchitnis/cocoapi">gautamchitnis</a> with command</p>
<p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">git+https://github.com/gautamchitnis/cocoapi.git&#64;cocodataset-master#subdirectory=PythonAPI</span></code></p>
</div>
<p>One note on the <code class="docutils literal notranslate"><span class="pre">labels</span></code>. The model considers class <code class="docutils literal notranslate"><span class="pre">0</span></code> as background. If your dataset does not contain the background class,
you should not have <code class="docutils literal notranslate"><span class="pre">0</span></code> in your <code class="docutils literal notranslate"><span class="pre">labels</span></code>. For example, assuming you have just two classes, <em>cat</em> and <em>dog</em>, you can
define <code class="docutils literal notranslate"><span class="pre">1</span></code> (not <code class="docutils literal notranslate"><span class="pre">0</span></code>) to represent <em>cats</em> and <code class="docutils literal notranslate"><span class="pre">2</span></code> to represent <em>dogs</em>. So, for instance, if one of the images has both
classes, your <code class="docutils literal notranslate"><span class="pre">labels</span></code> tensor should look like <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2]</span></code>.</p>
<p>Additionally, if you want to use aspect ratio grouping during training
(so that each batch only contains images with similar aspect ratios),
then it is recommended to also implement a <code class="docutils literal notranslate"><span class="pre">get_height_and_width</span></code>
method, which returns the height and the width of the image. If this
method is not provided, we query all elements of the dataset via
<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> , which loads the image in memory and is slower than if
a custom method is provided.</p>
<section id="writing-a-custom-dataset-for-pennfudan">
<h3>Writing a custom dataset for PennFudan<a class="headerlink" href="#writing-a-custom-dataset-for-pennfudan" title="Link to this heading">#</a></h3>
<p>Let’s write a dataset for the PennFudan dataset. First, let’s download the dataset and
extract the <a class="reference external" href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip">zip file</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">cis</span><span class="o">.</span><span class="n">upenn</span><span class="o">.</span><span class="n">edu</span><span class="o">/~</span><span class="n">jshi</span><span class="o">/</span><span class="n">ped_html</span><span class="o">/</span><span class="n">PennFudanPed</span><span class="o">.</span><span class="n">zip</span> <span class="o">-</span><span class="n">P</span> <span class="n">data</span>
<span class="n">cd</span> <span class="n">data</span> <span class="o">&amp;&amp;</span> <span class="n">unzip</span> <span class="n">PennFudanPed</span><span class="o">.</span><span class="n">zip</span>
</pre></div>
</div>
<p>We have the following folder structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PennFudanPed</span><span class="o">/</span>
  <span class="n">PedMasks</span><span class="o">/</span>
    <span class="n">FudanPed00001_mask</span><span class="o">.</span><span class="n">png</span>
    <span class="n">FudanPed00002_mask</span><span class="o">.</span><span class="n">png</span>
    <span class="n">FudanPed00003_mask</span><span class="o">.</span><span class="n">png</span>
    <span class="n">FudanPed00004_mask</span><span class="o">.</span><span class="n">png</span>
    <span class="o">...</span>
  <span class="n">PNGImages</span><span class="o">/</span>
    <span class="n">FudanPed00001</span><span class="o">.</span><span class="n">png</span>
    <span class="n">FudanPed00002</span><span class="o">.</span><span class="n">png</span>
    <span class="n">FudanPed00003</span><span class="o">.</span><span class="n">png</span>
    <span class="n">FudanPed00004</span><span class="o">.</span><span class="n">png</span>
</pre></div>
</div>
<p>Here is one example of a pair of images and segmentation masks</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">read_image</span>


<span class="n">image</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;data/PennFudanPed/PNGImages/FudanPed00046.png&quot;</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;data/PennFudanPed/PedMasks/FudanPed00046_mask.png&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Image&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Mask&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_torchvision_tutorial_001.png" srcset="../_images/sphx_glr_torchvision_tutorial_001.png" alt="Image, Mask" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage object at 0x7f0d933dbb90&gt;
</pre></div>
</div>
<p>So each image has a corresponding
segmentation mask, where each color correspond to a different instance.
Let’s write a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code></a> class for this dataset.
In the code below, we are wrapping images, bounding boxes and masks into
<a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.TVTensor.html#torchvision.tv_tensors.TVTensor" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.TVTensor</span></code></a> classes so that we will be able to apply torchvision
built-in transformations (<a class="reference external" href="https://pytorch.org/vision/stable/transforms.html">new Transforms API</a>)
for the given object detection and segmentation task.
Namely, image tensors will be wrapped by <a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.Image</span></code></a>, bounding boxes into
<a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.BoundingBoxes.html#torchvision.tv_tensors.BoundingBoxes" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.BoundingBoxes</span></code></a> and masks into <a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.Mask</span></code></a>.
As <a class="reference external" href="https://docs.pytorch.org/vision/stable/generated/torchvision.tv_tensors.TVTensor.html#torchvision.tv_tensors.TVTensor" title="(Torchvision v0.23에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.tv_tensors.TVTensor</span></code></a> are <a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> subclasses, wrapped objects are also tensors and inherit the plain
<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> API. For more information about torchvision <code class="docutils literal notranslate"><span class="pre">tv_tensors</span></code> see
<a class="reference external" href="https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors">this documentation</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">read_image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.ops.boxes</span><span class="w"> </span><span class="kn">import</span> <span class="n">masks_to_boxes</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">tv_tensors</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.transforms.v2</span><span class="w"> </span><span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="k">class</span><span class="w"> </span><span class="nc">PennFudanDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">root</span><span class="p">,</span> <span class="n">transforms</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root</span> <span class="o">=</span> <span class="n">root</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transforms</span>
        <span class="c1"># load all image files, sorting them to</span>
        <span class="c1"># ensure that they are aligned</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">imgs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="s2">&quot;PNGImages&quot;</span><span class="p">))))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="s2">&quot;PedMasks&quot;</span><span class="p">))))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># load images and masks</span>
        <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="s2">&quot;PNGImages&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">imgs</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">mask_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="s2">&quot;PedMasks&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="n">mask_path</span><span class="p">)</span>
        <span class="c1"># instances are encoded as different colors</span>
        <span class="n">obj_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
        <span class="c1"># first id is the background, so remove it</span>
        <span class="n">obj_ids</span> <span class="o">=</span> <span class="n">obj_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">num_objs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">obj_ids</span><span class="p">)</span>

        <span class="c1"># split the color-encoded mask into a set</span>
        <span class="c1"># of binary masks</span>
        <span class="n">masks</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="n">obj_ids</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

        <span class="c1"># get bounding box coordinates for each mask</span>
        <span class="n">boxes</span> <span class="o">=</span> <span class="n">masks_to_boxes</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

        <span class="c1"># there is only one class</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_objs</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="n">image_id</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">area</span> <span class="o">=</span> <span class="p">(</span><span class="n">boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="c1"># suppose all instances are not crowd</span>
        <span class="n">iscrowd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_objs</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="c1"># Wrap sample and targets into torchvision tv_tensors:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">tv_tensors</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

        <span class="n">target</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">target</span><span class="p">[</span><span class="s2">&quot;boxes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tv_tensors</span><span class="o">.</span><span class="n">BoundingBoxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;XYXY&quot;</span><span class="p">,</span> <span class="n">canvas_size</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">get_size</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
        <span class="n">target</span><span class="p">[</span><span class="s2">&quot;masks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tv_tensors</span><span class="o">.</span><span class="n">Mask</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>
        <span class="n">target</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="n">target</span><span class="p">[</span><span class="s2">&quot;image_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_id</span>
        <span class="n">target</span><span class="p">[</span><span class="s2">&quot;area&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">area</span>
        <span class="n">target</span><span class="p">[</span><span class="s2">&quot;iscrowd&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iscrowd</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">target</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">imgs</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s all for the dataset. Now let’s define a model that can perform
predictions on this dataset.</p>
</section>
</section>
<section id="defining-your-model">
<h2>Defining your model<a class="headerlink" href="#defining-your-model" title="Link to this heading">#</a></h2>
<p>In this tutorial, we will be using <a class="reference external" href="https://arxiv.org/abs/1703.06870">Mask
R-CNN</a>, which is based on top of
<a class="reference external" href="https://arxiv.org/abs/1506.01497">Faster R-CNN</a>. Faster R-CNN is a
model that predicts both bounding boxes and class scores for potential
objects in the image.</p>
<img alt="../_static/img/tv_tutorial/tv_image03.png" src="../_static/img/tv_tutorial/tv_image03.png" />
<p>Mask R-CNN adds an extra branch
into Faster R-CNN, which also predicts segmentation masks for each
instance.</p>
<img alt="../_static/img/tv_tutorial/tv_image04.png" src="../_static/img/tv_tutorial/tv_image04.png" />
<p>There are two common
situations where one might want
to modify one of the available models in TorchVision Model Zoo. The first
is when we want to start from a pre-trained model, and just finetune the
last layer. The other is when we want to replace the backbone of the
model with a different one (for faster predictions, for example).</p>
<p>Let’s go see how we would do one or another in the following sections.</p>
<section id="finetuning-from-a-pretrained-model">
<h3>1 - Finetuning from a pretrained model<a class="headerlink" href="#finetuning-from-a-pretrained-model" title="Link to this heading">#</a></h3>
<p>Let’s suppose that you want to start from a model pre-trained on COCO
and want to finetune it for your particular classes. Here is a possible
way of doing it:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.detection.faster_rcnn</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastRCNNPredictor</span>

<span class="c1"># load a model pre-trained on COCO</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">detection</span><span class="o">.</span><span class="n">fasterrcnn_resnet50_fpn</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;DEFAULT&quot;</span><span class="p">)</span>

<span class="c1"># replace the classifier with a new one, that has</span>
<span class="c1"># num_classes which is user-defined</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 1 class (person) + background</span>
<span class="c1"># get number of input features for the classifier</span>
<span class="n">in_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">box_predictor</span><span class="o">.</span><span class="n">cls_score</span><span class="o">.</span><span class="n">in_features</span>
<span class="c1"># replace the pre-trained head with a new one</span>
<span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">box_predictor</span> <span class="o">=</span> <span class="n">FastRCNNPredictor</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading: &quot;https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth&quot; to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth

  0%|          | 0.00/160M [00:00&lt;?, ?B/s]
  0%|          | 128k/160M [00:00&lt;07:42, 362kB/s]
  0%|          | 256k/160M [00:00&lt;05:26, 512kB/s]
  0%|          | 384k/160M [00:00&lt;04:41, 593kB/s]
  0%|          | 768k/160M [00:00&lt;02:26, 1.14MB/s]
  1%|          | 1.62M/160M [00:01&lt;01:07, 2.44MB/s]
  2%|▏         | 3.12M/160M [00:01&lt;00:36, 4.46MB/s]
  4%|▍         | 6.25M/160M [00:01&lt;00:18, 8.73MB/s]
  6%|▋         | 10.1M/160M [00:01&lt;00:12, 12.9MB/s]
  9%|▉         | 14.0M/160M [00:01&lt;00:09, 15.6MB/s]
 11%|█         | 17.8M/160M [00:02&lt;00:08, 17.4MB/s]
 13%|█▎        | 20.4M/160M [00:02&lt;00:08, 16.8MB/s]
 15%|█▌        | 24.2M/160M [00:02&lt;00:07, 18.3MB/s]
 18%|█▊        | 28.1M/160M [00:02&lt;00:07, 19.5MB/s]
 20%|██        | 32.0M/160M [00:02&lt;00:06, 20.2MB/s]
 22%|██▏       | 35.9M/160M [00:02&lt;00:06, 20.8MB/s]
 25%|██▍       | 39.8M/160M [00:03&lt;00:05, 21.2MB/s]
 27%|██▋       | 43.6M/160M [00:03&lt;00:05, 21.5MB/s]
 30%|██▉       | 47.4M/160M [00:03&lt;00:05, 21.4MB/s]
 32%|███▏      | 51.2M/160M [00:03&lt;00:05, 21.6MB/s]
 35%|███▍      | 55.1M/160M [00:03&lt;00:05, 21.8MB/s]
 37%|███▋      | 59.1M/160M [00:04&lt;00:04, 22.0MB/s]
 39%|███▉      | 63.0M/160M [00:04&lt;00:04, 22.0MB/s]
 42%|████▏     | 66.9M/160M [00:04&lt;00:04, 22.0MB/s]
 44%|████▍     | 70.8M/160M [00:04&lt;00:04, 22.0MB/s]
 47%|████▋     | 74.5M/160M [00:04&lt;00:04, 21.9MB/s]
 49%|████▉     | 78.4M/160M [00:04&lt;00:03, 21.9MB/s]
 51%|█████▏    | 82.2M/160M [00:05&lt;00:03, 21.9MB/s]
 54%|█████▍    | 86.1M/160M [00:05&lt;00:03, 22.0MB/s]
 56%|█████▋    | 90.0M/160M [00:05&lt;00:03, 22.0MB/s]
 59%|█████▉    | 93.9M/160M [00:05&lt;00:03, 21.8MB/s]
 61%|██████    | 97.6M/160M [00:05&lt;00:02, 21.7MB/s]
 64%|██████▎   | 102M/160M [00:06&lt;00:02, 21.8MB/s]
 66%|██████▌   | 105M/160M [00:06&lt;00:02, 21.0MB/s]
 68%|██████▊   | 109M/160M [00:06&lt;00:02, 21.2MB/s]
 70%|███████   | 112M/160M [00:06&lt;00:02, 21.2MB/s]
 73%|███████▎  | 116M/160M [00:06&lt;00:02, 21.5MB/s]
 75%|███████▌  | 120M/160M [00:06&lt;00:01, 21.4MB/s]
 78%|███████▊  | 124M/160M [00:07&lt;00:01, 21.7MB/s]
 80%|████████  | 128M/160M [00:07&lt;00:01, 21.8MB/s]
 82%|████████▏ | 132M/160M [00:07&lt;00:01, 21.5MB/s]
 85%|████████▍ | 136M/160M [00:07&lt;00:01, 21.6MB/s]
 87%|████████▋ | 139M/160M [00:07&lt;00:01, 21.2MB/s]
 90%|████████▉ | 143M/160M [00:08&lt;00:00, 21.4MB/s]
 92%|█████████▏| 147M/160M [00:08&lt;00:00, 21.4MB/s]
 95%|█████████▍| 151M/160M [00:08&lt;00:00, 21.5MB/s]
 97%|█████████▋| 155M/160M [00:08&lt;00:00, 21.2MB/s]
 99%|█████████▊| 158M/160M [00:08&lt;00:00, 19.5MB/s]
100%|██████████| 160M/160M [00:08&lt;00:00, 18.8MB/s]
</pre></div>
</div>
</section>
<section id="modifying-the-model-to-add-a-different-backbone">
<h3>2 - Modifying the model to add a different backbone<a class="headerlink" href="#modifying-the-model-to-add-a-different-backbone" title="Link to this heading">#</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.detection</span><span class="w"> </span><span class="kn">import</span> <span class="n">FasterRCNN</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.detection.rpn</span><span class="w"> </span><span class="kn">import</span> <span class="n">AnchorGenerator</span>

<span class="c1"># load a pre-trained model for classification and return</span>
<span class="c1"># only the features</span>
<span class="n">backbone</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">mobilenet_v2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;DEFAULT&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">features</span>
<span class="c1"># ``FasterRCNN`` needs to know the number of</span>
<span class="c1"># output channels in a backbone. For mobilenet_v2, it&#39;s 1280</span>
<span class="c1"># so we need to add it here</span>
<span class="n">backbone</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="mi">1280</span>

<span class="c1"># let&#39;s make the RPN generate 5 x 3 anchors per spatial</span>
<span class="c1"># location, with 5 different sizes and 3 different aspect</span>
<span class="c1"># ratios. We have a Tuple[Tuple[int]] because each feature</span>
<span class="c1"># map could potentially have different sizes and</span>
<span class="c1"># aspect ratios</span>
<span class="n">anchor_generator</span> <span class="o">=</span> <span class="n">AnchorGenerator</span><span class="p">(</span>
    <span class="n">sizes</span><span class="o">=</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),),</span>
    <span class="n">aspect_ratios</span><span class="o">=</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),)</span>
<span class="p">)</span>

<span class="c1"># let&#39;s define what are the feature maps that we will</span>
<span class="c1"># use to perform the region of interest cropping, as well as</span>
<span class="c1"># the size of the crop after rescaling.</span>
<span class="c1"># if your backbone returns a Tensor, featmap_names is expected to</span>
<span class="c1"># be [0]. More generally, the backbone should return an</span>
<span class="c1"># ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which</span>
<span class="c1"># feature maps to use.</span>
<span class="n">roi_pooler</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">MultiScaleRoIAlign</span><span class="p">(</span>
    <span class="n">featmap_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">],</span>
    <span class="n">output_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">sampling_ratio</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="c1"># put the pieces together inside a Faster-RCNN model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FasterRCNN</span><span class="p">(</span>
    <span class="n">backbone</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">rpn_anchor_generator</span><span class="o">=</span><span class="n">anchor_generator</span><span class="p">,</span>
    <span class="n">box_roi_pool</span><span class="o">=</span><span class="n">roi_pooler</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading: &quot;https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth&quot; to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth

  0%|          | 0.00/13.6M [00:00&lt;?, ?B/s]
 52%|█████▏    | 7.12M/13.6M [00:00&lt;00:00, 67.1MB/s]
100%|██████████| 13.6M/13.6M [00:00&lt;00:00, 79.2MB/s]
</pre></div>
</div>
</section>
<section id="object-detection-and-instance-segmentation-model-for-pennfudan-dataset">
<h3>Object detection and instance segmentation model for PennFudan Dataset<a class="headerlink" href="#object-detection-and-instance-segmentation-model-for-pennfudan-dataset" title="Link to this heading">#</a></h3>
<p>In our case, we want to finetune from a pre-trained model, given that
our dataset is very small, so we will be following approach number 1.</p>
<p>Here we want to also compute the instance segmentation masks, so we will
be using Mask R-CNN:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.detection.faster_rcnn</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastRCNNPredictor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models.detection.mask_rcnn</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaskRCNNPredictor</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_model_instance_segmentation</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
    <span class="c1"># load an instance segmentation model pre-trained on COCO</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">detection</span><span class="o">.</span><span class="n">maskrcnn_resnet50_fpn</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;DEFAULT&quot;</span><span class="p">)</span>

    <span class="c1"># get number of input features for the classifier</span>
    <span class="n">in_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">box_predictor</span><span class="o">.</span><span class="n">cls_score</span><span class="o">.</span><span class="n">in_features</span>
    <span class="c1"># replace the pre-trained head with a new one</span>
    <span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">box_predictor</span> <span class="o">=</span> <span class="n">FastRCNNPredictor</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="c1"># now get the number of input features for the mask classifier</span>
    <span class="n">in_features_mask</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">mask_predictor</span><span class="o">.</span><span class="n">conv5_mask</span><span class="o">.</span><span class="n">in_channels</span>
    <span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="c1"># and replace the mask predictor with a new one</span>
    <span class="n">model</span><span class="o">.</span><span class="n">roi_heads</span><span class="o">.</span><span class="n">mask_predictor</span> <span class="o">=</span> <span class="n">MaskRCNNPredictor</span><span class="p">(</span>
        <span class="n">in_features_mask</span><span class="p">,</span>
        <span class="n">hidden_layer</span><span class="p">,</span>
        <span class="n">num_classes</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>That’s it, this will make <code class="docutils literal notranslate"><span class="pre">model</span></code> be ready to be trained and evaluated
on your custom dataset.</p>
</section>
</section>
<section id="putting-everything-together">
<h2>Putting everything together<a class="headerlink" href="#putting-everything-together" title="Link to this heading">#</a></h2>
<p>In <code class="docutils literal notranslate"><span class="pre">references/detection/</span></code>, we have a number of helper functions to
simplify training and evaluating detection models. Here, we will use
<code class="docutils literal notranslate"><span class="pre">references/detection/engine.py</span></code> and <code class="docutils literal notranslate"><span class="pre">references/detection/utils.py</span></code>.
Just download everything under <code class="docutils literal notranslate"><span class="pre">references/detection</span></code> to your folder and use them here.
On Linux if you have <code class="docutils literal notranslate"><span class="pre">wget</span></code>, you can download them using below commands:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0
</pre></div>
</div>
<p>Since v0.15.0 torchvision provides <a class="reference external" href="https://pytorch.org/vision/stable/transforms.html">new Transforms API</a>
to easily write data augmentation pipelines for Object Detection and Segmentation tasks.</p>
<p>Let’s write some helper functions for data augmentation /
transformation:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">v2</span> <span class="k">as</span> <span class="n">T</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_transform</span><span class="p">(</span><span class="n">train</span><span class="p">):</span>
    <span class="n">transforms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ToDtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ToPureTensor</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="n">transforms</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="testing-forward-method-optional">
<h2>Testing <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method (Optional)<a class="headerlink" href="#testing-forward-method-optional" title="Link to this heading">#</a></h2>
<p>Before iterating over the dataset, it’s good to see what the model
expects during training and inference time on sample data.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">utils</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">detection</span><span class="o">.</span><span class="n">fasterrcnn_resnet50_fpn</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;DEFAULT&quot;</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">PennFudanDataset</span><span class="p">(</span><span class="s1">&#39;data/PennFudanPed&#39;</span><span class="p">,</span> <span class="n">get_transform</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">collate_fn</span>
<span class="p">)</span>

<span class="c1"># For Training</span>
<span class="n">images</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
<span class="n">images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">image</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">]</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>  <span class="c1"># Returns losses and detections</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># For inference</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">400</span><span class="p">)]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Returns predictions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;loss_classifier&#39;: tensor(0.0503, grad_fn=&lt;NllLossBackward0&gt;), &#39;loss_box_reg&#39;: tensor(0.0387, grad_fn=&lt;DivBackward0&gt;), &#39;loss_objectness&#39;: tensor(0.0080, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward0&gt;), &#39;loss_rpn_box_reg&#39;: tensor(0.0018, grad_fn=&lt;DivBackward0&gt;)}
{&#39;boxes&#39;: tensor([], size=(0, 4), grad_fn=&lt;StackBackward0&gt;), &#39;labels&#39;: tensor([], dtype=torch.int64), &#39;scores&#39;: tensor([], grad_fn=&lt;IndexBackward0&gt;)}
</pre></div>
</div>
<p>We want to be able to train our model on an <a class="reference external" href="https://pytorch.org/docs/stable/torch.html#accelerators">accelerator</a>
such as CUDA, MPS, MTIA, or XPU. Let’s now write the main function which performs the training and the validation:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_one_epoch</span><span class="p">,</span> <span class="n">evaluate</span>

<span class="c1"># train on the accelerator or on the CPU, if an accelerator is not available</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">current_accelerator</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="c1"># our dataset has two classes only - background and person</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># use our dataset and defined transformations</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">PennFudanDataset</span><span class="p">(</span><span class="s1">&#39;data/PennFudanPed&#39;</span><span class="p">,</span> <span class="n">get_transform</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">dataset_test</span> <span class="o">=</span> <span class="n">PennFudanDataset</span><span class="p">(</span><span class="s1">&#39;data/PennFudanPed&#39;</span><span class="p">,</span> <span class="n">get_transform</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="c1"># split the dataset in train and test set</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Subset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">50</span><span class="p">])</span>
<span class="n">dataset_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Subset</span><span class="p">(</span><span class="n">dataset_test</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="o">-</span><span class="mi">50</span><span class="p">:])</span>

<span class="c1"># define training and validation data loaders</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">collate_fn</span>
<span class="p">)</span>

<span class="n">data_loader_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset_test</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">collate_fn</span>
<span class="p">)</span>

<span class="c1"># get the model using our helper function</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_model_instance_segmentation</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># move model to the right device</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># construct an optimizer</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0005</span>
<span class="p">)</span>

<span class="c1"># and a learning rate scheduler</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">step_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="c1"># let&#39;s train it just for 2 epochs</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># train for one epoch, printing every 10 iterations</span>
    <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">print_freq</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="c1"># update the learning rate</span>
    <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># evaluate on the test dataset</span>
    <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader_test</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;That&#39;s it!&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading: &quot;https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth&quot; to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth

  0%|          | 0.00/170M [00:00&lt;?, ?B/s]
  0%|          | 128k/170M [00:00&lt;07:48, 380kB/s]
  0%|          | 256k/170M [00:00&lt;05:30, 539kB/s]
  0%|          | 384k/170M [00:00&lt;04:44, 624kB/s]
  0%|          | 768k/170M [00:00&lt;02:27, 1.20MB/s]
  1%|          | 1.62M/170M [00:01&lt;01:08, 2.56MB/s]
  2%|▏         | 3.12M/170M [00:01&lt;00:37, 4.67MB/s]
  4%|▎         | 6.25M/170M [00:01&lt;00:18, 9.19MB/s]
  6%|▌         | 10.0M/170M [00:01&lt;00:12, 13.3MB/s]
  8%|▊         | 13.8M/170M [00:01&lt;00:10, 16.1MB/s]
 10%|█         | 17.5M/170M [00:01&lt;00:08, 18.1MB/s]
 13%|█▎        | 21.4M/170M [00:02&lt;00:07, 19.5MB/s]
 15%|█▍        | 25.1M/170M [00:02&lt;00:07, 20.2MB/s]
 17%|█▋        | 28.5M/170M [00:02&lt;00:07, 20.2MB/s]
 19%|█▉        | 32.4M/170M [00:02&lt;00:06, 21.2MB/s]
 21%|██▏       | 36.1M/170M [00:02&lt;00:06, 21.6MB/s]
 24%|██▎       | 40.0M/170M [00:02&lt;00:06, 22.1MB/s]
 26%|██▌       | 43.9M/170M [00:03&lt;00:05, 22.3MB/s]
 28%|██▊       | 46.8M/170M [00:03&lt;00:06, 20.8MB/s]
 30%|██▉       | 50.6M/170M [00:03&lt;00:05, 21.5MB/s]
 32%|███▏      | 54.4M/170M [00:03&lt;00:05, 21.7MB/s]
 34%|███▍      | 58.1M/170M [00:03&lt;00:05, 21.9MB/s]
 36%|███▋      | 61.9M/170M [00:04&lt;00:05, 22.1MB/s]
 39%|███▊      | 65.6M/170M [00:04&lt;00:04, 22.2MB/s]
 41%|████      | 69.5M/170M [00:04&lt;00:04, 22.5MB/s]
 43%|████▎     | 73.4M/170M [00:04&lt;00:04, 22.7MB/s]
 45%|████▌     | 77.2M/170M [00:04&lt;00:04, 22.8MB/s]
 48%|████▊     | 81.0M/170M [00:04&lt;00:04, 22.7MB/s]
 50%|████▉     | 84.8M/170M [00:05&lt;00:03, 22.7MB/s]
 52%|█████▏    | 88.6M/170M [00:05&lt;00:03, 22.7MB/s]
 54%|█████▍    | 92.5M/170M [00:05&lt;00:03, 22.9MB/s]
 57%|█████▋    | 96.4M/170M [00:05&lt;00:03, 23.0MB/s]
 59%|█████▉    | 100M/170M [00:05&lt;00:03, 23.0MB/s]
 61%|██████▏   | 104M/170M [00:05&lt;00:02, 23.2MB/s]
 64%|██████▎   | 108M/170M [00:06&lt;00:02, 22.9MB/s]
 66%|██████▌   | 112M/170M [00:06&lt;00:02, 22.5MB/s]
 68%|██████▊   | 116M/170M [00:06&lt;00:02, 22.8MB/s]
 70%|███████   | 119M/170M [00:06&lt;00:02, 22.8MB/s]
 72%|███████▏  | 123M/170M [00:06&lt;00:02, 22.7MB/s]
 75%|███████▍  | 127M/170M [00:07&lt;00:01, 22.8MB/s]
 77%|███████▋  | 131M/170M [00:07&lt;00:01, 22.9MB/s]
 79%|███████▉  | 135M/170M [00:07&lt;00:01, 22.9MB/s]
 82%|████████▏ | 138M/170M [00:07&lt;00:01, 22.8MB/s]
 84%|████████▍ | 142M/170M [00:07&lt;00:01, 22.9MB/s]
 86%|████████▌ | 146M/170M [00:07&lt;00:01, 23.0MB/s]
 88%|████████▊ | 150M/170M [00:08&lt;00:00, 22.8MB/s]
 91%|█████████ | 154M/170M [00:08&lt;00:00, 22.8MB/s]
 93%|█████████▎| 158M/170M [00:08&lt;00:00, 22.7MB/s]
 95%|█████████▌| 161M/170M [00:08&lt;00:00, 22.8MB/s]
 97%|█████████▋| 165M/170M [00:08&lt;00:00, 22.7MB/s]
100%|█████████▉| 169M/170M [00:08&lt;00:00, 22.9MB/s]
100%|██████████| 170M/170M [00:08&lt;00:00, 19.9MB/s]
/workspace/tutorials-kr/intermediate_source/engine.py:30: FutureWarning:

`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast(&#39;cuda&#39;, args...)` instead.

Epoch: [0]  [ 0/60]  eta: 0:01:34  lr: 0.000090  loss: 2.9703 (2.9703)  loss_classifier: 0.3432 (0.3432)  loss_box_reg: 0.1413 (0.1413)  loss_mask: 2.4372 (2.4372)  loss_objectness: 0.0474 (0.0474)  loss_rpn_box_reg: 0.0012 (0.0012)  time: 1.5690  data: 0.0186  max mem: 1923
Epoch: [0]  [10/60]  eta: 0:00:16  lr: 0.000936  loss: 1.5786 (1.8444)  loss_classifier: 0.2983 (0.2945)  loss_box_reg: 0.2995 (0.2872)  loss_mask: 0.8385 (1.2297)  loss_objectness: 0.0300 (0.0288)  loss_rpn_box_reg: 0.0038 (0.0042)  time: 0.3270  data: 0.0194  max mem: 2590
Epoch: [0]  [20/60]  eta: 0:00:09  lr: 0.001783  loss: 0.8604 (1.2920)  loss_classifier: 0.1766 (0.2155)  loss_box_reg: 0.2502 (0.2794)  loss_mask: 0.4121 (0.7682)  loss_objectness: 0.0239 (0.0237)  loss_rpn_box_reg: 0.0044 (0.0051)  time: 0.1650  data: 0.0198  max mem: 2590
Epoch: [0]  [30/60]  eta: 0:00:05  lr: 0.002629  loss: 0.5386 (1.0609)  loss_classifier: 0.0845 (0.1710)  loss_box_reg: 0.2282 (0.2734)  loss_mask: 0.2187 (0.5925)  loss_objectness: 0.0129 (0.0192)  loss_rpn_box_reg: 0.0040 (0.0048)  time: 0.1235  data: 0.0199  max mem: 2590
Epoch: [0]  [40/60]  eta: 0:00:03  lr: 0.003476  loss: 0.4560 (0.9019)  loss_classifier: 0.0506 (0.1408)  loss_box_reg: 0.1597 (0.2458)  loss_mask: 0.2087 (0.4952)  loss_objectness: 0.0015 (0.0149)  loss_rpn_box_reg: 0.0043 (0.0052)  time: 0.1091  data: 0.0187  max mem: 2590
Epoch: [0]  [50/60]  eta: 0:00:01  lr: 0.004323  loss: 0.3754 (0.8045)  loss_classifier: 0.0468 (0.1240)  loss_box_reg: 0.1402 (0.2293)  loss_mask: 0.1805 (0.4328)  loss_objectness: 0.0015 (0.0126)  loss_rpn_box_reg: 0.0062 (0.0058)  time: 0.1009  data: 0.0190  max mem: 2634
Epoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.3970 (0.7452)  loss_classifier: 0.0479 (0.1133)  loss_box_reg: 0.1487 (0.2184)  loss_mask: 0.1736 (0.3959)  loss_objectness: 0.0017 (0.0111)  loss_rpn_box_reg: 0.0078 (0.0065)  time: 0.0974  data: 0.0203  max mem: 2638
Epoch: [0] Total time: 0:00:08 (0.1489 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:04  model_time: 0.0849 (0.0849)  evaluator_time: 0.0016 (0.0016)  time: 0.0932  data: 0.0064  max mem: 2638
Test:  [49/50]  eta: 0:00:00  model_time: 0.0185 (0.0464)  evaluator_time: 0.0040 (0.0050)  time: 0.0491  data: 0.0111  max mem: 2638
Test: Total time: 0:00:03 (0.0624 s / it)
Averaged stats: model_time: 0.0185 (0.0464)  evaluator_time: 0.0040 (0.0050)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.656
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.980
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.855
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.445
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.494
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.674
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.716
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.716
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.580
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.657
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.726
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.726
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.980
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.924
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.532
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.748
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.329
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.764
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.580
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776
Epoch: [1]  [ 0/60]  eta: 0:00:04  lr: 0.005000  loss: 0.3154 (0.3154)  loss_classifier: 0.0460 (0.0460)  loss_box_reg: 0.1283 (0.1283)  loss_mask: 0.1313 (0.1313)  loss_objectness: 0.0016 (0.0016)  loss_rpn_box_reg: 0.0083 (0.0083)  time: 0.0792  data: 0.0213  max mem: 2638
Epoch: [1]  [10/60]  eta: 0:00:03  lr: 0.005000  loss: 0.2622 (0.2920)  loss_classifier: 0.0339 (0.0385)  loss_box_reg: 0.0836 (0.0961)  loss_mask: 0.1357 (0.1468)  loss_objectness: 0.0016 (0.0026)  loss_rpn_box_reg: 0.0077 (0.0079)  time: 0.0783  data: 0.0167  max mem: 2638
Epoch: [1]  [20/60]  eta: 0:00:03  lr: 0.005000  loss: 0.2753 (0.3052)  loss_classifier: 0.0339 (0.0390)  loss_box_reg: 0.0836 (0.0997)  loss_mask: 0.1450 (0.1564)  loss_objectness: 0.0017 (0.0025)  loss_rpn_box_reg: 0.0060 (0.0076)  time: 0.0803  data: 0.0174  max mem: 2645
Epoch: [1]  [30/60]  eta: 0:00:02  lr: 0.005000  loss: 0.2605 (0.2878)  loss_classifier: 0.0387 (0.0370)  loss_box_reg: 0.0674 (0.0902)  loss_mask: 0.1391 (0.1520)  loss_objectness: 0.0010 (0.0021)  loss_rpn_box_reg: 0.0045 (0.0065)  time: 0.0880  data: 0.0179  max mem: 2645
Epoch: [1]  [40/60]  eta: 0:00:01  lr: 0.005000  loss: 0.2437 (0.2865)  loss_classifier: 0.0378 (0.0380)  loss_box_reg: 0.0732 (0.0883)  loss_mask: 0.1324 (0.1519)  loss_objectness: 0.0008 (0.0019)  loss_rpn_box_reg: 0.0045 (0.0064)  time: 0.0933  data: 0.0180  max mem: 2877
Epoch: [1]  [50/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2518 (0.2789)  loss_classifier: 0.0349 (0.0369)  loss_box_reg: 0.0737 (0.0857)  loss_mask: 0.1295 (0.1484)  loss_objectness: 0.0006 (0.0018)  loss_rpn_box_reg: 0.0045 (0.0060)  time: 0.0978  data: 0.0192  max mem: 2881
Epoch: [1]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2416 (0.2792)  loss_classifier: 0.0333 (0.0366)  loss_box_reg: 0.0716 (0.0852)  loss_mask: 0.1318 (0.1499)  loss_objectness: 0.0005 (0.0017)  loss_rpn_box_reg: 0.0035 (0.0058)  time: 0.0933  data: 0.0206  max mem: 2881
Epoch: [1] Total time: 0:00:05 (0.0893 s / it)
creating index...
index created!
Test:  [ 0/50]  eta: 0:00:01  model_time: 0.0142 (0.0142)  evaluator_time: 0.0013 (0.0013)  time: 0.0221  data: 0.0064  max mem: 2881
Test:  [49/50]  eta: 0:00:00  model_time: 0.0152 (0.0153)  evaluator_time: 0.0030 (0.0031)  time: 0.0304  data: 0.0113  max mem: 2881
Test: Total time: 0:00:01 (0.0296 s / it)
Averaged stats: model_time: 0.0152 (0.0153)  evaluator_time: 0.0030 (0.0031)
Accumulating evaluation results...
DONE (t=0.01s).
Accumulating evaluation results...
DONE (t=0.01s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.766
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.976
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.946
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.438
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.359
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.520
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.786
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.823
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.758
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.976
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.941
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.506
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.787
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.793
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.793
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.480
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812
That&#39;s it!
</pre></div>
</div>
<p>So after one epoch of training, we obtain a COCO-style mAP &gt; 50, and
a mask mAP of 65.</p>
<p>But what do the predictions look like? Let’s take one image in the
dataset and verify</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">draw_bounding_boxes</span><span class="p">,</span> <span class="n">draw_segmentation_masks</span>


<span class="n">image</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="s2">&quot;data/PennFudanPed/PNGImages/FudanPed00046.png&quot;</span><span class="p">)</span>
<span class="n">eval_transform</span> <span class="o">=</span> <span class="n">get_transform</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">eval_transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="c1"># convert RGBA -&gt; RGB and move to device</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="p">])</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="n">image</span> <span class="o">=</span> <span class="p">(</span><span class="mf">255.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">image</span> <span class="o">-</span> <span class="n">image</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">image</span><span class="o">.</span><span class="n">min</span><span class="p">()))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="n">pred_labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pedestrian: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">],</span> <span class="n">pred</span><span class="p">[</span><span class="s2">&quot;scores&quot;</span><span class="p">])]</span>
<span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="s2">&quot;boxes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="n">output_image</span> <span class="o">=</span> <span class="n">draw_bounding_boxes</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">pred_boxes</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>

<span class="n">masks</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="s2">&quot;masks&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output_image</span> <span class="o">=</span> <span class="n">draw_segmentation_masks</span><span class="p">(</span><span class="n">output_image</span><span class="p">,</span> <span class="n">masks</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">output_image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_torchvision_tutorial_002.png" srcset="../_images/sphx_glr_torchvision_tutorial_002.png" alt="torchvision tutorial" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage object at 0x7f0daa4d6910&gt;
</pre></div>
</div>
<p>The results look good!</p>
</section>
<section id="wrapping-up">
<h2>Wrapping up<a class="headerlink" href="#wrapping-up" title="Link to this heading">#</a></h2>
<p>In this tutorial, you have learned how to create your own training
pipeline for object detection models on a custom dataset. For
that, you wrote a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" title="(PyTorch v2.8에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code></a> class that returns the
images and the ground truth boxes and segmentation masks. You also
leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to
perform transfer learning on this new dataset.</p>
<p>For a more complete example, which includes multi-machine / multi-GPU
training, check <code class="docutils literal notranslate"><span class="pre">references/detection/train.py</span></code>, which is present in
the torchvision repository.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 52.968 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-torchvision-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4a542c9f39bedbfe7de5061767181d36/torchvision_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">torchvision_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7590258df9f28b5ae0994c3b5b035edf/torchvision_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">torchvision_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/aa82b146a324295d4af880b71bfd78cd/torchvision_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">torchvision_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="../domains.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Domains</p>
      </div>
    </a>
    <a class="right-next"
       href="../beginner/transfer_learning_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../domains.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Domains</p>
      </div>
    </a>
    <a class="right-next"
       href="../beginner/transfer_learning_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-the-dataset">Defining the Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#writing-a-custom-dataset-for-pennfudan">Writing a custom dataset for PennFudan</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-your-model">Defining your model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finetuning-from-a-pretrained-model">1 - Finetuning from a pretrained model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modifying-the-model-to-add-a-different-backbone">2 - Modifying the model to add a different backbone</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection-and-instance-segmentation-model-for-pennfudan-dataset">Object detection and instance segmentation model for PennFudan Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-everything-together">Putting everything together</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-forward-method-optional">Testing <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method (Optional)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapping-up">Wrapping up</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "TorchVision Object Detection Finetuning Tutorial",
       "headline": "TorchVision Object Detection Finetuning Tutorial",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/torchvision_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. TorchVision Object Detection Finetuning Tutorial# For this tutorial, we will be finetuning a pre-trained Mask R-CNN model on the Penn-Fudan Database for Pedestrian Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an object detection and instance segmentation model on a custom dataset. \ucc38\uace0 This tutorial works only with torchvision version \u003e=0.16 or nightly. If you\u2019re using torchvision\u003c=0.15, please follow this tutorial instead. Defining the Dataset# The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, and implement __len__ and __getitem__. The only specificity that we require is that the dataset __getitem__ should return a tuple: image: torchvision.tv_tensors.Image of shape [3, H, W], a pure tensor, or a PIL Image of size (H, W) target: a dict containing the following fields boxes, torchvision.tv_tensors.BoundingBoxes of shape [N, 4]: the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H labels, integer torch.Tensor of shape [N]: the label for each bounding box. 0 represents always the background class. image_id, int: an image identifier. It should be unique between all the images in the dataset, and is used during evaluation area, float torch.Tensor of shape [N]: the area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes. iscrowd, uint8 torch.Tensor of shape [N]: instances with iscrowd=True will be ignored during evaluation. (optionally) masks, torchvision.tv_tensors.Mask of shape [N, H, W]: the segmentation masks for each one of the objects If your dataset is compliant with above requirements then it will work for both training and evaluation codes from the reference script. Evaluation code will use scripts from pycocotools which can be installed with pip install pycocotools. \ucc38\uace0 For Windows, please install pycocotools from gautamchitnis with command pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI One note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels. For example, assuming you have just two classes, cat and dog, you can define 1 (not 0) to represent cats and 2 to represent dogs. So, for instance, if one of the images has both classes, your labels tensor should look like [1, 2]. Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratios), then it is recommended to also implement a get_height_and_width method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via __getitem__ , which loads the image in memory and is slower than if a custom method is provided. Writing a custom dataset for PennFudan# Let\u2019s write a dataset for the PennFudan dataset. First, let\u2019s download the dataset and extract the zip file: wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data cd data \u0026\u0026 unzip PennFudanPed.zip We have the following folder structure: PennFudanPed/ PedMasks/ FudanPed00001_mask.png FudanPed00002_mask.png FudanPed00003_mask.png FudanPed00004_mask.png ... PNGImages/ FudanPed00001.png FudanPed00002.png FudanPed00003.png FudanPed00004.png Here is one example of a pair of images and segmentation masks import matplotlib.pyplot as plt from torchvision.io import read_image image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\") mask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\") plt.figure(figsize=(16, 8)) plt.subplot(121) plt.title(\"Image\") plt.imshow(image.permute(1, 2, 0)) plt.subplot(122) plt.title(\"Mask\") plt.imshow(mask.permute(1, 2, 0)) \u003cmatplotlib.image.AxesImage object at 0x7f0d933dbb90\u003e So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let\u2019s write a torch.utils.data.Dataset class for this dataset. In the code below, we are wrapping images, bounding boxes and masks into torchvision.tv_tensors.TVTensor classes so that we will be able to apply torchvision built-in transformations (new Transforms API) for the given object detection and segmentation task. Namely, image tensors will be wrapped by torchvision.tv_tensors.Image, bounding boxes into torchvision.tv_tensors.BoundingBoxes and masks into torchvision.tv_tensors.Mask. As torchvision.tv_tensors.TVTensor are torch.Tensor subclasses, wrapped objects are also tensors and inherit the plain torch.Tensor API. For more information about torchvision tv_tensors see this documentation. import os import torch from torchvision.io import read_image from torchvision.ops.boxes import masks_to_boxes from torchvision import tv_tensors from torchvision.transforms.v2 import functional as F class PennFudanDataset(torch.utils.data.Dataset): def __init__(self, root, transforms): self.root = root self.transforms = transforms # load all image files, sorting them to # ensure that they are aligned self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\")))) self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\")))) def __getitem__(self, idx): # load images and masks img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx]) mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx]) img = read_image(img_path) mask = read_image(mask_path) # instances are encoded as different colors obj_ids = torch.unique(mask) # first id is the background, so remove it obj_ids = obj_ids[1:] num_objs = len(obj_ids) # split the color-encoded mask into a set # of binary masks masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8) # get bounding box coordinates for each mask boxes = masks_to_boxes(masks) # there is only one class labels = torch.ones((num_objs,), dtype=torch.int64) image_id = idx area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # suppose all instances are not crowd iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # Wrap sample and targets into torchvision tv_tensors: img = tv_tensors.Image(img) target = {} target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img)) target[\"masks\"] = tv_tensors.Mask(masks) target[\"labels\"] = labels target[\"image_id\"] = image_id target[\"area\"] = area target[\"iscrowd\"] = iscrowd if self.transforms is not None: img, target = self.transforms(img, target) return img, target def __len__(self): return len(self.imgs) That\u2019s all for the dataset. Now let\u2019s define a model that can perform predictions on this dataset. Defining your model# In this tutorial, we will be using Mask R-CNN, which is based on top of Faster R-CNN. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image. Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance. There are two common situations where one might want to modify one of the available models in TorchVision Model Zoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example). Let\u2019s go see how we would do one or another in the following sections. 1 - Finetuning from a pretrained model# Let\u2019s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it: import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # load a model pre-trained on COCO model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\") # replace the classifier with a new one, that has # num_classes which is user-defined num_classes = 2 # 1 class (person) + background # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth 0%| | 0.00/160M [00:00\u003c?, ?B/s] 0%| | 128k/160M [00:00\u003c07:42, 362kB/s] 0%| | 256k/160M [00:00\u003c05:26, 512kB/s] 0%| | 384k/160M [00:00\u003c04:41, 593kB/s] 0%| | 768k/160M [00:00\u003c02:26, 1.14MB/s] 1%| | 1.62M/160M [00:01\u003c01:07, 2.44MB/s] 2%|\u258f | 3.12M/160M [00:01\u003c00:36, 4.46MB/s] 4%|\u258d | 6.25M/160M [00:01\u003c00:18, 8.73MB/s] 6%|\u258b | 10.1M/160M [00:01\u003c00:12, 12.9MB/s] 9%|\u2589 | 14.0M/160M [00:01\u003c00:09, 15.6MB/s] 11%|\u2588 | 17.8M/160M [00:02\u003c00:08, 17.4MB/s] 13%|\u2588\u258e | 20.4M/160M [00:02\u003c00:08, 16.8MB/s] 15%|\u2588\u258c | 24.2M/160M [00:02\u003c00:07, 18.3MB/s] 18%|\u2588\u258a | 28.1M/160M [00:02\u003c00:07, 19.5MB/s] 20%|\u2588\u2588 | 32.0M/160M [00:02\u003c00:06, 20.2MB/s] 22%|\u2588\u2588\u258f | 35.9M/160M [00:02\u003c00:06, 20.8MB/s] 25%|\u2588\u2588\u258d | 39.8M/160M [00:03\u003c00:05, 21.2MB/s] 27%|\u2588\u2588\u258b | 43.6M/160M [00:03\u003c00:05, 21.5MB/s] 30%|\u2588\u2588\u2589 | 47.4M/160M [00:03\u003c00:05, 21.4MB/s] 32%|\u2588\u2588\u2588\u258f | 51.2M/160M [00:03\u003c00:05, 21.6MB/s] 35%|\u2588\u2588\u2588\u258d | 55.1M/160M [00:03\u003c00:05, 21.8MB/s] 37%|\u2588\u2588\u2588\u258b | 59.1M/160M [00:04\u003c00:04, 22.0MB/s] 39%|\u2588\u2588\u2588\u2589 | 63.0M/160M [00:04\u003c00:04, 22.0MB/s] 42%|\u2588\u2588\u2588\u2588\u258f | 66.9M/160M [00:04\u003c00:04, 22.0MB/s] 44%|\u2588\u2588\u2588\u2588\u258d | 70.8M/160M [00:04\u003c00:04, 22.0MB/s] 47%|\u2588\u2588\u2588\u2588\u258b | 74.5M/160M [00:04\u003c00:04, 21.9MB/s] 49%|\u2588\u2588\u2588\u2588\u2589 | 78.4M/160M [00:04\u003c00:03, 21.9MB/s] 51%|\u2588\u2588\u2588\u2588\u2588\u258f | 82.2M/160M [00:05\u003c00:03, 21.9MB/s] 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 86.1M/160M [00:05\u003c00:03, 22.0MB/s] 56%|\u2588\u2588\u2588\u2588\u2588\u258b | 90.0M/160M [00:05\u003c00:03, 22.0MB/s] 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 93.9M/160M [00:05\u003c00:03, 21.8MB/s] 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 97.6M/160M [00:05\u003c00:02, 21.7MB/s] 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 102M/160M [00:06\u003c00:02, 21.8MB/s] 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 105M/160M [00:06\u003c00:02, 21.0MB/s] 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 109M/160M [00:06\u003c00:02, 21.2MB/s] 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 112M/160M [00:06\u003c00:02, 21.2MB/s] 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 116M/160M [00:06\u003c00:02, 21.5MB/s] 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 120M/160M [00:06\u003c00:01, 21.4MB/s] 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 124M/160M [00:07\u003c00:01, 21.7MB/s] 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 128M/160M [00:07\u003c00:01, 21.8MB/s] 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 132M/160M [00:07\u003c00:01, 21.5MB/s] 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 136M/160M [00:07\u003c00:01, 21.6MB/s] 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 139M/160M [00:07\u003c00:01, 21.2MB/s] 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 143M/160M [00:08\u003c00:00, 21.4MB/s] 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 147M/160M [00:08\u003c00:00, 21.4MB/s] 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 151M/160M [00:08\u003c00:00, 21.5MB/s] 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 155M/160M [00:08\u003c00:00, 21.2MB/s] 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 158M/160M [00:08\u003c00:00, 19.5MB/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 160M/160M [00:08\u003c00:00, 18.8MB/s] 2 - Modifying the model to add a different backbone# import torchvision from torchvision.models.detection import FasterRCNN from torchvision.models.detection.rpn import AnchorGenerator # load a pre-trained model for classification and return # only the features backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features # ``FasterRCNN`` needs to know the number of # output channels in a backbone. For mobilenet_v2, it\u0027s 1280 # so we need to add it here backbone.out_channels = 1280 # let\u0027s make the RPN generate 5 x 3 anchors per spatial # location, with 5 different sizes and 3 different aspect # ratios. We have a Tuple[Tuple[int]] because each feature # map could potentially have different sizes and # aspect ratios anchor_generator = AnchorGenerator( sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),) ) # let\u0027s define what are the feature maps that we will # use to perform the region of interest cropping, as well as # the size of the crop after rescaling. # if your backbone returns a Tensor, featmap_names is expected to # be [0]. More generally, the backbone should return an # ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which # feature maps to use. roi_pooler = torchvision.ops.MultiScaleRoIAlign( featmap_names=[\u00270\u0027], output_size=7, sampling_ratio=2 ) # put the pieces together inside a Faster-RCNN model model = FasterRCNN( backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler ) Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth 0%| | 0.00/13.6M [00:00\u003c?, ?B/s] 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 7.12M/13.6M [00:00\u003c00:00, 67.1MB/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.6M/13.6M [00:00\u003c00:00, 79.2MB/s] Object detection and instance segmentation model for PennFudan Dataset# In our case, we want to finetune from a pre-trained model, given that our dataset is very small, so we will be following approach number 1. Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN: import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor def get_model_instance_segmentation(num_classes): # load an instance segmentation model pre-trained on COCO model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\") # get number of input features for the classifier in_features = model.roi_heads.box_predictor.cls_score.in_features # replace the pre-trained head with a new one model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # now get the number of input features for the mask classifier in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels hidden_layer = 256 # and replace the mask predictor with a new one model.roi_heads.mask_predictor = MaskRCNNPredictor( in_features_mask, hidden_layer, num_classes ) return model That\u2019s it, this will make model be ready to be trained and evaluated on your custom dataset. Putting everything together# In references/detection/, we have a number of helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py and references/detection/utils.py. Just download everything under references/detection to your folder and use them here. On Linux if you have wget, you can download them using below commands: os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\") os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\") os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\") os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\") os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\") 0 Since v0.15.0 torchvision provides new Transforms API to easily write data augmentation pipelines for Object Detection and Segmentation tasks. Let\u2019s write some helper functions for data augmentation / transformation: from torchvision.transforms import v2 as T def get_transform(train): transforms = [] if train: transforms.append(T.RandomHorizontalFlip(0.5)) transforms.append(T.ToDtype(torch.float, scale=True)) transforms.append(T.ToPureTensor()) return T.Compose(transforms) Testing forward() method (Optional)# Before iterating over the dataset, it\u2019s good to see what the model expects during training and inference time on sample data. import utils model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\") dataset = PennFudanDataset(\u0027data/PennFudanPed\u0027, get_transform(train=True)) data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, collate_fn=utils.collate_fn ) # For Training images, targets = next(iter(data_loader)) images = list(image for image in images) targets = [{k: v for k, v in t.items()} for t in targets] output = model(images, targets) # Returns losses and detections print(output) # For inference model.eval() x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] predictions = model(x) # Returns predictions print(predictions[0]) {\u0027loss_classifier\u0027: tensor(0.0503, grad_fn=\u003cNllLossBackward0\u003e), \u0027loss_box_reg\u0027: tensor(0.0387, grad_fn=\u003cDivBackward0\u003e), \u0027loss_objectness\u0027: tensor(0.0080, grad_fn=\u003cBinaryCrossEntropyWithLogitsBackward0\u003e), \u0027loss_rpn_box_reg\u0027: tensor(0.0018, grad_fn=\u003cDivBackward0\u003e)} {\u0027boxes\u0027: tensor([], size=(0, 4), grad_fn=\u003cStackBackward0\u003e), \u0027labels\u0027: tensor([], dtype=torch.int64), \u0027scores\u0027: tensor([], grad_fn=\u003cIndexBackward0\u003e)} We want to be able to train our model on an accelerator such as CUDA, MPS, MTIA, or XPU. Let\u2019s now write the main function which performs the training and the validation: from engine import train_one_epoch, evaluate # train on the accelerator or on the CPU, if an accelerator is not available device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device(\u0027cpu\u0027) # our dataset has two classes only - background and person num_classes = 2 # use our dataset and defined transformations dataset = PennFudanDataset(\u0027data/PennFudanPed\u0027, get_transform(train=True)) dataset_test = PennFudanDataset(\u0027data/PennFudanPed\u0027, get_transform(train=False)) # split the dataset in train and test set indices = torch.randperm(len(dataset)).tolist() dataset = torch.utils.data.Subset(dataset, indices[:-50]) dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:]) # define training and validation data loaders data_loader = torch.utils.data.DataLoader( dataset, batch_size=2, shuffle=True, collate_fn=utils.collate_fn ) data_loader_test = torch.utils.data.DataLoader( dataset_test, batch_size=1, shuffle=False, collate_fn=utils.collate_fn ) # get the model using our helper function model = get_model_instance_segmentation(num_classes) # move model to the right device model.to(device) # construct an optimizer params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD( params, lr=0.005, momentum=0.9, weight_decay=0.0005 ) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR( optimizer, step_size=3, gamma=0.1 ) # let\u0027s train it just for 2 epochs num_epochs = 2 for epoch in range(num_epochs): # train for one epoch, printing every 10 iterations train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) # update the learning rate lr_scheduler.step() # evaluate on the test dataset evaluate(model, data_loader_test, device=device) print(\"That\u0027s it!\") Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth 0%| | 0.00/170M [00:00\u003c?, ?B/s] 0%| | 128k/170M [00:00\u003c07:48, 380kB/s] 0%| | 256k/170M [00:00\u003c05:30, 539kB/s] 0%| | 384k/170M [00:00\u003c04:44, 624kB/s] 0%| | 768k/170M [00:00\u003c02:27, 1.20MB/s] 1%| | 1.62M/170M [00:01\u003c01:08, 2.56MB/s] 2%|\u258f | 3.12M/170M [00:01\u003c00:37, 4.67MB/s] 4%|\u258e | 6.25M/170M [00:01\u003c00:18, 9.19MB/s] 6%|\u258c | 10.0M/170M [00:01\u003c00:12, 13.3MB/s] 8%|\u258a | 13.8M/170M [00:01\u003c00:10, 16.1MB/s] 10%|\u2588 | 17.5M/170M [00:01\u003c00:08, 18.1MB/s] 13%|\u2588\u258e | 21.4M/170M [00:02\u003c00:07, 19.5MB/s] 15%|\u2588\u258d | 25.1M/170M [00:02\u003c00:07, 20.2MB/s] 17%|\u2588\u258b | 28.5M/170M [00:02\u003c00:07, 20.2MB/s] 19%|\u2588\u2589 | 32.4M/170M [00:02\u003c00:06, 21.2MB/s] 21%|\u2588\u2588\u258f | 36.1M/170M [00:02\u003c00:06, 21.6MB/s] 24%|\u2588\u2588\u258e | 40.0M/170M [00:02\u003c00:06, 22.1MB/s] 26%|\u2588\u2588\u258c | 43.9M/170M [00:03\u003c00:05, 22.3MB/s] 28%|\u2588\u2588\u258a | 46.8M/170M [00:03\u003c00:06, 20.8MB/s] 30%|\u2588\u2588\u2589 | 50.6M/170M [00:03\u003c00:05, 21.5MB/s] 32%|\u2588\u2588\u2588\u258f | 54.4M/170M [00:03\u003c00:05, 21.7MB/s] 34%|\u2588\u2588\u2588\u258d | 58.1M/170M [00:03\u003c00:05, 21.9MB/s] 36%|\u2588\u2588\u2588\u258b | 61.9M/170M [00:04\u003c00:05, 22.1MB/s] 39%|\u2588\u2588\u2588\u258a | 65.6M/170M [00:04\u003c00:04, 22.2MB/s] 41%|\u2588\u2588\u2588\u2588 | 69.5M/170M [00:04\u003c00:04, 22.5MB/s] 43%|\u2588\u2588\u2588\u2588\u258e | 73.4M/170M [00:04\u003c00:04, 22.7MB/s] 45%|\u2588\u2588\u2588\u2588\u258c | 77.2M/170M [00:04\u003c00:04, 22.8MB/s] 48%|\u2588\u2588\u2588\u2588\u258a | 81.0M/170M [00:04\u003c00:04, 22.7MB/s] 50%|\u2588\u2588\u2588\u2588\u2589 | 84.8M/170M [00:05\u003c00:03, 22.7MB/s] 52%|\u2588\u2588\u2588\u2588\u2588\u258f | 88.6M/170M [00:05\u003c00:03, 22.7MB/s] 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 92.5M/170M [00:05\u003c00:03, 22.9MB/s] 57%|\u2588\u2588\u2588\u2588\u2588\u258b | 96.4M/170M [00:05\u003c00:03, 23.0MB/s] 59%|\u2588\u2588\u2588\u2588\u2588\u2589 | 100M/170M [00:05\u003c00:03, 23.0MB/s] 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 104M/170M [00:05\u003c00:02, 23.2MB/s] 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 108M/170M [00:06\u003c00:02, 22.9MB/s] 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 112M/170M [00:06\u003c00:02, 22.5MB/s] 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116M/170M [00:06\u003c00:02, 22.8MB/s] 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 119M/170M [00:06\u003c00:02, 22.8MB/s] 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 123M/170M [00:06\u003c00:02, 22.7MB/s] 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 127M/170M [00:07\u003c00:01, 22.8MB/s] 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 131M/170M [00:07\u003c00:01, 22.9MB/s] 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 135M/170M [00:07\u003c00:01, 22.9MB/s] 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 138M/170M [00:07\u003c00:01, 22.8MB/s] 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 142M/170M [00:07\u003c00:01, 22.9MB/s] 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 146M/170M [00:07\u003c00:01, 23.0MB/s] 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 150M/170M [00:08\u003c00:00, 22.8MB/s] 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 154M/170M [00:08\u003c00:00, 22.8MB/s] 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 158M/170M [00:08\u003c00:00, 22.7MB/s] 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 161M/170M [00:08\u003c00:00, 22.8MB/s] 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 165M/170M [00:08\u003c00:00, 22.7MB/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 169M/170M [00:08\u003c00:00, 22.9MB/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:08\u003c00:00, 19.9MB/s] /workspace/tutorials-kr/intermediate_source/engine.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast(\u0027cuda\u0027, args...)` instead. Epoch: [0] [ 0/60] eta: 0:01:34 lr: 0.000090 loss: 2.9703 (2.9703) loss_classifier: 0.3432 (0.3432) loss_box_reg: 0.1413 (0.1413) loss_mask: 2.4372 (2.4372) loss_objectness: 0.0474 (0.0474) loss_rpn_box_reg: 0.0012 (0.0012) time: 1.5690 data: 0.0186 max mem: 1923 Epoch: [0] [10/60] eta: 0:00:16 lr: 0.000936 loss: 1.5786 (1.8444) loss_classifier: 0.2983 (0.2945) loss_box_reg: 0.2995 (0.2872) loss_mask: 0.8385 (1.2297) loss_objectness: 0.0300 (0.0288) loss_rpn_box_reg: 0.0038 (0.0042) time: 0.3270 data: 0.0194 max mem: 2590 Epoch: [0] [20/60] eta: 0:00:09 lr: 0.001783 loss: 0.8604 (1.2920) loss_classifier: 0.1766 (0.2155) loss_box_reg: 0.2502 (0.2794) loss_mask: 0.4121 (0.7682) loss_objectness: 0.0239 (0.0237) loss_rpn_box_reg: 0.0044 (0.0051) time: 0.1650 data: 0.0198 max mem: 2590 Epoch: [0] [30/60] eta: 0:00:05 lr: 0.002629 loss: 0.5386 (1.0609) loss_classifier: 0.0845 (0.1710) loss_box_reg: 0.2282 (0.2734) loss_mask: 0.2187 (0.5925) loss_objectness: 0.0129 (0.0192) loss_rpn_box_reg: 0.0040 (0.0048) time: 0.1235 data: 0.0199 max mem: 2590 Epoch: [0] [40/60] eta: 0:00:03 lr: 0.003476 loss: 0.4560 (0.9019) loss_classifier: 0.0506 (0.1408) loss_box_reg: 0.1597 (0.2458) loss_mask: 0.2087 (0.4952) loss_objectness: 0.0015 (0.0149) loss_rpn_box_reg: 0.0043 (0.0052) time: 0.1091 data: 0.0187 max mem: 2590 Epoch: [0] [50/60] eta: 0:00:01 lr: 0.004323 loss: 0.3754 (0.8045) loss_classifier: 0.0468 (0.1240) loss_box_reg: 0.1402 (0.2293) loss_mask: 0.1805 (0.4328) loss_objectness: 0.0015 (0.0126) loss_rpn_box_reg: 0.0062 (0.0058) time: 0.1009 data: 0.0190 max mem: 2634 Epoch: [0] [59/60] eta: 0:00:00 lr: 0.005000 loss: 0.3970 (0.7452) loss_classifier: 0.0479 (0.1133) loss_box_reg: 0.1487 (0.2184) loss_mask: 0.1736 (0.3959) loss_objectness: 0.0017 (0.0111) loss_rpn_box_reg: 0.0078 (0.0065) time: 0.0974 data: 0.0203 max mem: 2638 Epoch: [0] Total time: 0:00:08 (0.1489 s / it) creating index... index created! Test: [ 0/50] eta: 0:00:04 model_time: 0.0849 (0.0849) evaluator_time: 0.0016 (0.0016) time: 0.0932 data: 0.0064 max mem: 2638 Test: [49/50] eta: 0:00:00 model_time: 0.0185 (0.0464) evaluator_time: 0.0040 (0.0050) time: 0.0491 data: 0.0111 max mem: 2638 Test: Total time: 0:00:03 (0.0624 s / it) Averaged stats: model_time: 0.0185 (0.0464) evaluator_time: 0.0040 (0.0050) Accumulating evaluation results... DONE (t=0.01s). Accumulating evaluation results... DONE (t=0.01s). IoU metric: bbox Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.656 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.980 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.855 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.445 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.494 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.674 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.311 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.716 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.716 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.580 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.657 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.726 IoU metric: segm Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.726 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.980 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.924 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.267 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.532 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.748 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.329 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.764 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.765 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.580 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.776 Epoch: [1] [ 0/60] eta: 0:00:04 lr: 0.005000 loss: 0.3154 (0.3154) loss_classifier: 0.0460 (0.0460) loss_box_reg: 0.1283 (0.1283) loss_mask: 0.1313 (0.1313) loss_objectness: 0.0016 (0.0016) loss_rpn_box_reg: 0.0083 (0.0083) time: 0.0792 data: 0.0213 max mem: 2638 Epoch: [1] [10/60] eta: 0:00:03 lr: 0.005000 loss: 0.2622 (0.2920) loss_classifier: 0.0339 (0.0385) loss_box_reg: 0.0836 (0.0961) loss_mask: 0.1357 (0.1468) loss_objectness: 0.0016 (0.0026) loss_rpn_box_reg: 0.0077 (0.0079) time: 0.0783 data: 0.0167 max mem: 2638 Epoch: [1] [20/60] eta: 0:00:03 lr: 0.005000 loss: 0.2753 (0.3052) loss_classifier: 0.0339 (0.0390) loss_box_reg: 0.0836 (0.0997) loss_mask: 0.1450 (0.1564) loss_objectness: 0.0017 (0.0025) loss_rpn_box_reg: 0.0060 (0.0076) time: 0.0803 data: 0.0174 max mem: 2645 Epoch: [1] [30/60] eta: 0:00:02 lr: 0.005000 loss: 0.2605 (0.2878) loss_classifier: 0.0387 (0.0370) loss_box_reg: 0.0674 (0.0902) loss_mask: 0.1391 (0.1520) loss_objectness: 0.0010 (0.0021) loss_rpn_box_reg: 0.0045 (0.0065) time: 0.0880 data: 0.0179 max mem: 2645 Epoch: [1] [40/60] eta: 0:00:01 lr: 0.005000 loss: 0.2437 (0.2865) loss_classifier: 0.0378 (0.0380) loss_box_reg: 0.0732 (0.0883) loss_mask: 0.1324 (0.1519) loss_objectness: 0.0008 (0.0019) loss_rpn_box_reg: 0.0045 (0.0064) time: 0.0933 data: 0.0180 max mem: 2877 Epoch: [1] [50/60] eta: 0:00:00 lr: 0.005000 loss: 0.2518 (0.2789) loss_classifier: 0.0349 (0.0369) loss_box_reg: 0.0737 (0.0857) loss_mask: 0.1295 (0.1484) loss_objectness: 0.0006 (0.0018) loss_rpn_box_reg: 0.0045 (0.0060) time: 0.0978 data: 0.0192 max mem: 2881 Epoch: [1] [59/60] eta: 0:00:00 lr: 0.005000 loss: 0.2416 (0.2792) loss_classifier: 0.0333 (0.0366) loss_box_reg: 0.0716 (0.0852) loss_mask: 0.1318 (0.1499) loss_objectness: 0.0005 (0.0017) loss_rpn_box_reg: 0.0035 (0.0058) time: 0.0933 data: 0.0206 max mem: 2881 Epoch: [1] Total time: 0:00:05 (0.0893 s / it) creating index... index created! Test: [ 0/50] eta: 0:00:01 model_time: 0.0142 (0.0142) evaluator_time: 0.0013 (0.0013) time: 0.0221 data: 0.0064 max mem: 2881 Test: [49/50] eta: 0:00:00 model_time: 0.0152 (0.0153) evaluator_time: 0.0030 (0.0031) time: 0.0304 data: 0.0113 max mem: 2881 Test: Total time: 0:00:01 (0.0296 s / it) Averaged stats: model_time: 0.0152 (0.0153) evaluator_time: 0.0030 (0.0031) Accumulating evaluation results... DONE (t=0.01s). Accumulating evaluation results... DONE (t=0.01s). IoU metric: bbox Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.766 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.976 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.946 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.438 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.359 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.808 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.808 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.520 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.786 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.823 IoU metric: segm Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.758 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.976 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.941 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.506 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.787 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.343 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.793 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.793 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.480 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812 That\u0027s it! So after one epoch of training, we obtain a COCO-style mAP \u003e 50, and a mask mAP of 65. But what do the predictions look like? Let\u2019s take one image in the dataset and verify import matplotlib.pyplot as plt from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\") eval_transform = get_transform(train=False) model.eval() with torch.no_grad(): x = eval_transform(image) # convert RGBA -\u003e RGB and move to device x = x[:3, ...].to(device) predictions = model([x, ]) pred = predictions[0] image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8) image = image[:3, ...] pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])] pred_boxes = pred[\"boxes\"].long() output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\") masks = (pred[\"masks\"] \u003e 0.7).squeeze(1) output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\") plt.figure(figsize=(12, 12)) plt.imshow(output_image.permute(1, 2, 0)) \u003cmatplotlib.image.AxesImage object at 0x7f0daa4d6910\u003e The results look good! Wrapping up# In this tutorial, you have learned how to create your own training pipeline for object detection models on a custom dataset. For that, you wrote a torch.utils.data.Dataset class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset. For a more complete example, which includes multi-machine / multi-GPU training, check references/detection/train.py, which is present in the torchvision repository. Total running time of the script: (0 minutes 52.968 seconds) Download Jupyter notebook: torchvision_tutorial.ipynb Download Python source code: torchvision_tutorial.py Download zipped: torchvision_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/torchvision_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>